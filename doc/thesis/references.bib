@Book{gG07,
author = "Gratzer, George A.",
title = "More Math Into LaTeX",
publisher = "Birkhauser",
address = "Boston",
year = 2007,
edition = "4th"
}

@mastersthesis{sharma2020design,
  title={A Design Space for Distributed Producer-Consumer Data Structures Using RDMA},
  author={Sharma, Manoj Kumar},
  year={2020},
  school={University of Waterloo}
}

@inproceedings{anuj-guide,
author = {Kalia, Anuj and Kaminsky, Michael and Andersen, David G.},
title = {Design Guidelines for High Performance RDMA Systems},
year = {2016},
isbn = {9781931971300},
publisher = {USENIX Association},
address = {USA},
abstract = {Modern RDMA hardware offers the potential for exceptional performance, but design choices including which RDMA operations to use and how to use them significantly affect observed performance. This paper lays out guidelines that can be used by system designers to navigate the RDMA design space. Our guidelines emphasize paying attention to low-level details such as individual PCIe transactions and NIC architecture. We empirically demonstrate how these guidelines can be used to improve the performance of RDMA-based systems: we design a networked sequencer that outperforms an existing design by 50x, and improve the CPU efficiency of a prior high-performance key-value store by 83%. We also present and evaluate several new RDMA optimizations and pitfalls, and discuss how they affect the design of RDMA systems.},
booktitle = {Proceedings of the 2016 USENIX Conference on Usenix Annual Technical Conference},
pages = {437–450},
numpages = {14},
location = {Denver, CO, USA},
series = {USENIX ATC '16}
}


@INPROCEEDINGS{eval-mpp,  author={H. {Huang} and S. {Ghandeharizadeh}},  booktitle={2019 IEEE International Conference on Big Data (Big Data)},   title={An Evaluation of RDMA-based Message Passing Protocols},   year={2019},  volume={},  number={},  pages={3854-3863},  abstract={An enumeration of RDMA messaging verbs (READ, WRITE, SEND/RECEIVE) and queue pair types creates a diverse set of message passing protocols. This paper constructs three abstract communication paradigms to quantify the performance and scalability characteristics of five protocols. With each abstraction, different protocols provide different results and the identity of the protocol that is superior to the others changes. Factors such as the number of queue pairs per node, the size of messages, the number of pending requests per queue pair, and the abstract communication paradigm dictate the superiority of a protocol. These results are important for design and implementation of algorithms and techniques that use the emerging RDMA.},  keywords={Bandwidth;Protocols;Optical wavelength conversion;Computer architecture;Scalability;IP networks;Servers;RDMA;message passing protocols;performance},  doi={10.1109/BigData47090.2019.9005637},  ISSN={},  month={Dec},}


@inproceedings{fasst,
author = {Kalia, Anuj and Kaminsky, Michael and Andersen, David G.},
title = {FaSST: Fast, Scalable and Simple Distributed Transactions with Two-Sided (RDMA) Datagram RPCs},
year = {2016},
isbn = {9781931971331},
publisher = {USENIX Association},
address = {USA},
abstract = {FaSST is an RDMA-based system that provides distributed in-memory transactions with serializability and durability. Existing RDMA-based transaction processing systems use one-sided RDMA primitives for their ability to bypass the remote CPU. This design choice brings several drawbacks. First, the limited flexibility of one-sided RDMA reduces performance and increases software complexity when designing distributed data stores. Second, deep-rooted technical limitations of RDMA hardware limit scalability in large clusters. FaSST eschews one-sided RDMA for fast RPCs using two-sided unreliable datagrams, which we show drop packets extremely rarely on modern RDMA networks. This approach provides better performance, scalability, and simplicity, without requiring expensive reliability mechanisms in software. In comparison with published numbers, FaSST outperforms FaRM on the TATP benchmark by almost 2x while using close to half the hardware resources, and it outperforms DrTM+R on the SmallBank benchmark by around 1.7x without making data locality assumptions.},
booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
pages = {185–201},
numpages = {17},
location = {Savannah, GA, USA},
series = {OSDI'16}
}

@inproceedings{herd,
author = {Kalia, Anuj and Kaminsky, Michael and Andersen, David G.},
title = {Using RDMA Efficiently for Key-Value Services},
year = {2014},
isbn = {9781450328364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2619239.2626299},
doi = {10.1145/2619239.2626299},
abstract = {This paper describes the design and implementation of HERD, a key-value system designed to make the best use of an RDMA network. Unlike prior RDMA-based key-value systems, HERD focuses its design on reducing network round trips while using efficient RDMA primitives; the result is substantially lower latency, and throughput that saturates modern, commodity RDMA hardware.HERD has two unconventional decisions: First, it does not use RDMA reads, despite the allure of operations that bypass the remote CPU entirely. Second, it uses a mix of RDMA and messaging verbs, despite the conventional wisdom that the messaging primitives are slow. A HERD client writes its request into the server's memory; the server computes the reply. This design uses a single round trip for all requests and supports up to 26 million key-value operations per second with 5μs average latency. Notably, for small key-value items, our full system throughput is similar to native RDMA read throughput and is over 2X higher than recent RDMA-based key-value systems. We believe that HERD further serves as an effective template for the construction of RDMA-based datacenter services.},
booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM},
pages = {295–306},
numpages = {12},
keywords = {RDMA, ROCE, infiniband, key-value stores},
location = {Chicago, Illinois, USA},
series = {SIGCOMM '14}
}

@article{10.1145/2740070.2626299,
author = {Kalia, Anuj and Kaminsky, Michael and Andersen, David G.},
title = {Using RDMA Efficiently for Key-Value Services},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2740070.2626299},
doi = {10.1145/2740070.2626299},
abstract = {This paper describes the design and implementation of HERD, a key-value system designed to make the best use of an RDMA network. Unlike prior RDMA-based key-value systems, HERD focuses its design on reducing network round trips while using efficient RDMA primitives; the result is substantially lower latency, and throughput that saturates modern, commodity RDMA hardware.HERD has two unconventional decisions: First, it does not use RDMA reads, despite the allure of operations that bypass the remote CPU entirely. Second, it uses a mix of RDMA and messaging verbs, despite the conventional wisdom that the messaging primitives are slow. A HERD client writes its request into the server's memory; the server computes the reply. This design uses a single round trip for all requests and supports up to 26 million key-value operations per second with 5μs average latency. Notably, for small key-value items, our full system throughput is similar to native RDMA read throughput and is over 2X higher than recent RDMA-based key-value systems. We believe that HERD further serves as an effective template for the construction of RDMA-based datacenter services.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = aug,
pages = {295–306},
numpages = {12},
keywords = {key-value stores, infiniband, ROCE, RDMA}
}



@INPROCEEDINGS{rpc-vs-rdma,  author={B. A. {Brock} and Y. {Chen} and J. {Yan} and J. {Owens} and A. {Buluç} and K. {Yelick}},  booktitle={2019 IEEE/ACM 9th Workshop on Irregular Applications: Architectures and Algorithms (IA3)},   title={RDMA vs. RPC for Implementing Distributed Data Structures},   year={2019},  volume={},  number={},  pages={17-22},  doi={10.1109/IA349570.2019.00009}}

@article{ziegler2020rdma,
  title={RDMA Communciation Patterns},
  author={Ziegler, Tobias and Leis, Viktor and Binnig, Carsten},
  journal={Datenbank-Spektrum},
  volume={20},
  number={3},
  pages={199--210},
  year={2020},
  publisher={Springer}
}

@INPROCEEDINGS{rdma-fast-dbms,  author={P. {Fent} and A. v. {Renen} and A. {Kipf} and V. {Leis} and T. {Neumann} and A. {Kemper}},  booktitle={2020 IEEE 36th International Conference on Data Engineering (ICDE)},   title={Low-Latency Communication for Fast DBMS Using RDMA and Shared Memory},   year={2020},  volume={},  number={},  pages={1477-1488},  doi={10.1109/ICDE48307.2020.00131}}


@inproceedings{pilaf,
author = {Mitchell, Christopher and Geng, Yifeng and Li, Jinyang},
title = {Using One-Sided RDMA Reads to Build a Fast, CPU-Efficient Key-Value Store},
year = {2013},
publisher = {USENIX Association},
address = {USA},
abstract = {Recent technological trends indicate that future datacenter networks will incorporate High Performance Computing network features, such as ultra-low latency and CPU bypassing. How can these features be exploited in datacenter-scale systems infrastructure? In this paper, we explore the design of a distributed in-memory key-value store called Pilaf that takes advantage of Remote Direct Memory Access to achieve high performance with low CPU overhead.In Pilaf, clients directly read from the server's memory via RDMA to perform gets, which commonly dominate key-value store workloads. By contrast, put operations are serviced by the server to simplify the task of synchronizing memory accesses. To detect inconsistent RDMA reads with concurrent CPU memory modifications, we introduce the notion of self-verifying data structures that can detect read-write races without client-server coordination. Our experiments show that Pilaf achieves low latency and high throughput while consuming few CPU resources. Specifically, Pilaf can surpass 1.3 million ops/sec (90% gets) using a single CPU core compared with 55K for Memcached and 59K for Redis.},
booktitle = {Proceedings of the 2013 USENIX Conference on Annual Technical Conference},
pages = {103–114},
numpages = {12},
location = {San Jose, CA},
series = {USENIX ATC'13}
}

@inproceedings{farm,
author = {Dragojevi\'{c}, Aleksandar and Narayanan, Dushyanth and Hodson, Orion and Castro, Miguel},
title = {FaRM: Fast Remote Memory},
year = {2014},
isbn = {9781931971096},
publisher = {USENIX Association},
address = {USA},
abstract = {We describe the design and implementation of FaRM, a new main memory distributed computing platform that exploits RDMA to improve both latency and throughput by an order of magnitude relative to state of the art main memory systems that use TCP/IP. FaRM exposes the memory of machines in the cluster as a shared address space. Applications can use transactions to allocate, read, write, and free objects in the address space with location transparency. We expect this simple programming model to be sufficient for most application code. FaRM provides two mechanisms to improve performance where required: lock-free reads over RDMA, and support for collocating objects and function shipping to enable the use of efficient single machine transactions. FaRM uses RDMA both to directly access data in the shared address space and for fast messaging and is carefully tuned for the best RDMA performance. We used FaRM to build a key-value store and a graph store similar to Facebook's. They both perform well, for example, a 20-machine cluster can perform 167 million key-value lookups per second with a latency of 31µs.},
booktitle = {Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation},
pages = {401–414},
numpages = {14},
location = {Seattle, WA},
series = {NSDI'14}
}

@inproceedings{socksdirect,
author = {Li, Bojie and Cui, Tianyi and Wang, Zibo and Bai, Wei and Zhang, Lintao},
title = {Socksdirect: Datacenter Sockets Can Be Fast and Compatible},
year = {2019},
isbn = {9781450359566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341302.3342071},
doi = {10.1145/3341302.3342071},
abstract = {Communication intensive applications in hosts with multi-core CPU and high speed networking hardware often put considerable stress on the native socket system in an OS. Existing socket replacements often leave significant performance on the table, as well have limitations on compatibility and isolation.In this paper, we describe SocksDirect, a user-space high performance socket system. SocksDirect is fully compatible with Linux socket and can be used as a drop-in replacement with no modification to existing applications. To achieve high performance, SocksDirect leverages RDMA and shared memory (SHM) for inter-host and intra-host communication, respectively. To bridge the semantics gap between socket and RDMA / SHM, we optimize for the common cases while maintaining compatibility in general. SocksDirect achieves isolation by employing a trusted monitor daemon to handle control plane operations such as connection establishment and access control. The data plane is peer-to-peer between processes, in which we remove multi-thread synchronization, buffer management, large payload copy and process wakeup overheads in common cases. Experiments show that SocksDirect achieves 7~20x better message throughput and 17~35x better latency than Linux socket, and reduces Nginx HTTP latency to 1/5.5.},
booktitle = {Proceedings of the ACM Special Interest Group on Data Communication},
pages = {90–103},
numpages = {14},
keywords = {socket, datacenter, RDMA, user-space},
location = {Beijing, China},
series = {SIGCOMM '19}
}



@article{loggp,
title = "LogGP: Incorporating Long Messages into the LogP Model for Parallel Computation",
journal = "Journal of Parallel and Distributed Computing",
volume = "44",
number = "1",
pages = "71 - 79",
year = "1997",
issn = "0743-7315",
doi = "https://doi.org/10.1006/jpdc.1997.1346",
url = "http://www.sciencedirect.com/science/article/pii/S0743731597913460",
author = "Albert Alexandrov and Mihai F. Ionescu and Klaus E. Schauser and Chris Scheiman",
abstract = "We present a new model of parallel computation—the LogGP model—and use it to analyze a number of algorithms, most notably, the single node scatter (one-to-all personalized broadcast). The LogGP model is an extension of the LogP model for parallel computation which abstracts the communication of fixed-sized short messages through the use of four parameters: the communication latency (L), overhead (o), bandwidth (g), and the number of processors (P). As evidenced by experimental data, the LogP model can accurately predict communication performance when only short messages are sent (as on the CM-5). However, many existing parallel machines have special support for long messages and achieve a much higher bandwidth for long messages than for short messages (e.g., IBM SP-2, Paragon, Meiko CS-2, Ncube/2). We extend the basic LogP model with a linear model for long messages. This combination, which we call the LogGP model of parallel computation, has one additional parameter,G, which captures the bandwidth obtained for long messages. Experimental data collected on the Meiko CS-2 shows that this simple extension of the LogP model can quite accurately predict communication performance for both short and long messages. This paper discusses algorithm design and analysis under the new model. We also examine, in more detail, the single node scatter problem under LogGP. We derive solutions for this problem which are qualitatively different from those obtained under the simpler LogP model, reflecting the importance of capturing long messages in a model."
}

@inproceedings{dare,
  author={Marius Poke and Torsten Hoefler},
  title={{DARE: High-Performance State Machine Replication on RDMA Networks}},
  year={2015},
  month={06},
  pages={107--118},
  booktitle={Proceedings of the 24th International Symposium on High-Performance Parallel and Distributed Computing (HPDC'15)},
  location={Portland, OR, USA},
  publisher={ACM},
  isbn={978-1-4503-3550-8},
}

@inproceedings{scal-rdma-rpc,
author = {Chen, Youmin and Lu, Youyou and Shu, Jiwu},
title = {Scalable RDMA RPC on Reliable Connection with Efficient Resource Sharing},
year = {2019},
isbn = {9781450362818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302424.3303968},
doi = {10.1145/3302424.3303968},
abstract = {RDMA provides extremely low latency and high bandwidth to distributed systems. Unfortunately, it fails to scale and suffers from performance degradation when transferring data to an increasing number of targets on Reliable Connection (RC). We observe that the above scalability issue has its root in the resource contention in the NIC cache, the CPU cache and the memory of each server. In this paper, we propose ScaleRPC, an efficient RPC primitive using one-sided RDMA verbs on reliable connection to provide scalable performance. To effectively alleviate the resource contention, ScaleRPC introduces 1) connection grouping to organize the network connections into groups, so as to balance the saturation and thrashing of the NIC cache; 2) virtualized mapping to enable a single message pool to be shared by different groups of connections, which reduces CPU cache misses and improve memory utilization. Such scalable connection management provides substantial performance benefits: By deploying ScaleRPC both in a distributed file system and a distributed transactional system, we observe that it achieves high scalability and respectively improves performance by up to 90% and 160% for metadata accessing and SmallBank transaction processing.},
booktitle = {Proceedings of the Fourteenth EuroSys Conference 2019},
articleno = {19},
numpages = {14},
keywords = {RDMA, Scalability, Resource Sharing},
location = {Dresden, Germany},
series = {EuroSys '19}
}

@article{ziegler2020rdma,
  title={RDMA Communciation Patterns},
  author={Ziegler, Tobias and Leis, Viktor and Binnig, Carsten},
  journal={Datenbank-Spektrum},
  volume={20},
  number={3},
  pages={199--210},
  year={2020},
  publisher={Springer}
}

@article{Huang2019AnEO,
  title={An Evaluation of RDMA-based Message Passing Protocols},
  author={H. Huang and Shahram Ghandeharizadeh},
  journal={2019 IEEE International Conference on Big Data (Big Data)},
  year={2019},
  pages={3340-3349}
}

@ARTICLE{nessie,  author={B. {Cassell} and T. {Szepesi} and B. {Wong} and T. {Brecht} and J. {Ma} and X. {Liu}},  journal={IEEE Transactions on Parallel and Distributed Systems},   title={Nessie: A Decoupled, Client-Driven Key-Value Store Using RDMA},   year={2017},  volume={28},  number={12},  pages={3537-3552},  doi={10.1109/TPDS.2017.2729545}}


@inproceedings{memcached,
author = {Jose, Jithin and Subramoni, Hari and Luo, Miao and Zhang, Minjia and Huang, Jian and Rahman, Md and Islam, Nusrat and Ouyang, Xiangyong and Wang, Hao and Sur, Sayantan and Panda, D.K.},
year = {2011},
month = {10},
pages = {743 - 752},
title = {Memcached Design on High Performance RDMA Capable Interconnects},
doi = {10.1109/ICPP.2011.37}
}

@inproceedings{dbrackjoin,
author = {Barthels, Claude and Loesing, Simon and Alonso, Gustavo and Kossmann, Donald},
title = {Rack-Scale In-Memory Join Processing Using RDMA},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2750547},
doi = {10.1145/2723372.2750547},
abstract = {Database systems running on a cluster of machines, i.e. rack-scale databases, are a common architecture for many large databases and data appliances. As the data movement across machines is often a significant bottleneck, these systems typically use a low-latency, high-throughput network such as InfiniBand. To achieve the necessary performance, parallel join algorithms must take advantage of the primitives provided by the network to speed up data transfer.In this paper we focus on implementing parallel in-memory joins using Remote Direct Memory Access (RDMA), a communication mechanism to transfer data directly into the memory of a remote machine. The results of this paper are, to our knowledge, the first detailed analysis of parallel hash joins using RDMA. To capture their behavior independently of the network characteristics, we develop an analytical model and test our implementation on two different types of networks. The experimental results show that the model is accurate and the resulting distributed join exhibits good performance.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {1463–1475},
numpages = {13},
keywords = {rack scale databases, distributed join, distributed query processing, join processing with rdma},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}


@inproceedings{broker,
author = {Makait, Hendrik},
title = {Rethinking Message Brokers on RDMA and NVM},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384403},
doi = {10.1145/3318464.3384403},
abstract = {Over the last years, message brokers have become an important part of enterprise systems. As microservice architectures gain popularity and the need to analyze data produced by these services grows, companies increasingly rely on message brokers to orchestrate the flow of events between different applications as well as between data-producing services and streaming engines that analyze the data in real-time.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2833–2835},
numpages = {3},
keywords = {messaging, modern hardware, NVRAM, RDMA, distributed, message broker},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@INPROCEEDINGS{hbase,  author={J. {Huang} and X. {Ouyang} and J. {Jose} and M. {Wasi-ur-Rahman} and H. {Wang} and M. {Luo} and H. {Subramoni} and C. {Murthy} and D. K. {Panda}},  booktitle={2012 IEEE 26th International Parallel and Distributed Processing Symposium},   title={High-Performance Design of HBase with RDMA over InfiniBand},   year={2012},  volume={},  number={},  pages={774-785},  doi={10.1109/IPDPS.2012.74}}



@INPROCEEDINGS{hdfs,  author={N. S. {Islam} and M. W. {Rahman} and J. {Jose} and R. {Rajachandrasekar} and H. {Wang} and H. {Subramoni} and C. {Murthy} and D. K. {Panda}},  booktitle={SC '12: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},   title={High performance RDMA-based design of HDFS over InfiniBand},   year={2012},  volume={},  number={},  pages={1-12},  doi={10.1109/SC.2012.65}}


@article{polarfs,
author = {Cao, Wei and Liu, Zhenjun and Wang, Peng and Chen, Sen and Zhu, Caifeng and Zheng, Song and Wang, Yuhui and Ma, Guoqing},
title = {PolarFS: An Ultra-Low Latency and Failure Resilient Distributed File System for Shared Storage Cloud Database},
year = {2018},
issue_date = {August 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3229863.3229872},
doi = {10.14778/3229863.3229872},
abstract = {PolarFS is a distributed file system with ultra-low latency and high availability, designed for the POLARDB database service, which is now available on the Alibaba Cloud. PolarFS utilizes a lightweight network stack and I/O stack in user-space, taking full advantage of the emerging techniques like RDMA, NVMe, and SPDK. In this way, the end-to-end latency of PolarFS has been reduced drastically and our experiments show that the write latency of PolarFS is quite close to that of local file system on SSD. To keep replica consistency while maximizing I/O throughput for PolarFS, we develop ParallelRaft, a consensus protocol derived from Raft, which breaks Raft's strict serialization by exploiting the out-of-order I/O completion tolerance capability of databases. ParallelRaft inherits the understand-ability and easy implementation of Raft while providing much better I/O scalability for PolarFS. We also describe the shared storage architecture of PolarFS, which gives a strong support for POLARDB.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1849–1862},
numpages = {14}
}

@inproceedings{rfp,
author = {Su, Maomeng and Zhang, Mingxing and Chen, Kang and Guo, Zhenyu and Wu, Yongwei},
title = {RFP: When RPC is Faster than Server-Bypass with RDMA},
year = {2017},
isbn = {9781450349383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3064176.3064189},
doi = {10.1145/3064176.3064189},
abstract = {Remote Direct Memory Access (RDMA) has been widely deployed in modern data centers. However, existing usages of RDMA lead to a dilemma between performance and redesign cost. They either directly replace socket-based send/receive primitives with the corresponding RDMA counterpart (server-reply), which only achieves moderate performance improvement; or push performance further by using one-sided RDMA operations to totally bypass the server (server-bypass), at the cost of redesigning the software.In this paper, we introduce two interesting observations about RDMA. First, RDMA has asymmetric performance characteristics, which can be used to improve server-reply's performance. Second, the performance of server-bypass is not as good as expected in many cases, because more rounds of RDMA may be needed if the server is totally bypassed. We therefore introduce a new RDMA paradigm called Remote Fetching Paradigm (RFP). Although RFP requires users to set several parameters to achieve the best performance, it supports the legacy RPC interfaces and hence avoids the need of redesigning application-specific data structures. Moreover, with proper parameters, it can achieve even higher IOPS than that of the previous paradigms.We have designed and implemented an in-memory key-value store based on RFP to evaluate its effectiveness. Experimental results show that RFP improves performance by 1.6\texttimes{}~4\texttimes{} compared with both server-reply and server-bypass paradigms.},
booktitle = {Proceedings of the Twelfth European Conference on Computer Systems},
pages = {1–15},
numpages = {15},
location = {Belgrade, Serbia},
series = {EuroSys '17}
}

@inproceedings{scalerpc,
author = {Chen, Youmin and Lu, Youyou and Shu, Jiwu},
title = {Scalable RDMA RPC on Reliable Connection with Efficient Resource Sharing},
year = {2019},
isbn = {9781450362818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302424.3303968},
doi = {10.1145/3302424.3303968},
abstract = {RDMA provides extremely low latency and high bandwidth to distributed systems. Unfortunately, it fails to scale and suffers from performance degradation when transferring data to an increasing number of targets on Reliable Connection (RC). We observe that the above scalability issue has its root in the resource contention in the NIC cache, the CPU cache and the memory of each server. In this paper, we propose ScaleRPC, an efficient RPC primitive using one-sided RDMA verbs on reliable connection to provide scalable performance. To effectively alleviate the resource contention, ScaleRPC introduces 1) connection grouping to organize the network connections into groups, so as to balance the saturation and thrashing of the NIC cache; 2) virtualized mapping to enable a single message pool to be shared by different groups of connections, which reduces CPU cache misses and improve memory utilization. Such scalable connection management provides substantial performance benefits: By deploying ScaleRPC both in a distributed file system and a distributed transactional system, we observe that it achieves high scalability and respectively improves performance by up to 90% and 160% for metadata accessing and SmallBank transaction processing.},
booktitle = {Proceedings of the Fourteenth EuroSys Conference 2019},
articleno = {19},
numpages = {14},
keywords = {Scalability, Resource Sharing, RDMA},
location = {Dresden, Germany},
series = {EuroSys '19}
}
