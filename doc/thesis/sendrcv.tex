\section{Send Receive} \label{sec:conn:send}\label{sendrcv}\label{sendrcv-design}

The send-receive protocol is by far the simplest, as the send and receive verbs already provide the required 
message passing interface. There are still multiple ways to implement and optimize such a protocol and a few pitfalls 
we need to address to get good performance.

\paragraph{} We presented the basic function of the send verb in Section \ref{sec:bg:send}. It allows the sender to transmit 
a message to the receiver, without any additional information, as long as the receiver has posted a receive buffer that is 
large enough to receive the message. To transform this verb to a fully functioning connection, we add two basic things. 
A kind of \emph{receive buffer management} that allows the receiver to receive multiple messages at the same time, 
and to get any reasonable performance we need to be able to send multiple messages asynchronously.

\subsection{Protocol} 

The sender assumes that the receiver is always ready to receive the message and has at least one receive buffer 
posted. This means sending essentially only involves issuing a send work request. As we already mentioned earlier, it is very 
important to keep the sending pipeline full to get good performance. Using \emph{unacknowledged messages} can increase
throughput by a factor of five or more.
We use the trick of issuing send requests with monotonically increasing \code{wr\_id}s, explained in Section~\ref{sec:protocols},
to be able to support multiple 
outstanding messages.

\paragraph{} The receiver always needs to have at least one posted receive buffer. We achieve this by having an array of 
posted receive buffers, which are initially registered during connection setup. When receiving a new message we return 
the corresponding buffer. We can match a completion event to the correct buffer by using the array index of the receive 
buffer as a \code{wr\_id} when posting the receive request. As soon as the application is done processing the message,
it marks it as free which will repost the corresponding buffer. By having multiple receive buffers it allows us to have 
numerous unacknowledged messages which improves performance drastically.

\paragraph{} It is important to note that in production systems there needs to be a way for the sender to notice whether 
enough receive buffers are ready. If there is no posted receive buffer available when a message is received,
the receiver  generates
a so-called \emph{Reader Not Ready} error. This will either cause a large back-off for the sender 
or even cause the connection
to break.

We observed this problem specifically for N:1 communication. This can be mitigated to some extend by optimizing receiving and
reposting buffers.




\subsection{Extensions}

There are multiple extensions to the described base protocol. The changes either improve performance, allow us to
share resources between connections, or enable different kinds of communication patterns.

\subsubsection{Inline Sending}
The send verb has a slight variation called \emph{inline send}. Inline sending means that instead of 
simply referencing the payload in a work request, the sending CPU directly copies it to the RNIC using MMIO. This prevents
the NIC from having to issue an addition DMA and can reduce latency for very small messages~\cite{anuj-guide}. It does,
however,
increase the load for the sending CPU. As we will see, the sending CPU is oftentimes the bottleneck so we did not further 
evaluate inline sending in this thesis. It can, however, be a viable optimization for small messages.


\subsubsection{Receive Batching} 
As described, there are significant penalties if the receiver is not able to keep up and stalls the sender.
This can be mitigated a little by optimizing the receiver. For one, instead of polling one receive CQ at a time we poll up to 32
at a time into an array of CQEs. We observed in microbenchmarks that this can improve the observed throughput by nearly half.
And by batching the reposting of receive buffers we can also further improve throughput.

\subsubsection{Shared Receive Queues} 
By having an array of $k$ posted receive buffers for each receiver, the memory reserved for receiving messages can grow quite
large. If a node has  open connections to $N$ other nodes, it needs to reserve  $N*k*max\_msg\_size$ bytes, even if 
the total request volume is quite small (i.e., we expect only burst of $k$ messages but from different nodes at different times).

We can reduce the total memory usage by using \emph{Shared Receive Queues~(SRQ)}. As the name already tells us SRQs allow us
to share receive queues between multiple QPs. This means, we can reuse a single receive queue
for multiple connections, allowing
multiple receivers to share the same receive buffers. This means the total memory usage does not grow with the number of
open connections, but stays constant.

The usage of SRQs does not impact completion queues. The completion event for consuming a posted receive buffer still ends up 
in the CQ of the corresponding QP.

One major change when using an SRQ is that we do not submit receive work requests to the respective QPs but to the single SRQ. This
means that we either have to introduce locking to access the SRQ, which introduces a significant performance penalty, or 
delegate the posting of the receive buffer to a single thread. Instead of reposting the buffer themselves, each connection 
enqueues the id of the to be posted buffer into a queue. The single reposting thread will dequeue and repost it. This also
allows us to have central batching for reposting the buffers. We opted to use the latter approach for our SRQ based 
implementation.

\subsubsection{Single Receiver} 
It is very common to have an N:1 communication pattern where a single server receives messages from multiple clients. This
could be achieved by simply round-robin over the $N$ connections. For this connection, however, we used the fact that we can 
associate a single completion queue with multiple queue pairs. This means, if we are in a \emph{single receiver mode}, all 
receive completion events will end up in a single CQ. Allowing us to poll a single queue to receive a message from $N$ 
different sender.


\subsection{Feature Analysis}

The Send and Receive verbs are generally regarded as the simplest verbs to work with and they give us a lot of features
that make them suitable for many applications. We take a look at the traits we introduced in Section~\ref{sec:features} 
and see which of them are met by our send-receive protocol.

\paragraph{} The protocol fulfills our requirements for being \emph{non-blocking}, which means if the receiving application does not free a single
received buffer, it does not impact the protocols ability to transmit new messages.
It also provides us with convenient \emph{interrupts} that allow us to save a lot CPU utilization for certain workloads.
Finally, when using shared receive queues, the protocol allows us to \emph{share resources} between multiple connections.

\paragraph{} It does, however, not allow for \emph{variable message sizes} which can unnecessarily blow up the total memory
requirements if an application needs to send many small and a few large messages, as each of the small messages will have
to consume a buffer that is large enough to contain a large message. We also argue that the protocol is not truly 
\emph{zero copy} as for almost all real world use cases we would need to copy the received message out of the
receive buffer and into a data structure.

