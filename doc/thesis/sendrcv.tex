\section{Send Receive} \label{sec:conn:send}\label{sendrcv}
\subsection{Design} \label{sendrcv-design}
The Send Receive based protocol is by far the simplest, as the send and receive verbs basically already provide the 
message passing interface we want. There are still multiple ways to implement such a protocol and a few pitfalls 
we need to address to get good performance.

\paragraph{} We presented the basic function of the send verb in Section \ref{sec:bg:send}. It allows the sender to transmit 
a message to the receiver, without any additional information, as long as the receiver has posted a receiver buffer that is 
large enough to receive the message. To transform this verb to a fully functioning connection we need two basic things. 
Some kind of \emph{receive buffer management} that allows the receiver to receive multiple messages at the same time, 
and to get any reasonable performance we need to be able to send multiple messages asynchronously.

\subsubsection{Sender} 
The sender assumes that the receiver is always ready to receive the message and has at least one receive buffer posted. This
means sending essentially only involves issuing a send work request.

As we already mentioned earlier it is very important to keep the sending pipeline full to get good performance. 
This kind of \emph{unacknowledged messages} can increase throughout by a factor of 5 or more.

\paragraph{} We are able to wait for a specific work request to complete by issuing monotonically increasing IDs. We store the
\comment{This section is a general approach used for almost all connections. Maybe move this?}
last ID with a matching completion event. If we try to wait on a lower ID we know we already received a matching completion 
event prior, because of the in order guarantees of InfiniBand.\comment{Maybe cite something?} If the provided ID is higher then the
last seen ID, we  poll the completion queue until we receive one with a matching \code{wr\_id}.

This approach will eventually lead to an overflow, but given our observed maximum message rate we do not expect to see this 
happen in the next 10 thousand years. 


\paragraph{Batching} As briefly mentioned in Section \ref{sec:bg:verbs} \comment{do this..} we can post multiple work request at the same time. 
This \emph{Doorbell batching} reduces the number of generated MMIOs~\cite{anuj-guide}, reduces the CPU load and can in turn 
drastically improve bandwidth if the connection is send-side CPU bottleneck. We observed a performance increase by up to 
a factor of two. It is however questionable if this connection level batching is the right approach when using the 
\emph{Reliable Connection} that we evaluated. With RC all messages need to end up at the same receiver meaning application 
level batching should be possible and we observed that sending larger messages will outperform sending small messages in 
batches.

\paragraph{Inline Sending} The send verb has a slight variation called inline send. Inline sending means that instead of 
simply referencing the payload in a work request, the sending CPU will directly copy it to the RNIC using MMIO. This prevents
the NIC from having to issue an addition DMA and can reduce latency for very small messages.~\cite{anuj-guide} It does however
increase the load for the sending CPU. As we will see the sending CPU is oftentimes the bottleneck so we did not further 
evaluate inline sending in this section.

\subsubsection{Receiver}

The receiver always needs to have at least one posted receive buffer. We achieve this by having an array of posted receive 
buffers. When receiving a new message we return the corresponding receive buffer. As soon as the application is done processing
the message, it will have to mark it as free which will repost the corresponding buffer.


Slow processing of messages can still stall the sender, but by having multiple receive buffer it allows us to have numerous 
\emph{in-flight messages} which improves performance drastically.

\paragraph{} It is important that we note here that in production systems there needs to be a way for the sender to notice whether 
enough receive buffers are ready. If there is no posted receive buffer when a message is received the receiver will generate
a so called \emph{reader not ready} error. This will either cause a large back off for the sender or even cause the connection
to break.

We observed this problem specifically for N:1 communication. This can be mitigated to some extend by optimizing receiving and
reposting buffers through batching.

\paragraph{Batching} As described there are significant penalties if the receiver is not able to keep up and stalls the sender.
This can be mitigated a little optimizing the receiver. For one instead of polling one receive CQ at a time we poll up to 32
at a time into a array of CQEs. We observed in microbenchmarks that this can improve the observed throughput by nearly half. By
batching the receive work requests we can further improve throughput similarly to batching send requests.

\paragraph{Shared Receive Queues} By having an array of $k$ posted receive buffers for each receiver the memory reserved for 
receiving messages can grow quite large. If a node has an open connection to $N$ other nodes it will need to reserve 
$N*k*max\_msg\_size$ Bytes, even if the total request volume is quite small (i.e. we expect only burst of $k$ messages but 
from different nodes at different times).

We can reduce the total memory usage by using \emph{Shared Receive Queues~(SRQ)}. As the name already tells us SRQs allow us
share receive queues between multiple QPs. This means we can reuse a single group of receive queues for multiple connections.
This means the total memory usage does not grow with $N$.

Its worth noting that the usage of SRQs does not impact completion queues. The completion event for consuming a posted receive
buffer still ends up in the CQ of the corresponding QP.

One major change with using an SRQ is that we do not submit receive work request to each of the QPs but to the single SRQ. This
means that we will either have to introduce locking to access the SRQ, which introduces a significant performance penalty or 
delegate the posting of the receive buffer to a single thread. Instead of reposting the buffer themselves each connection 
enqueues the id to the to be posted buffer to a queue. The single reposting thread will dequeue it and repost it. This also
allows us to have central batching for reposting the buffers.

\paragraph{Single Receiver} It is very common to have an N:1 communication pattern where a single server receives messages 
from multiple clients. This could be achieved by simply round-robin over the $N$ connections. For this connection however 
we used the fact that we can associate a singe Completion Queue with multiple Queue Pairs. This means if we are in a
\emph{single receiver mode} all receive completion events will end up in a single CQ. Allowing us to poll a single queue to
receive a message from $N$ different sender.

Especially if we are not using an SRQ we will have to know by which QP this message was received by as we will need to repost
the buffer to it. We can get the 32 bit QP number from the Completion Queue Event, which identifies the Queue Pair. 


\subsection{Evaluation} \label{conn:send:eval}

We ran all our evaluations on two machines running CentOS 7 containing two Intel Xeon Gold 6152 and 384 GiB of memory.
The two nodes each contain a Mellanox ConnectX-5 (100Gbps) and are connected through a 100 Gbps switch. All measurements
have been performed using RoCE.


\begin{figure}[h]
\includegraphics[width=1\textwidth]{send-lat-msgsize.png}
\caption{Evaluation of the Send Receive latency between two nodes with varying message sizes}
\label{fig:plot-sndrcv-lat}
\end{figure}

\subsubsection{Latency} 
Figure \ref{fig:plot-sndrcv-lat} shows the latency the latency of sending a message of varying sizes using
our send receive implementation. In our benchmark a single client and server perform a \emph{ping-pong}. With that 
we mean that the client initiates the communication and measures the RTT and the server mirrors all received packages. 
We then take half of this RTT as our measurement of latency.

\paragraph{} We can see that there is a constant overhead of a little more than 2 $\mu s$ and a more or less linearly 
increasing latency with increasing message size. This is exactly what we expected given our model from 
Section~\ref{sec:model}. It is however very hard to tell how exactly this overhead is made up. We expect the lion share
of this 2 $\mu s$ to be caused by the network latency $L$.\comment{Is this enough? Do we need to talk about latency a bit
more?}



\subsubsection{Bandwidth}

We established in Section~\ref{sec:model} that to get good performance we need to be able to queue enough transmission
to keep the NIC busy. The amount of such \emph{unacknowledged} outstanding messages drastically changes performance, 
but it is not a priory clear how many of these unacknowledged messages we need. 

\paragraph{} In Figure \ref{fig:plot-sndrcv-bw-unack} we show the bandwidth for varying number of unacknowledged
messages. We evaluate this for a message size of 16 bytes, so that we are definitely not limited by the device bandwidth.
\comment{I'm not sure anymore if this is the correct approach. $g_{snd}$ is larger for larger messages}
As we can see the throughout increases 
linearly until we hit an other bottleneck for 32 unacknowledged messages. We can explain this behaviour with our model
as it seems that the cost of posting a send $o_{snd}$ is about one 32th that of the delay $g_{snd}$ until the send is 
acknowledged.


\begin{figure}[]
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-unack.png}
  \caption{Bandwidth with message size of 16 bytes with varying number of unacknowledged messages}
  \label{fig:plot-sndrcv-bw-unack}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-batch.png}
  \caption{Bandwidth with message size of 16 bytes with varying batch size}
  \label{fig:plot-sndrcv-bw-batch}
\end{subfigure}
\begin{subfigure}[b]{1\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-msgsize.png}
  \caption{Evaluation of the Send Receive bandwidth between two nodes and our performance model}
  \label{fig:plot-sndrcv-bw}
\end{subfigure}
\end{figure}


\paragraph{} We established earlier we can reduce the overhead $o_{snd}$ per message by batching send requests using
doorbelling. Figure \ref{fig:plot-sndrcv-bw-batch} shows the throughout given a fixed message size of 16 bytes with 
varying batch sizes for multiple numbers of unacknowledged messages. We can see that that throughout increases more or
less linearly but we do not seem to gain any throughput with a batch size larger than 8. We also notice that we are
quickly bottleneck again by the amount of unacknowledged messages. When increasing both batch size and the number of
unacknowledged messages we seem to hit another bottleneck at a little over 1.7 Gbit/s or about 14 million requests 
per second.\comment{Check numbers. Also compare with perftest}. This now seems to be the sending device itself that 
is the bottleneck.



\paragraph{} This gives us three different configurations for the send connection. The \emph{sequential} configuration
which will only allow a single unacknowledged message, the \emph{unbatched} configuration, which allows for up to 64
unacknowledged messages, but does not batch send operations, and the \emph{batched} version which batches 8 message
in a single work request and allows for a total of 256 unacknowledged messages.

In Figure \ref{fig:plot-sndrcv-bw} we can that the bandwidth measurements for these three versions with varying message
sizes. We can clearly see the importance of having unacknowledged messages. The sequential version is nowhere near the
theoretical throughput of 100 Gbps. Our model actually predicts that the throughput scales sublinearly with message size,
which seems to fit our observations.

For the unbatched version we can clearly see that we scale linearly with the message size, as we are bottlenecked by 
the number of work request we can issue, until we seem to hit the devices network bandwidth. We are not quite able to
utilize the complete 100 Gbps, but this is to be expected and we see similar result from micorobenchmarking tools such
as perftest.

Finally the batched version seems to be limited by the device only. We see a quicker, more or less linear increase in 
bandwidth until we are limited again by the network speed.


\paragraph{} It is worth noting that we did not hit any receiver side bottlenecks in our single threaded benchmarks.
Even without receiver side batching, receiving and reposting is sufficiently fast when handling a single connection.
We can be sure of that, as we did not encounter any \emph{Reader-Not-Ready (RNR)} errors, which occur if an incoming
is unable to consume a prepared receive buffer.

\subsubsection{Multithreading}

We can see that when we have single point to point connection, we are almost always limited by the sender and the 
amount of requests we are able to issue. In practice however we usually do not have a such a simple setup, but we
need to send and receive from and to multiple different nodes. This means each nodes needs to handle multiple open
connections. 

To evaluate the performance of our Send Receive protocol with multiple open connections we again only use two nodes.  
On each node however we run $T$ threads, each thread $t_k$ opens a connection with the corresponding 
thread on the other node, giving us a total of $T$ connection sharing the same NIC. We evaluate the throughput for 
three different message sizes which had different characteristics in our single threaded evaluation: 16 bytes, 
which was heavily bound by how fast we could post send requests, 512 bytes, which was also limited by the sender
but less extreme, and 8192 bytes, which is limited by the actual device bandwidth. We do not perform any sender side
batching but allow for 64 unacknowledged messages, keeping the pipeline full.

\begin{figure}[]
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-threads.png}
  \caption{Bandwidth with varying number of threads}
  \label{fig:plot-sndrcv-bw-thread}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-srq-threads.png}
  \caption{Bandwidth with varying number of threads using SRQ}
  \label{fig:plot-sndrcv-bw-thread-srq}
\end{subfigure}
\end{figure}


Figure \ref{fig:plot-sndrcv-bw-thread} shows the total throughput of all connections with varying number of connections.
We can easily see that for large messages we keep being bottlenecked by the device bandwidth of 100 Gbit/s. For smaller
messages first increases linearly until we hit a bottleneck. \comment{This is weird and I cannot explain it..}. Interestingly
for a message size of 16 bytes we are able to send overy twice the amount of messages per second compaired to a message size
of 512 bytes. We suspect this to be caused by  NIC level optimizations for small messages 
such as inline receiving~\cite{anuj-guide} which is supported by Mellanox NICs up to about 64 bytes.

This seems to be the total throughput of the receiving NIC. This is significantly higher than our maximum throughput 
seen for a single connection, this can be attributed to the usage of multiple processing units~\cite{anuj-guide}


Figure \ref{fig:plot-sndrcv-bw-thread-srq} shows the same data when using a shared receive queue (SRQ) to share memory
between the connections. In this case we seem to be limited by the speed the receiver can repost receive buffers. We
had to limit all senders to about 1.8 Mop/s to not into RNR erros.\comment{This is actually horribel. The single reposter
actually hurt performance I think}

We later show that when using SRQs we are limited a maximum of 2 MOp/s. So even with more optimized receive buffer management
this is drastically lower than without any resource  sharing and heavily limits the performance for small messages. This seems
to be a limit of the receivers NIC, or in our model this means that $o_{nrcv}$ is significantly larger when using a 
SRQ. 

\subsubsection{Single Receiver}


\begin{figure}[]
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-n1.png}
  \caption{N:1 Bandwidth with varying number of threads}
  \label{fig:plot-sndrcv-bw-n1}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-srq-n1.png}
  \caption{N:1 Bandwidth with varying number of threads using SRQ}
  \label{fig:plot-sndrcv-bw-srq-n1}
\end{subfigure}
\end{figure}



One of the most prevalent communication pattern is the N:1 configuration, where a single server handles the messages
of multiple clients. We described early how we handle such a pattern with a single completion queue.

\paragraph{} Figure~\ref{fig:plot-sndrcv-bw-n1} shows the throughput with a single receiver and varying number of senders.
We again look at three different message sizes. Messages of size 8196 are again limited by the link speed. For smaller messages
we are limited by the receivers CPU. To prevent RNR errors we limit the sender to a stable sending rate.

For 16 byte messages we seem to be limited at around 11 Mops, while for 512 bytes we limited at around 16 Mops. We expect the
difference in sustainable message rates to be cause by the inline receives for 16 byte messages, which actually has detrimental
effects on throughput, as this causes additional overhead for the CPU which is already the bottleneck in this situation.
\comment{I'm talking out of my ass here a little. Konstantin does this sound reasonable?}

However for both smaller message sizes we see a drop in performance when further increasing the number of sender. We explain 
this by increased cache misses, caused by having to access more QPs and the linearly growing number of receive buffers.

\paragraph{} Figure~\ref{fig:plot-sndrcv-bw-srq-n1} shows the same plot while using a shared receive queue for all QPs. For
large messages we are again only limited by the link speed.

For small messages of 32 bytes or lower we see similar throughout
as without using a shared receive queue. We do however not see any performance drops when using an increasing amount of sender.
This would fit our explanation of cache misses for receive buffers, as when we are using a SRQ the number of receive buffer
stays constant. \comment{Or our reposting code without SRQ is slow? But I don't think so}


More interesting are message sizes above 32 bytes that are not limited by the link speed. Without the receive optimizations 
for very small messages we seem to be limited at 2 Mops when using SRQs. Interestingly this effect does not show up when only
one QP is using the SRQ. We expect this to be some kind of locking or atomic operation of the NIC itself.





\subsection{Conclusion}

The Send and Receive verbs are generally regarded as the simplest verbs to work with. In this section we have shown how to
build a complete data exchange protocol. We have shown that even with this rather simple operations there are multiple tuning
possibilities and that predicting performance  is not always trivial.

\paragraph{} We have shown that send-receive connections can achieve high performance. They provide our requirements of being
\emph{non-blocking}, gives us convenient \emph{interrupts}, and when using shared receive queues allows us to 
\emph{share resources} between multiple  connections.

We pointed out multiple pitfalls when developing such connections. For production implementations there needs
to be some kind of acknowledging of messages to prevent stalling the sender, which causes RNR errors and significant 
performance drops or even failures. Receive buffer management needs to be looked at very carefully to achieve good receive
side performance.

We evaluated Shared Receive Queue and while SRQs allows us to share resources they also come with significant drawbacks. At
least in our Melanox NIC there seem to optimization that allows SRQs to match the performance of not shared connections for
very small messages. We expect this to be related to inline receives. For messages above 32 bytes we where limited to 
2 Mops at the receiver side. This is nearly 10 times slower than using separate receive queues for each QP. This makes 
using SRQs unsuitable for any system that sends messages between 32 and about 6800 bytes, at which point we are again limited 
by the link speed.


\paragraph{} In the end working with Send and Receive are still the easiest verbs to work with and when there is some kind of
acknowledgement system in place they are very reliable. They give us some nice features that other connections cannot provide
us and SRQs can be good option to save resources if the throughput is good enough for the system.

