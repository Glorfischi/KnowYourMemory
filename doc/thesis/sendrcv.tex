\section{Send Receive} \label{sec:conn:send}\label{sendrcv}
\subsection{Design} \label{sendrcv-design}
The Send Receive based protocol is by far the simplest, as the send and receive verbs basically already provide the 
message passing interface we want. There are still multiple ways to implement such a protocol and a few pitfalls 
we need to address to get good performance.

\paragraph{} We presented the basic function of the send verb in Section \ref{sec:bg:send}. It allows the sender to transmit 
a message to the receiver, without any additional information, as long as the receiver has posted a receiver buffer that is 
large enough to receive the message. To transform this verb to a fully functioning connection we need two basic things. 
Some kind of \emph{receive buffer management} that allows the receiver to receive multiple messages at the same time, 
and to get any reasonable performance we need to be able to send multiple messages asynchronously.

\subsubsection{Sender} 
The sender assumes that the receiver is always ready to receive the message and has at least one receive buffer posted.
The Sender has only two essential methods:

\begin{lstlisting}  
    // Takes a registered region containing the message
    // Posts a signaled send work request to the QP
    // Returns the unique wr_id associated with the WR 
    uint64_t SendAsync(SendRegion reg);

    // Polls the CQ until there is completion event 
    // associated with the given id.
    void Wait(uint64_t id);

\end{lstlisting}

This allows us to queue multiple messages before waiting for them to complete. In practice it is very important to keep the
sending pipeline full to get good performance. This kind of \emph{unacknowledged messages} can increase throughout by a factor
of 5 or more.


\paragraph{} We are able to wait for a specific work request to complete by issuing monotonically increasing IDs. We store the
last ID with a matching completion event. If we try to wait on a lower ID we know we already received a matching completion 
event prior, because of the in order guarantees of RDMA.\comment{Maybe cite something?} If the provided ID is higher then the
last seen ID, we will poll the completion queue until we receive one with a matching \code{wr\_id}.

This approach will eventually lead to an overflow, but given our observed maximum message rate we do not expect to see this 
happen in the next 10 thousand years.


\paragraph{Batching} As briefly mentioned in Section \ref{sec:bg:verbs} we can post multiple work request at the same time. 
This \emph{Doorbell batching} reduces the number of generated MMIOs~\cite{anuj-guide}, reduces the CPU load and can in turn 
drastically improve bandwidth if the connection is send-side CPU bottleneck. We observed a performance increase by up to 
a factor of two. It is however questionable if this connection level batching is the right approach when using the 
\emph{Reliable Connection} that we evaluated. With RC all messages need to end up at the same receiver meaning application 
level batching should be possible and we observed that sending larger messages will outperform sending small messages in 
batches.

\paragraph{Inline Sending} \todo{We should probably add some kind of benchmarks for this.}

\subsubsection{Receiver}

The receiver always needs to have at least one posted receive buffer. We achieve this by having an array of posted receive 
buffers. When receiving a new message we return the corresponding receive buffer. As soon as the application is done processing
the message, it will have to mark it as free which will repost the corresponding buffer.

\begin{lstlisting}  
    // Waits for the next CQE on the receive CQ
    // Returns the matching receive region
    ReceiveRegion Receive();

    // Marks the given receive region as free
    // to be reused and will post a receive
    // request for this region
    void Free(ReceiveRegion reg);

\end{lstlisting}

Slow processing of messages can still stall the sender, but by having multiple receive buffer it allows us to have numerous 
\emph{in-flight messages} which improves performance drastically.

\paragraph{} It is important that we note here that in production systems there needs to be a way for the sender to whether 
enough receive buffers are ready. If there is no posted receive buffer when a message is received the receiver will generate
a so called \emph{reader not ready} error. This will either cause a large back off for the sender or even cause the connection
to break.

We observed this problem specifically for N:1 communication. This can be mitigated to some extend by optimizing receiving and
reposting buffers through batching.

\paragraph{Batching} As described there are significant penalties if the receiver is not able to keep up and stalls the sender.
This can be mitigated a little optimizing the receiver. For one instead of polling one receive CQ at a time we poll up to 32
at a time into a array of CQEs. We observed in microbenchmarks that this can improve the observed throughput by nearly half. By
batching the receive work requests we can further improve throughput similarly to batching send requests.

\paragraph{Shared Receive Queues} By having an array of $k$ posted receive buffers for each receiver the memory reserved for 
receiving messages can grow quite large. If a node has an open connection to $N$ other nodes it will need to reserve 
$N*k*max\_msg\_size$ Bytes, even if the total request volume is quite small (i.e. we expect only burst of $k$ messages but 
from different nodes at different times).

We can reduce the total memory usage by using \emph{Shared Receive Queues (SRQ)}. As the name already tells us SRQs allow us
share receive queues between multiple QPs. This means we can reuse a single group of receive queues for multiple connections.
This means the total memory usage does not grow with $N$.

Its worth noting that the usage of SRQs does not impact completion queues. The completion event for consuming a posted receive
buffer still ends up in the CQ of the corresponding QP.

One major change with using an SRQ is that we do not submit receive work request to each of the QPs but to the single SRQ. This
means that we will either have to introduce locking to access the SRQ, which introduces a significant performance penalty or 
delegate the posting of the receive buffer to a single thread. Instead of reposting the buffer themselves each connection 
enqueues the id to the to be posted buffer to a queue. The single reposting thread will dequeue it and repost it. This also
allows us to have central batching for reposting the buffers.

\paragraph{Single Receiver} It is very common to have an N:1 communication pattern where a single server receives messages 
from multiple clients. This could be achieved by simply round-robin over the $N$ connections. For this connection however 
we used the fact that we can associate a singe Completion Queue with multiple Queue Pairs. This means if we are in a
\emph{single receiver mode} all receive completion events will end up in a single CQ. Allowing us to poll a single queue to
receive a message from $N$ different sender.

Especially if we are not using an SRQ we will have to know by which QP this message was received by as we will need to repost
the buffer to it. We can get the 32 bit QP number from the Completion Queue Event, which identifies the Queue Pair. 

\subsection{Evaluation}

We ran all our evaluations on two machines running CentOS 7 containing two Intel Xeon Gold 6152 and 384 GiB of memory.
The two nodes each contain a Mellanox ConnectX-5 (100Gbps) and are connected through a 100 Gbps switch. All measurements
have been performed using RoCE.


\subsubsection{Model Parameters}

\todo{This should be the spot to evaluate the parameters}

We are using the model presented in Section \ref{sec:perf-model}, with the specific protocol 
we can look at what each of these parameters represents and allows us to at least give a qualitative understanding.


\begin{itemize}
  \item $L$: represents the latency of moving a byte from the senders memory to the receivers memory. It includes the 
    network latency as well as two DMA access latencies.
  \item $G$: is the bandwidth between the sender and receiver, which we expect to be equal to the maximum network bandwidth.
  \item $g_{snd}$: the minimum time interval between consecutive message transmissions. In this case we expect this to be 
    equal to polling the \emph{send completion queue}. If we do not allow for sufficient unacknowledged messages, this will
    also include waiting for these messages to be acknowledged.
  \item $o_{snd}$: the \emph{send overhead} for the CPU. This will be the time of posting a send request.
  \item $o_{nsnd}$: the \emph{send NIC overhead}, which is the overhead of sending a single message for the NIC.
  \item $o_{nrcv}$: the \emph{receive NIC overhead}. This represents the overhead of receiving a single message and the cost of 
    consuming a receive buffer and generating a \emph{WQE}.
  \item $o_{rcv}$: the \emph{receive overhead} for the CPU. This represents the overhead of polling the \emph{CQ}.
  \item $g_{rcv}$: the \emph{receive gap}. In this case we expect this to represent the cost of reposting a receive buffer.
\end{itemize}


In practice it is fairly hard to asses some of these parameters. \todo{some sentences on what we would expect}


\begin{figure}[h]
\includegraphics[width=1\textwidth]{send-lat-msgsize.png}
\caption{Evaluation of the Send Receive latency between two nodes with varying message sizes}
\label{fig:plot-sndrcv-lat}
\end{figure}

\subsubsection{Latency} 
Figure \ref{fig:plot-sndrcv-lat} shows the latency the latency of sending a message of varying sizes using
our send receive implementation. In our benchmark a single client and server perform a \emph{ping-pong}. With that 
we mean that the client initiates the communication and measures the RTT and the server mirrors all received packages. 
We then take half of this RTT as our measurement of latency.

\paragraph{} We can see that there is a constant overhead of a little more than 2 $\mu s$ and a more or less linearly 
increasing latency with increasing message size. This is exactly what we expected given our model from 
Section~\ref{sec:model}. It is however very hard to tell how exactly this overhead is made up. We expect the lion share
of this 2 $\mu s$ to be caused by the network latency $L$.\comment{Is this enough? Do we need to talk about latency a bit
more?}



\subsubsection{Bandwidth}

We established in Section~\ref{sec:model} that to get good performance we need to be able to queue enough transmission
to keep the NIC busy. The amount of such \emph{unacknowledged} outstanding messages drastically changes performance, 
but it is not a priory clear how many of these unacknowledged messages we need. 

\paragraph{} In Figure \ref{fig:plot-sndrcv-bw-unack} we show the bandwidth for varying number of unacknowledged
messages. We evaluate this for a message size of 16 bytes, so that we are definitely not limited by the device bandwidth.
\comment{I'm not sure anymore if this is the correct approach. $g_{snd}$ is larger for larger messages}
As we can see the throughout increases 
linearly until we hit an other bottleneck for 32 unacknowledged messages. We can explain this behaviour with our model
as it seems that the cost of posting a send $o_{snd}$ is about one 32th that of the delay $g_{snd}$ until the send is 
acknowledged.


\begin{figure}[]
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-unack.png}
  \caption{Bandwidth with message size of 16 bytes with varying number of unacknowledged messages}
  \label{fig:plot-sndrcv-bw-unack}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-batch.png}
  \caption{Bandwidth with message size of 16 bytes with varying batch size \todo{redo measurement for 64 unack}}
  \label{fig:plot-sndrcv-bw-batch}
\end{subfigure}
\begin{subfigure}[b]{1\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-msgsize.png}
  \caption{Evaluation of the Send Receive bandwidth between two nodes and our performance model}
  \label{fig:plot-sndrcv-bw}
\end{subfigure}
\end{figure}


\paragraph{} We established earlier we can reduce the overhead $o_{snd}$ per message by batching send requests using
doorbelling. Figure \ref{fig:plot-sndrcv-bw-batch} shows the throughout given a fixed message size of 16 bytes with 
varying batch sizes for multiple numbers of unacknowledged messages. We can see that that throughout increases more or
less linearly but we do not seem to gain any throughput with a batch size larger than 8. We also notice that we are
quickly bottleneck again by the amount of unacknowledged messages. When increasing both batch size and the number of
unacknowledged messages we seem to hit another bottleneck at a little over 1.7 Gbit/s or about 14 million requests 
per second.\comment{Check numbers. Also compare with perftest}. This now seems to be the sending device itself that 
is the bottleneck.



\paragraph{} This gives us three different configurations for the send connection. The \emph{sequential} configuration
which will only allow a single unacknowledged message, the \emph{unbatched} configuration, which allows for up to 64
unacknowledged messages, but does not batch send operations, and the \emph{batched} version which batches 8 message
in a single work request and allows for a total of 256 unacknowledged messages.

In Figure \ref{fig:plot-sndrcv-bw} we can that the bandwidth measurements for these three versions with varying message
sizes. We can clearly see the importance of having unacknowledged messages. The sequential version is nowhere near the
theoretical throughput of 100 Gbps. Our model actually predicts that the throughput scales sublinearly with message size,
which seems to fit our observations.

For the unbatched version we can clearly see that we scale linearly with the message size, as we are bottlenecked by 
the number of work request we can issue, until we seem to hit the devices network bandwidth. We are not quite able to
utilize the complete 100 Gbps, but this is to be expected and we see similar result from micorobenchmarking tools such
as perftest.

Finally the batched version seems to be limited by the device only. We see a quicker, more or less linear increase in 
bandwidth until we are limited again by the network speed.


\paragraph{} It is worth noting that we did not hit any receiver side bottlenecks in our single threaded benchmarks.
Even without receiver side batching, receiving and reposting is sufficiently fast when handling a single connection.
We can be sure of that, as we did not encounter any \emph{Reader-Not-Ready (RNR)} errors, which occur if an incoming
is unable to consume a prepared receive buffer.

\subsubsection{Multithreading}

We can see that when we have single point to point connection, we are almost always limited by the sender and the 
amount of requests we are able to issue. In practice however we usually do not have a such a simple setup, but we
need to send and receive from and to multiple different nodes. This means each nodes needs to handle multiple open
connections. 

To evaluate the performance of our Send Receive protocol with multiple open connections we again only use two nodes.  
On each node however we run $T$ threads, each thread $t_k$ opens a connection with the corresponding 
thread on the other node, giving us a total of $T$ connection sharing the same NIC. We evaluate the throughput for 
three different message sizes which had different characteristics in our single threaded evaluation: 16 bytes, 
which was heavily bound by how fast we could post send requests, 512 bytes, which was also limited by the sender
but less extreme, and 8192 bytes, which is limited by the actual device bandwidth. We do not perform any sender side
batching but allow for 64 unacknowledged messages, keeping the pipeline full.

\begin{figure}[]
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-threads.png}
  \caption{Bandwidth with varying number of threads}
  \label{fig:plot-sndrcv-bw-thread}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-srq-threads.png}
  \caption{Bandwidth with varying number of threads using SRQ}
  \label{fig:plot-sndrcv-bw-thread-srq}
\end{subfigure}
\end{figure}


Figure \ref{fig:plot-sndrcv-bw-thread} shows the total throughput of all connections with varying number of connections.
We can easily see that for large messages we keep being bottlenecked by the device bandwidth of 100 Gbit/s. For smaller
messages first increases linearly until we hit a bottleneck. \comment{This is weird and I cannot explain it..}. Interestingly
for a message size of 16 bytes we are able to send overy twice the amount of messages per second compaired to a message size
of 512 bytes. We suspect this to be caused by  NIC level optimizations for small messages 
such as inline receiving~\cite{anuj-guide} which is supported by Mellanox NICs up to about 64 bytes.

This seems to be the total throughput of the receiving NIC. This is significantly higher than our maximum throughput 
een for a single connection, this can be attributed to the usage of multiple processing units~\cite{anuj-guide}


Figure \ref{fig:plot-sndrcv-bw-thread-srq} shows the same data when using a shared receive queue (SRQ) to share memory
between the connections. In this case we seem to be limited by the speed the receiver can repost receive buffers. We
had to limit all senders to about 1.8 Mop/s to not into RNR erros.\comment{This is actually horribel. The single reposter
actually hurt performance I think}

We later show that when using SRQs we are limited a maximum of 2 MOp/s. So even with more optimized receive buffer management
this is drastically lower than without any resource  sharing and heavily limits the performance for small messages. This seems
to be a limit of the receivers NIC, or in our model this means that $o_{nrcv}$ is significantly larger when using a 
SRQ. 

\subsubsection{Single Receiver}


\begin{figure}[]
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-n1.png}
  \caption{N:1 Bandwidth with varying number of threads}
  \label{fig:plot-sndrcv-bw-n1}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-srq-n1.png}
  \caption{N:1 Bandwidth with varying number of threads using SRQ}
  \label{fig:plot-sndrcv-bw-srq-n1}
\end{subfigure}
\end{figure}



One of the most prevalent communication pattern is the N:1 configuration, where a single server handles the messages
of multiple clients.

\paragraph{} w/o SRQ: Needed limiter for both 16 and 512 bytes. Both get slower for more connections. Either cause by ouri
repost implementation or cache?


\paragraph{}SRQ: needed limiter for 16 byte. Seems to have same performance as 16 byte without srq. Related to inline receive? Some
kind of optimization. 512 drops significantly and stays at 2 Mops. 8192 limited by link




\subsection{Result}

SRQ not suited for msgsize between 32 and 6800 bytes. There seem to be optimization below and above we are limite by link

