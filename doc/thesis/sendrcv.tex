\section{Send Receive} \label{sec:conn:send}\label{sendrcv}
\subsection{Design} \label{sendrcv-design}
\todo{Description of the connection}



\begin{enumerate}
  \item We start a transfer by copying a \emph{Work Request} to the \emph{NIC} using Memory Mapped IO (MMIO). This gives
    us constant overhead $o_{snd}$ independent of the size of the message.
  \item Given the information in the \emph{Work Request} the \emph{NIC} then accesses the messages payload using DMA. This 
    might generate more than one PCIe transaction \cite{atc16-kalia}. The payload is then sent over the network.
  \item As soon as the receiver receives the first segment it will consume posted receive buffer, write incoming payload 
    to the receive buffer and generate a \emph{Work Completion Event (WQE)}. We will call the combined overhead of consuming a receive
    buffer and generating a WQE $o_{rcv}$, which is also independent from $k$.
  \item At last the receiving CPU will have to poll the \emph{Completion Queue (CQ)}. This will again give us a constant 
    overhead of $o_{cq}$.
\end{enumerate}

\paragraph{Inline Sending}
\todo{Describe inline sending. How does our performance model change?}
\todo{What about inline receiving?}
\paragraph{Shared Receive Queues}
\todo{Look at srq. Talk about congestion when reposting buffers?}

\subsection{Evaluation}

\subsubsection{Model Parameters}

We are using the model presented in Section \ref{sec:perf-model}, with the specific protocol 
we can look at what each of these parameters represents and allows us to at least give a qualitative understanding.


\begin{itemize}
  \item $L$: represents the latency of moving a byte from the senders memory to the receivers memory. It includes the 
    network latency as well as two DMA access latencies.
  \item $G$: is the bandwidth between the sender and receiver, which we expect to be equal to the maximum network bandwidth.
  \item $g_{snd}$: the minimum time interval between consecutive message transmissions. In this case we expect this to be 
    equal to polling the \emph{send completion queue}. If we do not allow for sufficient unacknowledged messages, this will
    also include waiting for these messages to be acknowledged.
  \item $o_{snd}$: the \emph{send overhead} for the CPU. This will be the time of posting a send request.
  \item $o_{nsnd}$: the \emph{send NIC overhead}, which is the overhead of sending a single message for the NIC.
  \item $o_{nrcv}$: the \emph{receive NIC overhead}. This represents the overhead of receiving a single message and the cost of 
    consuming a receive buffer and generating a \emph{WQE}.
  \item $o_{rcv}$: the \emph{receive overhead} for the CPU. This represents the overhead of polling the \emph{CQ}.
  \item $g_{rcv}$: the \emph{receive gap}. In this case we expect this to represent the cost of reposting a receive buffer.
\end{itemize}


In practice it is fairly hard to asses some of these parameters. \todo{some sentences on what we would expect}


\begin{figure}[h]
\includegraphics[width=1\textwidth]{send-lat-msgsize.png}
\caption{Evaluation of the Send Receive latency between two nodes and our performance model}
\label{fig:plot-sndrcv-lat}
\end{figure}

\subsubsection{Latency} \todo{filler}. For point to point latency, our model basically reduces to the $\alpha$-$\beta$ model. We
have the bandwidth $G$ and a bunch of overheads that are unrelated to the size of the message $k$.

$$
t \geq o_{snd} + o_{nsnd}  + (k-1)G + L + o_{nrcv} + o_{rcv}
$$

Figure \ref{fig:plot-sndrcv-lat} shows the latency the latency of sending a message of varying sizes using our send receive 
implementation. In our benchmark a single client and server 
perform a \emph{ping-pong}. With that we mean that the client initiates the communication and measures the RTT and the server
mirrors all received packages. We then take half of this RTT as our measurement of latency.

\todo{SRQ?}

Figure \ref{fig:plot-sndrcv-lat} also evaluates or model. Given that there is quite a lot of noise, our simplified model fits
the observed data well. \todo{Except it doesn't really.. DARE uses a different B for messages over MTU. Why? Would this 
fix the model?}. 

\subsubsection{Bandwidth}
\todo{filler, what do these terms actually mean?}


$$
bw \leq \max ( \frac{k}{o_{snd} + g_{snd}}, \frac{k}{o_{nsnd} + (k-1)G}, \frac{k}{o_{nrcv} + (k-1)G}, \frac{k}{o_{rcv} + g_{rcv}})
$$

So for our model comes down to finding the bottleneck. Our first finding has to be that if we do not allow for multiple 
unacknowledged messages, we are severely bottleneck by $\frac{k}{o_{snd} + g_{snd}}$ as $g_{snd} > 2L$. Figure 
\ref{fig:plot-sndrcv-bw-unack} shows this very well. Given a fixed message size of 16 bytes and no batching, the measured 
throughput increases linearly until we allow for about 32 unacknowledged messages. Afterwards performance does not further 
increase as we are bottleneck by something else. \todo{handwavy explanation why we now expect 32 to be enough for larger 
message sizes}

\begin{figure}[h]
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-unack.png}
  \caption{Bandwidth with message size of 16 bytes with varying number of unacknowledged messages}
  \label{fig:plot-sndrcv-bw-unack}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-batch.png}
  \caption{Bandwidth with message size of 16 bytes with varying batch size \todo{redo measurement for 64 unack}}
  \label{fig:plot-sndrcv-bw-batch}
\end{subfigure}
\end{figure}


\todo{Reference final bw graph with three benchmarks. Without unack, with unack, with unack and batch. As well as our model}

We can clearly see that we are sill bottlenecked by something other then the device and network. We can rule out that we are
bottlenecked by the receiver by setting \texttt{rnr\_retry\_count} to zero, which means that if any incoming messages is unable
to find a posted receive buffer, it will fail and the sender will not retry it. This means we will immediately notice if 
the receiver cannot keep up. At least for a simple 1:1 communication this does not seem to be the case.

This leaves us with the overhead $o_{snd}$ that seems to bottleneck us for smaller messages. We can reduce this overhead by 
posting multiple send requests in a batch. \todo{I remember some paper explicitly saying that batching for RC is not reasonable
as the messages have the same destination. Find this and write something why this can still be helpful in message broker like
situations where a single thread handles messages for multiple threads. Or other situations where application level batching 
is not practical}

Figure \ref{fig:plot-sndrcv-bw-batch} shows the throughout given a fixed message size of 16 bytes with varying batch sizes for
multiple numbers of unacknowledged messages. We can see that that throughout increases more or less linearly but we do not seem
to gain any throughput with a batch size larger than 8. We also notice that we are quickly bottleneck again by the amount of 
unacknowledged messages. When increasing both batch size and the number of unacknowledged messages we seem to hit
another bottleneck.

With adjusted batch size of 8 and a maximum number of unacknowledged messages of 256 we now seem to bottlenecked by the 
device itself. In Figure \ref{fig:plot-sndrcv-bw} we can that the measurements for the batched connections align fairly 
well with what our model predicts when we are bottlenecked by the sending device. \todo{Except it doesn't. What am I doing wrong?}

\begin{figure}[h]
\includegraphics[width=1\textwidth]{send-bw-msgsize.png}
\caption{Evaluation of the Send Receive bandwidth between two nodes and our performance model}
\label{fig:plot-sndrcv-bw}
\end{figure}


\subsubsection{Multithreading}

We can see that when we have single point to point connection, we are almost always limited by the sender and the 
amount of requests we are able to issue. In practice however we usually do not have a such a simple setup, but we
need to send and receive from and to multiple different nodes. This means each nodes needs to handle multiple open
connections.

To evaluate the performance of our Send Receive protocol with multiple open connections we again us two nodes. Each 
running \todo{Specs}. On each node we run $T$ threads, each thread $t_k$ opens a connection with the corresponding 
thread on the other node, giving us a total of $T$ connection sharing the same NIC. We evaluate the throughput for 
three different message sizes which had different characteristics in our single threaded evaluation: 16 bytes, 
which was heavily bound by how fast we could post send requests, 512 bytes, which was also limited by the sender
but less extreme, and 8192 bytes, which is limited by the actual device bandwidth. We do not perform any sender side
batching but allow for 64 unacknowledged messages, keeping the pipeline full.


Figure \ref{fig:plot-sndrcv-bw-thread} shows the total throughput of all connections with varying number of connections.
We can easily see that for large messages we keep being bottlenecked by the device bandwidth of 100 Gbit/s. For smaller
messages first increases linearly until we hit a bottleneck of ?? MOp/s. This seems to be the total throughput of the 
receiving NIC \todo{Or sending?}. This is significantly higher than our maximum throughput seen for a single connection,
this can be attributed to the usage of multiple processing units~\cite{}


Figure \ref{fig:plot-sndrcv-bw-thread-srq} shows the same data when using a shared receive queue (SRQ) to share memory
between the connections. It shows that all receivers sharing an SRQ are limited by it to a maximum of 2 MOp/s. This is 
drastically lower than without any resource sharing and heavily limits the performance for small messages. This seems
to be a limit of the receivers NIC, or in our model this means that $o_{nrcv}$ is significantly larger when using a 
SRQ. \todo{This does not really fit with what we observe in the microbenchmark. How do we explain this?}


\subsubsection{Single Receiver}

One of the most prevalent communication pattern is the N:1 configuration, where a single server handles the messages
of multiple clients.

\subsection{Result}


