\section{Direct Write}\label{sec:conn:direct_write}

\subsection{Design} \label{sendrcv-design}
The idea of the \emph{Unbuffered} or \emph{Direct Write Protocol} is to avoid unnecessary copying we have when using
a buffered write connection with a ring buffer. We can achieve this by having the receiver specify specific memory 
regions for the sender to write to. 

\paragraph{} For this thesis we implemented something very reminiscent of the send receive protocol where the receiver 
posts receive buffer by sending the corresponding metadata to the sender, which will use these buffers in order. This does
not really make total use of this protocol as one would still need to copy the received message out of these buffers. This
could however fairly easily be extended by sending additional information with receive buffer, allowing the sender to write
the message to the correct buffer.\comment{not sure if this is clear / how to correctly explain this}


\begin{figure}[!htb]
\begin{center}
\begin{tikzpicture}[node distance=2cm,auto,>=stealth', minimum width=.75cm,minimum height=.5cm]
  \draw[rounded corners] (-.5, -3) rectangle (-4.5, 3) {};
  \node[align=center] at (-2.5, 2.5) {Sender};
  \draw[rounded corners] (.5, -3) rectangle (4.5, 3) {};
  \node[align=center] at (2.5, 2.5) {Receiver};

  \node [fill=white] at (-2.5, 1.5) (A) {\tiny addr};
  \node [anchor=west, fill=white] at (A.east) (A1) {\tiny rkey};
  \node [anchor=west, minimum width=.25cm, fill=white] at (A1.east) (A2) {\tiny v};

  \node [anchor=north, fill=black!35] at (A.south) (B) {\tiny a3};
  \node [anchor=west, fill=black!35] at (B.east) (B1) {\tiny 1};
  \node [anchor=west, minimum width=.25cm, fill=black!35] at (B1.east) (B2) {\tiny 0};

  \node [anchor=north, fill=black!35] at (B.south) (C) {\tiny b5};
  \node [anchor=west, fill=black!35] at (C.east) (C1) {\tiny 1};
  \node [anchor=west, minimum width=.25cm, fill=black!35] at (C1.east) (C2) {\tiny 0};

  \node [anchor=north, fill=black!05] at (C.south) (D) {\tiny c8};
  \node [anchor=west, fill=black!05] at (D.east) (D1) {\tiny 1};
  \node [anchor=west, minimum width=.25cm, fill=black!05] at (D1.east) (D2) {\tiny 1};
  \draw [<-,black!40!blue] (D.west) --  +(-.5,0)
        node [black!40!blue,left,inner xsep=.2, draw=none] (Tail) {\tiny next send};


  \node [anchor=north, fill=black!05] at (D.south) (E) {\tiny d2};
  \node [anchor=west, fill=black!05] at (E.east) (E1) {\tiny 1};
  \node [anchor=west, minimum width=.25cm, fill=black!05] at (E1.east) (E2) {\tiny 1};

  \node [anchor=north, fill=black!35] at (E.south) (F) {\tiny f3};
  \node [anchor=west, fill=black!35] at (F.east) (F1) {\tiny 1};
  \node [anchor=west, minimum width=.25cm, fill=black!35] at (F1.east) (F2) {\tiny 0};
  \draw [<-,black!40!green] (F2.east) --  +(.5,0)
        node [black!40!green,right,inner xsep=.2, draw=none] (head) {\tiny next free};



  \node [fill=black!35, minimum width=2cm] at (2.75, 1.5) (rB) {};
  \node [anchor=east, fill=white, minimum width=.5cm] at (rB.west) (rB1) {\tiny a3};
  \node [anchor=west, fill=black!35, minimum width=.5cm] at (rB.east) (rB2) {\tiny 1};

  \node [anchor=north, fill=black!35, minimum width=2cm, outer ysep=2] at (rB.south) (rC) {};
  \node [anchor=east, fill=white, minimum width=.5cm] at (rC.west) (rC1) {\tiny b5};
  \node [anchor=west, fill=black!35, minimum width=.5cm] at (rC.east) (rC2) {\tiny 1};

  \node [anchor=north, fill=black!05, minimum width=2cm, outer ysep=2] at (rC.south) (rD) {};
  \node [anchor=east, fill=white, minimum width=.5cm] at (rD.west) (rD1) {\tiny c8};
  \node [anchor=west, fill=black!05, minimum width=.5cm] at (rD.east) (rD2) {\tiny 0};

  \node [anchor=north, fill=black!05, minimum width=2cm, outer ysep=2] at (rD.south) (rE) {};
  \node [anchor=east, fill=white, minimum width=.5cm] at (rE.west) (rE1) {\tiny d2};
  \node [anchor=west, fill=black!05, minimum width=.5cm] at (rE.east) (rE2) {\tiny 0};

  \node [anchor=north, fill=black!35, minimum width=2cm, outer ysep=2] at (rE.south) (rF) {};
  \node [anchor=east, fill=white, minimum width=.5cm] at (rF.west) (rF1) {\tiny f3};
  \node [anchor=west, fill=black!35, minimum width=.5cm] at (rF.east) (rF2) {\tiny 1};

\end{tikzpicture}
\end{center}
\caption{Resources of the Direct Send Connection}
\label{fig:dirwrite-resources}
\end{figure}

Figure~\ref{fig:dirwrite-resources} is a representation of the data structures involved in the direct write 
connection. In the following subsections we will walk tough the steps involved in sending and receiving a message and
will reference these data structures.

\subsubsection{Sender}

The sender has an array of structs, which we will call \code{BufferInfo}, in local memory. This array is populated by the 
receiver and represent receive buffer to which the sender is allowed to write to. The struct contains three fields: 
\code{addr}, which is the virtual address of the corresponding buffer in the receivers memory, \code{rkey}, which is the 
rkey of that buffer, and \code{valid} which indicates whether this entry is actually valid.

\paragraph{} This array acts as a ring buffer, so when sending a message, the sender will look at the next field. If it is
valid it will send the message to the corresponding buffer at the receiver and invalidates it. We will always send the complete
buffer followed by a signaling byte, which will allow the receiver to poll on this byte to notice incoming messages.

\paragraph{} Our implementation of the sender is very simple, but it can easily be extended to be more sophisticated by adding
tags to the \code{BufferInfo} or by having multiple different arrays, which would allows us to \emph{route} the message to the
correct buffer at the sender and potentially eliminate the need for a further copy at the receiver. One could also change
the way of signaling an incoming message by switching to write with immediate or by applying a metadata approach as implemented
for the buffered write protocol.

\subsubsection{Receiver}

In a way the direct write receiver works in a similar way to the send and receive receiver. It can \code{Receive()} and 
\code{Free()} buffers. 

\paragraph{}\emph{Freeing} a buffer means making it available for the sender to write to. As we established at the sender 
there is a queue of \code{BufferInfo}. To \emph{free} a buffer we will first set the very last signal byte to 0, then we
write its address, rkey, and a valid byte of 1 to the next empty or invalidated queue entry. The receiver will keep a very 
similar local queue to keep track of the oder of posted buffers.

\paragraph{} To \emph{Receive} a buffer the receiver polls on the signal bit of the oldest freed buffer. This will be set
to one when the transmission is completed. We are again using the fact that RDMA updates memory in increasing memory order, 
for any modern systems~\cite{herd, farm}.

\subsection{Evaluation}


\begin{figure}[htp]
  \centering
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-direct-lat-msgsize.png}
  \caption{Latency}
  \label{fig:plot-wdir-lat}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-direct-bw-msgsize.png}
  \caption{Bandwidth}
  \label{fig:plot-wdir-bw}
\end{subfigure}
\caption{Measurements between two nodes with varying message size}
\end{figure}
\subsubsection{Latency}

Figure~\ref{fig:plot-wdir-lat} shows the latency of our direct write connections with varying message size. We
also plot the Send Receive latency for reference. We see that latency wise both approaches yield very similar results.

\paragraph{} There seems to be a constant overhead of about $2.25 \mu s$ and then a linear increase in latency with 
increasing message size. This in line with what we observed for previous connections and aligns with our model.

\subsubsection{Bandwidth}

Figure~\ref{fig:plot-wdir-bw} shows the point to point throughput for different message sizes. Again for reference we also
plot the Send Receive bandwidth. We allowed for $64$ unacknowledged messages for both connections to ensure a full pipline.
We do not see any performance improvements with an increased number of unacknowledged messages.

\paragraph{} We see the same linearly increase in bandwidth we observed in nearly every connection, indicating that we limited
by a per message overhead until we hit the maximum link speed for large messages. Interestingly, compared to the Send Receive
connection, we achieve significantly lower throughput for medium sized messages. 

\paragraph{} We ensured that we are not limited by the receiver, so this still is a sender side limitation. It seems we are
actually limited by the sending NIC or possibly by the PCIe bus, as the CPU needs to wait for the operation to be completed 
and we are not limited by our ability to issue work requests like we are for the Send Receive connection.

We suspect that the large amount of returning writes interferes with outgoing writes and increases the per message overhead
for the NIC. \comment{I'm in no way sure if this is correct. I made some quick hacks to check that we actually routinely need
  to wait multiple polls to receive a completion event, so I believe that this is a NIC bottleneck, but I don't know whether
this explanation is reasonable}
  

\subsubsection{Multithreading}

Figure~\ref{fig:plot-wdir-bw-threads} shows the protocols performance with multiple open connections. We still use two nodes,
but each nodes spawns $T$ threads, which each open a separate connection. Each connections will share the same sending and 
receiving NIC.

\paragraph{} We again evaluated the throughput of three different message sizes: 16 bytes, 512 bytes, and 8192 bytes. We can see that 
although we are already limited by the NIC throughput for a single connection, we still see linear performance improvements 
for smaller messages. This can be explained by the more efficient usage of NIC processing units by using multiple 
QPs~\cite{anuj-guide}.

\paragraph{} When using 8192 byte messages, we are again quickly limited by the maximum link speed. However when using 
512 byte messages we seem to bottleneck at around 70 Gbit/s, which is less then the 80 Gbit/s maximum we saw for both the
Send Receive connection as well as the Buffer Read connection. This is more or less then same performance we saw for the
Write Offset connections in Figure~\ref{fig:plot-write-bw-thread-512}. This reinforces our assumption that the returning write
for each message is impacting our throughput, as the Write Offset connection also needs to issue two writes per message.

\begin{figure}[ht]
  \begin{subfigure}[b]{0.48\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-direct-bw-threads.png}
  \caption{Using $N$ receivers}
  \label{fig:plot-wdir-bw-threads}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-direct-bw-n1.png}
  \caption{Using a single receiver}
  \label{fig:plot-wdir-bw-n1}
  \end{subfigure}
  \caption{Bandwidth with varying number of connections $N$}
\end{figure}

\subsubsection{Single Receiver}

Figure~\ref{fig:plot-wdir-bw-n1} shows the protocols throughput with a single receiver and varying number of senders.
We again look at three different message sizes. 

\paragraph{} In this situation we are almost always very quickly limited by the receiving CPU. Very large messages achieve the maximum 
line rate,  while smaller messages seem to be limited by the reposting speed of the receiver at around 4 MOp/s, which is 
already a bottleneck with two open connections.

\paragraph{} This bottleneck gives us a performance caps for 512 byte messages at around 15 Gbit/s and at 0.5 Gbit/s for 16 byte messages.
There seems to be a very slight reduction in throughput when further increasing the number of open connections, which we 
attribute to increased cache misses.

\subsection{Conclusion}

With our Direct Write connection we essentially rebuilt the Send and Receive verbs using only RDMA Writes. This gives us more
control over the protocol, which could allow us to extend it for specific systems.

\paragraph{} By adding metadata when posting a buffer this could enable \emph{True Zero Copy} capabilities for certain 
applications and with a more sophisticated buffer management one could more effectively utilizes the available memory by 
posting different size messages. The protocol also keeps the \emph{fairness} guarantees from the Send Receive connection. 
This gives us a very versatile protocol that can be adapted when necessary.

\paragraph{} But our benchmarking also showed that it is not trivial to achieve the same performance we got using the Send
and Receive verb. The general wisdom of RDMA writes being faster was not confirmed in our case. Our protocol can 
definitely be further optimized, by reposting buffers in batches or by redesigning this reposting entirely to reduce
the number of returning writes. But it showed that achieving good performance with such a protocol requires significant 
optimization work.







