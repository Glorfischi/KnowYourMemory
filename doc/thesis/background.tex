\section{RDMA} \label{sec:rdma}

Remote Direct Memory Access (RDMA) is a network mechanism that allows moving buffers between applications over the network.
The main difference to traditional network protocols like TCP/IP is that it is able to completely bypass the hosts kernel
and even circumvents the CPU for data transfer. This allows applications using RDMA to achieve latencies as low as 2 $\mu s$
and throughputs of up to 100 $Gbit/s$, all while having a smaller CPU footprint.


\paragraph{} While initially developed as part of the \emph{InfiniBand} network protocol, which completely replaces the OSI 
layers and is primarily used in high performance computing, RDMA is also available for commodity Ethernet using 
\emph{Internet Wide Area RDMA (iWARP)} or \emph{RDMA over Converged Ethernet (RoCE)}.


\paragraph{} So to be able to use RDMA we need to have a network interface card that implements one of these technologies. 
From now on, we will call such a card an RNIC (RDMA Network Interface Card). A good RNIC implements all logic needed for 
the RDMA protocol in hardware including flow control, reliability, and segmentation.


\begin{figure}[!htp]
\begin{center}
\begin{tikzpicture}[>=latex,every node/.style={minimum width=4cm,minimum height=1cm, outer sep=7pt,draw=black,semithick, scale=0.6}]

  \draw[rounded corners] (-2, -4) rectangle (1.5, 0.5) {};
  \node [] at (0,0) (A) {};
  \node [draw=none,, minimum width=2.75cm, anchor=east] at (A.east) (A0) {Application};
  \node [anchor=north] at (A.south) (B) {};
  \node [draw=none,, minimum width=2.75cm, anchor=east] at (B.east) (B0) {Sockets};
  \node [anchor=north] at (B.south) (C) {};
  \node [draw=none,, minimum width=2.5cm, anchor=east] at (C.east) (C0) {\small Protocol Driver};
  \node [anchor=north] at (C.south) (D) {};
  \node [draw=none,, minimum width=2.75cm, anchor=east] at (D.east) (D0) {NIC Driver};

  \node [anchor=north, minimum width=2cm, minimum height=0.75cm, fill=red!60] at (0.6, -3.2) (NICA) {NIC};
  \draw [<-,black!20!red, thick] (0, -3.6) -- ++(-.70,.5) -- ++(0, 3);
  \node [ellipse, minimum width=1.25cm, minimum height=0.75cm, fill=red!60, anchor=west] at (A.west) (A1) {\small Buffer};
  \node [ellipse, minimum width=1.25cm, minimum height=0.75cm, fill=red!60, anchor=west] at (B.west) (B1) {\small Buffer};
  \node [ellipse, minimum width=1.25cm, minimum height=0.75cm, fill=red!60, anchor=west] at (C.west) (C1) {\small Buffer};
  \node [ellipse, minimum width=1.25cm, minimum height=0.75cm, fill=red!60, anchor=west] at (D.west) (D1) {\small Buffer};
  \draw [->,black!20!red, thick] (1.25, -3.6) -- ++(1,0);

  \draw [->,black!20!red, thick] (3.50, -3.6) -- ++(.73,.5) -- ++(0, 2.9);



  \draw[rounded corners] (2, -4) rectangle (5.5, 0.5) {};
  \node [] at (3.5,0) (A) {};
  \node [draw=none,, minimum width=2.75cm, anchor=west] at (A.west) (A0) {Application};
  \node [ellipse, minimum width=1.25cm, minimum height=0.75cm, fill=red!60, anchor=east] at (A.east) (A1) {\small Buffer};
  \node [anchor=north] at (A.south) (B) {};
  \node [draw=none,, minimum width=2.75cm, anchor=west] at (B.west) (B0) {Sockets};
  \node [ellipse, minimum width=1.25cm, minimum height=0.75cm, fill=red!60, anchor=east] at (B.east) (B1) {\small Buffer};
  \node [anchor=north] at (B.south) (C) {};
  \node [draw=none,, minimum width=2.5cm, anchor=west] at (C.west) (C0) {\small Protocol Driver};
  \node [ellipse, minimum width=1.25cm, minimum height=0.75cm, fill=red!60, anchor=east] at (C.east) (C1) {\small Buffer};
  \node [anchor=north] at (C.south) (D) {};
  \node [draw=none,, minimum width=2.75cm, anchor=west] at (D.west) (D0) {NIC Driver};
  \node [ellipse, minimum width=1.25cm, minimum height=0.75cm, fill=red!60, anchor=east] at (D.east) (D1) {\small Buffer};

  \node [anchor=north, minimum width=2cm, minimum height=0.75cm, fill=red!60] at (2.9, -3.2) (NICA) {NIC};
\end{tikzpicture}
\end{center}
\caption{TCP/IP communication}
\label{fig:tcpip}
\end{figure}



\begin{figure}[!htp]
\begin{center}
\begin{tikzpicture}[>=latex,every node/.style={minimum width=4cm,minimum height=1cm, outer sep=7pt,draw=black,semithick, scale=0.6}]
  \draw[rounded corners] (-2, -4) rectangle (1.5, 0.5) {};
  \node [] at (0,0) (A) {};
  \node [draw=none,, minimum width=2.75cm, anchor=east] at (A.east) (A0) {Application};


  \draw [<-,black!20!red, thick] (0, -3.6) -- ++(-.70,.5) -- ++(0, 3);
  \draw [->,black!20!red, thick] (1.25, -3.6) -- ++(1,0);

  \draw [->,black!20!red, thick] (3.50, -3.6) -- ++(.73,.5) -- ++(0, 2.9);
  \node [anchor=north, minimum width=2cm, minimum height=0.75cm, fill=red!60] at (0.6, -3.2) (NICA) {RNIC};
  \node [ellipse, minimum width=1.25cm, minimum height=0.75cm, fill=red!60, anchor=west] at (A.west) (A1) {\small Buffer};

  \draw[rounded corners] (2, -4) rectangle (5.5, 0.5) {};
  \node [] at (3.5,0) (A) {};
  \node [draw=none,, minimum width=2.75cm, anchor=west] at (A.west) (A0) {Application};
  \node [ellipse, minimum width=1.25cm, minimum height=0.75cm, fill=red!60, anchor=east] at (A.east) (A1) {\small Buffer};


  \node [anchor=north, minimum width=2cm, minimum height=0.75cm, fill=red!60] at (2.9, -3.2) (NICA) {RNIC};
\end{tikzpicture}
\end{center}
\caption{RDMA communication}
\label{fig:rdmacom}
\end{figure}

\paragraph{} Traditional socket based protocols like TCP/IP or UDP have fundamental limitations requiring data to be copied 
multiple times both at the sender as well as the receiver, through multiple layers of abstraction. This makes them far less 
efficient than RDMA which is designed to completely bypass the OS as well as the CPU and can achieve true zero copy capabilities.

\paragraph{} RDMA gives applications direct access to the RNIC through a user space library, completely circumventing the 
operating system and any drivers. The RNIC is able to read data directly from the applications memory space through direct 
memory access (DMA) resulting in a zero copy transfer.


\subsection{Verbs API}

All three technologies RoCE, iWARP, and InfiniBand share a common API called \emph{Verbs API}. The Verbs API gives us a
user space library called \emph{libibverbs} that gives developers direct low level access to the device, bypassing the kernel.

\paragraph{} The Verbs API is very different from traditional socket programming. Instead of having a simple socket which an 
application can write to and read from, the Verbs API provides endpoints, called Queue Pairs (QP), to which an application 
can issue multiple different operations called \emph{verbs}. These verbs are:

\begin{itemize}
  \item \textbf{Send (with Immediate):} Transfers data from the senders memory to a prepared memory region at the receiver.
  \item \textbf{Receive:} Prepares a memory region to receive data through the send verb.
  \item \textbf{Write (with Immediate):} Copies data from the senders memory to a known memory location at the receiver without any 
    interaction from the remote CPU.
  \item \textbf{Read:} Copies data from remote memory to a local buffer without any inteaction from the remote CPU.
  \item \textbf{Atomics:} Two different atomic operations. Compare and Swap (CAS) and Fetch and Add (FAA). They can access 64-bit 
    values in the remote memory. 
\end{itemize}

\paragraph{} Like traditional socket, these QPs come in different transport modes: Reliable Connection (RC),
Unreliable Connection (UC),
and Unreliable Datagram (UD). While UD supports sending to arbitrary other endpoints, similarly to a UDP socket, RC and UC 
need to establish a one to one connection between Queue Pairs, similarly to TCP sockets. Only RC supports all 
verbs and we will focus on this transport mode.


\paragraph{} Queue Pairs give us a direct connection to the RNIC. A QP essentially consists of two queues that allow us to 
issue verbs directly to the RNIC. The \emph{Send Queue} is used to issue Send, Write, Read, and Atomic verbs, and the 
\emph{Receive Queue} is used to issue  Receive verbs. These verbs are issued by pushing a \mbox{\emph{Work Request~(WR)}}
to the respective queue. A work request is simply a struct that contains an id, the type of verb to issue, and all necessary 
additional information to perform it. The RNIC will pop the WR from the queue and execute the corresponding action.

\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}[node distance=2cm,auto,>=stealth']
  \queue[Send Queue]{0,3};
  \queue[Receive Queue]{0,1.5};
  \rqueue[Completion Queue]{0,0};

  \draw[rounded corners] (-1, .4) rectangle (2.25, 3.4) {};
  \node[align=center] at (-.6, 3.1) {QP};
  \draw[rounded corners] (-1.5, -1.5) rectangle (5.5, 4) {};
  \node[align=center] at (2cm, 3.7cm) {RNIC};
  \draw[rounded corners] (4, 1) circle (1) {};

  \draw[thick, ->] (2.1, 2.5) -- (3, 1.75);
  \draw[thick, ->] (2.1, 1) -- (2.8, 1);
  \draw[thick, <-] (2.1, -.5) -- (3, 0.25);

  \draw[thick, <->] (5.2, 1) -- (6.5, 1);
\end{tikzpicture}
\end{center}
\caption{Resources of the Verbs API}
\label{fig:rdma-parts}
\end{figure}


\paragraph{} This gives us an asynchronous interface. Issuing a work request for a Write operations
does not mean that the Write was
performed, but simply that the RNIC will eventually process this request. To signal the completion of a
work requests there
is an additional type of queue called the \emph{Completion Queue (CQ)}. There needs to be a CQ associated with each Send and 
Receive Queue. When the RNIC completes a work request it will enqueue a \mbox{\emph{Completion Queue Entry~(CQE)}} to the respective
CQ. This CQE informs the application whether the request was processed successfully. The application can match CQEs to 
previously issued work requests by the ID it provided during issuing. It is also possible to post an \emph{unsignaled} network
operation that does not generate a CQE after its completion.


\paragraph{}  All locally and remotely accessible memory needs to be previously registered for the RNIC to be able write to
or read from it. We call these preregistered regions of memory \emph{Memory Regions (MRs)}.  
Registering memory pins it so that it is not swapped out by the host. The process of registering is orders of magnitude slower 
then data operations like sending or writing. So in general all accessed memory is registered at connection setup. Henceforth,  
we will assume these memory regions to be registered if not specified otherwise.



\subsection{RDMA Verbs}\label{sec:bg:verbs}

\subsubsection{Send / Receive} \label{sec:bg:send}
The \emph{Send} and \emph{Receive} verbs are the most traditional operations. The seemingly simple verbs allow 
us to send a single message to  a receiver. The main difference to typical TCP/IP based message passing protocols is that
they are completely asynchronous.

That means both sending and receiving is handled independently by the RNIC. The sending CPU only needs to provide
the RNIC with the message to be sent, which it does through the send verb. The receiving CPU needs to provide its RNIC with 
a receive buffer to write the incoming message to, which it does through the receive verb.




\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}[node distance=2cm,auto,>=stealth']
  \node[align=center] at (-6.4,1) {System A};
  \draw[rounded corners] (-9, -6) rectangle (-3.8, 1.5) {};
  \node[align=center] at (-0.6,1) {System B};
  \draw[rounded corners] (-3.2, -6) rectangle (2, 1.5) {};
  \seqnode{B_cpu}{RAM};
  \seqnode[left of=B_cpu]{B_nic}{NIC};
  \hseqnode[right of=B_cpu, node distance=1.5cm]{B_acpu}{};
  \seqnode[left of=B_cpu, node distance=7cm]{A_cpu}{CPU / RAM};
  \seqnode[right of=A_cpu]{A_nic}{NIC};
  \node[align=center, circle, draw=black, minimum size=.5mm] at (-1,-0.4) {\small 1};
  \msg{B_cpu}{B_nic}{.2}{WR MMIO}
  \node[align=center, circle, draw=black, minimum size=.5mm] at (-5.7,-0.7) {\small 2};
  \msg{A_cpu}{A_nic}{.25}{WR MMIO}
  \msg[below]{A_cpu}{A_nic}{.3}{payload DMA}
  \msg{A_nic}{B_nic}{.5}{network transfer}
  \node[align=center, circle, draw=black, minimum size=.5mm] at (-4.5,-2) {\small 3};
  \msg{B_nic}{B_cpu}{.65}{payload DMA}
  \msg[below]{B_nic}{B_cpu}{.7}{CQE DMA}
  \node[align=center, circle, draw=black, minimum size=.5mm] at (-1.3,-2.6) {\small 4};
  \msg[below]{B_nic}{A_nic}{.71}{Acknowledgement}
  \node[align=center, circle, draw=black, minimum size=.5mm] at (-4.5,-4.3) {\small 5};
  \msg[below]{A_nic}{A_cpu}{.76}{DMA CQE}
  \node[align=center, circle, draw=black, minimum size=.5mm] at (0.5,-4.5) {\small 6};
  \fetch{B_acpu}{B_cpu}{.8}{poll CQ}
\end{tikzpicture}
\end{center}
\caption{Send Receive sequence}
\label{fig:seq-sndrcv}
\end{figure}


\paragraph{} To better understand this communication model we will walk through the operations involved in sending a single
message from a system A to another system B. We assume that the two nodes have already setup a connection. In this
thesis we will not go into the details of connections setup. Each node has prepared a QP and associated a completion 
queue to it. Both systems have registered a MR of at least the size of the message to be sent.

\begin{enumerate}
  \item First, the receiving system provides a \emph{Receive Buffer} to its RNIC to write the next incoming message to.
    Otherwise, the receiving NIC cannot process the message which results in a so called \emph{Reader Not Ready (RNR) Error}.
    This forces the sending NIC to retry the transfer and will break the connection after a number of retries.

    So in order to enable its NIC to handle the message it issues a \emph{Receive Request}. This is done by moving a work request into its 
    Receive Queue. This WR contains a pointer to the MR it prepared. We will often call this posting a receive buffer. 

    System B now polls its CQ until it receives a CQE for its issued receive request.

  \item System A initiates the transfer by posting a \emph{Send Request}. To do this it copies a work request to 
    the Send Queue. This request contains a pointer to its local memory containing the  message to be sent
    and its size. This request notifies the NIC to initiate the transfer.
    It also starts polling its CQ to notice the completion of the send request.

  \item The previous step was actually an MMIO operation, writing the Work Request to the NIC, which now 
    accesses the application's send buffer using DMA. The payload is then sent over the network.

  \item As soon as the first segment arrives at system B's NIC, it consumes the posted receive buffer. It writes the incoming
    data to the referenced buffer and generates a \emph{Completion Queue Entry (CQE)}. 

  \item The successful writing of the message generates an acknowledgement, which is sent back to the sender. The sending NIC
    generates a CQE for the send request as soon as the acknowledgement arrives. The sending CPU is now able to poll this CQE
    from its completion queue, which signals that the transfer is successful and that the send buffer can be reused.

  \item Finally the receiving CPU polls its \emph{Completion Queue (CQ)}, which signals that its receive request has been 
    consumed and that a new message has been written to the receive buffer.
\end{enumerate}

So once we get over the rather unusual interface through these three queues and the rather tedious manual management of 
memory regions, the send and receive verbs are quite simple. They give us a fairly clean message passing notion.


\subsubsection{Write} \label{sec:bg:write}

The \emph{RDMA Write} verb is less conventional. Instead of giving us a message passing interface, it allows us to write at 
an arbitrary, but previously registered, memory region. This does not require any interaction by the remote CPU, apart form
the initial connection setup. 
That also means the remote CPU is not notified of the operation and no CQE is generated at the 
remote. This is the main reason why RDMA writes are generally considered to be faster then send and receive~\cite{anuj-guide}.


\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}[node distance=2cm,auto,>=stealth']
  \node[align=center] at (-6.4,1) {System A};
  \draw[rounded corners] (-9, -6) rectangle (-3.8, 1.5) {};
  \node[align=center] at (-0.6,1) {System B};
  \draw[rounded corners] (-3.2, -6) rectangle (2, 1.5) {};
  \seqnode{B_cpu}{RAM};
  \seqnode[left of=B_cpu]{B_nic}{NIC};
  \hseqnode[right of=B_cpu, node distance=1.5cm]{B_acpu}{};
  \seqnode[left of=B_cpu, node distance=7cm]{A_cpu}{CPU / RAM};
  \seqnode[right of=A_cpu]{A_nic}{NIC};
  \node[align=center, circle, draw=black, minimum size=.5mm] at (-5.7,-0.7) {\small 1};
  \msg{A_cpu}{A_nic}{.25}{WR MMIO}
  \msg[below]{A_cpu}{A_nic}{.3}{payload DMA}
  \msg{A_nic}{B_nic}{.5}{network transfer}
  \node[align=center, circle, draw=black, minimum size=.5mm] at (-4.5,-2) {\small 2};
  \msg{B_nic}{B_cpu}{.65}{payload DMA}
  \node[align=center, circle, draw=black, minimum size=.5mm] at (-1.3,-2.6) {\small 3};
  \msg[below]{B_nic}{A_nic}{.71}{Acknowledgement}
  \node[align=center, circle, draw=black, minimum size=.5mm] at (-4.5,-4.3) {\small 4};
  \msg[below]{A_nic}{A_cpu}{.76}{DMA CQE}
\end{tikzpicture}
\end{center}
\caption{RDMA Write sequence}
\label{fig:seq-wrt}
\end{figure}

Figure~\ref{fig:seq-wrt} show the operations involved in writing data to remote memory using RDMA write. It is generally very
similar to the send and receive sequence presented in the previous section. The sending CPU still issues a work request which
is handled by the NIC and is notified of its completion through the completion queue.
The main difference is that the remote system does not need to post a receive buffer and there is no CQE generated at the 
remote. Also, the work request is a structured differently. It does not only contain a pointer to the local send buffer but
also provides the remote address to write it to.

\paragraph{} The standard RDMA write does not generate a completion queue entry at the receiver, 
which is generally more efficient.
However, sometimes it is very helpful for the receiver to be notified of a completed write. For this purpose the Verbs API 
also provides a related operation called \emph{Write with Immediate}.
This operation works very similarly to a normal RDMA write, but it generates a CQE at the receiver, in the same way 
the send verb does. This means it will also consume a posted receive request, so the receiver needs to post a receive request 
prior to the transfer. Write with Immediate however will not write any data to the associated receive buffer.

It further allows us to send a 32 bit value called \emph{Immediate} with the request. This value is added to
the completion entry at the receiver. Thus, Write with Immediate allows us to notify the remote that we have written data and to 
send additional out of band data, but it also negates any performance improvements over the send verb.


\subsubsection{Read}

The \emph{RDMA Read} verb is conceptually similar to the RDMA write, as it does also not require any interaction by the 
remote CPU. The key difference being that instead of writing to the remote memory, RDMA read \emph{reads} from it.


\begin{figure}[!htp]
\begin{center}
\begin{tikzpicture}[node distance=2cm,auto,>=stealth']
  \node[align=center] at (-6.4,1) {System A};
  \draw[rounded corners] (-9, -6) rectangle (-3.8, 1.5) {};
  \node[align=center] at (-0.6,1) {System B};
  \draw[rounded corners] (-3.2, -6) rectangle (2, 1.5) {};
  \seqnode{B_cpu}{RAM};
  \seqnode[left of=B_cpu]{B_nic}{NIC};
  \hseqnode[right of=B_cpu, node distance=1.5cm]{B_acpu}{};
  \seqnode[left of=B_cpu, node distance=7cm]{A_cpu}{CPU / RAM};
  \seqnode[right of=A_cpu]{A_nic}{NIC};
  \node[align=center, circle, draw=black, minimum size=.5mm] at (-5.7,-0.7) {\small 1};
  \msg{A_cpu}{A_nic}{.25}{WR MMIO}
  \msg[below]{A_cpu}{A_nic}{.3}{payload DMA}
  \msg{A_nic}{B_nic}{.5}{network transfer}
  \node[align=center, circle, draw=black, minimum size=.5mm] at (-4.5,-2) {\small 2};
  \msg{B_nic}{B_cpu}{.65}{payload DMA}
  \node[align=center, circle, draw=black, minimum size=.5mm] at (-1.3,-2.6) {\small 3};
  \msg[below]{B_nic}{A_nic}{.71}{Acknowledgement}
  \node[align=center, circle, draw=black, minimum size=.5mm] at (-4.5,-4.3) {\small 4};
  \msg[below]{A_nic}{A_cpu}{.76}{DMA CQE}
\end{tikzpicture}
\end{center}
\caption{RDMA Read sequence}
\label{fig:seq-rd}
\end{figure}

Figure~\ref{fig:seq-rd} shows the sequence necessary to perform an RDMA Read. For this operation, the remote source memory as
well as the local target buffer need to be appropriately registered as memory regions, in the same way necessary for the
other verbs.

\paragraph{} The order of operations again looks very similar. We first need to issue a work request. The work request needs 
to include the remote address to read from, the amount of data to read from it, and a pointer to a local target buffer that is large
enough to receive the payload. As soon as the  transfer is completed a CQE is generated. 

The key difference to the RDMA write verb is, 
instead of sending the payload, writing to memory using DMA, and replying with lightweight acknowledgement, the 
RDMA read verb sends a lightweight request, performs a DMA reading from memory, and replies with the payload.

\pagebreak
\subsubsection{Atomics}

While it is guaranteed that multiple Write and Send operations of the same QP will appear in the same order as they are 
issued~\cite{rdma-reference}, it is not guaranteed that Read operations will show the effects of previous writes. 
Additionally, operations from
different QPs have hardly any synchronization guarantees.

To prevent data races when accessing the same data from different QPs the Verbs API provides two different atomic operations 
on remote 64 bit values. These operations do not need any involvement by the remote CPU and the necessary operations 
to perform these are very similar to the read verb.


\begin{itemize}
  \item \textbf{Fetch and Add} reads a 64 bit value from the remote memory space, adds the additionally provided 
    \code{compare\_add} value, and writes the original data to the provided local memory.
  \item \textbf{Compare and Swap} reads a 64 bit value from the remote memory space and compares it to 
    the \code{compare\_add} value provided in the work request. If they are equal, it writes the value \code{swap},
    which is also provided in the work request. Regardless of success, it returns the original data to the requester.
\end{itemize}

Atomic operations open up a lot of options for more complex protocols, but they are currently rarely used as they generally
provide much lower throughput~\cite{anuj-guide}.


