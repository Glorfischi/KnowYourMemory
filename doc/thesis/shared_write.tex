\section{Shared Write} \label{sec:conn:shared_write}
In Section~\ref{sec:conn:buf_write} we presented multiple ways to build up a message passing protocol based on the 
RDMA write verb and a ring buffer. In this section we present a way to share a single ring buffer for multiple connections.

\paragraph{} We only implement a single variant based on the \emph{Write with Immediate} verb and focus on a N:1 connection pattern only,
where multiple sender transmit messages to a single receiver.

We point out that there are multiple different approaches that use the same atomics based reserving we present below. One
could use the same \emph{write offset} or \emph{write reverse} approach presented earlier and multiple receivers could
utilize the same ring buffer when using \emph{write with immediate}. A deep dive in all this variations however is out
of scope of this thesis.

\subsection{Protocol}

\paragraph{} We present a protocol with two phases

\begin{itemize}
  \item In the first phase, the sender reserves space in the buffer by modifying a remote counter using 
    the \emph{Fetch and Add} verb, and makes sure that the reserved space is actually free.
  \item In the second phase the sender writes to the reserved space.
\end{itemize}
\subsubsection{Reservation}
To allow for multiple sender to write to a shared ring buffer we need to coordinate them. This boils down to having to
assign unique destination addresses for each of sender. This translates to implementing a \emph{sequencer} that issues
increasing addresses based on the message size. This can either be implemented using atomic operations or by using RPCs
and handling the sequencing at the receives CPU. An RPC based approach is expected to provide us with higher 
throughput~\cite{anuj-guide}, but introduces more complexity and CPU overhead for the receiver. So we focus on an atomics based 
implementation.

\paragraph{} In our implementation we have 16 bytes of metadata at the receiver that is accessible to all senders. It consists
of a 64~bit tail offset and 64~bit head offset. The tail is updated by the receiver and the head represents the end of the
last reserved buffer segment of any sender. This does not mean that this message has been written. Both counters are
monotonically increasing and translate to the actual ring buffer offset by taking the remainder when dividing it by
the buffer size.

We are aware that this approach will eventually lead to an overflow of the head and tail offset. If a system expects to receive
more than 18 exabytes, this approach will not work. An extension using some kind of epochs to handle offset overflows could
fix this, but this was not explored in this thesis.

\paragraph{}When a sender wants to send a message of size $s$ it issues a \emph{fetch and add} on the remote tail with the 
size $s$. It receives its destination address and can be assured that the next $s$ bytes are reserved. It then needs
to check whether the buffer segment is actually free to write to by comparing the tail with the head. This might cause one 
or multiple reads to update the cached head value.

\paragraph{} We could also use \emph{compare and swap} for the reservation phase, but that would introduce another read and
we expect a lot of contention for high loads.

\subsubsection{Transmission}
After the reservation phase the sender can write to it and needs to signal the incoming message to the receiver. We decided 
to do this using the \emph{Write with Immediate} verb. This allows us to notify the receiver with completion event. 

One thing we need to note is that this time the completion events do not have to arrive in the same oder as they appear in
the ring buffer. That means we will need to send the offset of the corresponding buffer segment as immediate data.


\paragraph{} We reuse the same single receiver approach presented in Section~\ref{sec:conn:send} with a single completion queue for multiple
QPs. The out of order freeing of messages can then be solved in very similar way as presented in the last section.
\comment{Not quite. I actually used a little different approach to be able to support N*1:1 down the line. But this is not 
a bottleneck and is not really interesting}


\subsection{Device Memory}
Since fairly recently RDMA enabled NICs provide on device memory that can be accessed using the Verbs API. This device memory can 
be accessed the same way as regular memory through verbs and is completely transparent for the remote system. It does however
reduce latency and does not have to make any PCI accesses. From this we especially expect atomic operations to be a lot faster
when using device memory.

\paragraph{} Quick micro benchmarks reaffirm these expectations. In our case maximum throughput of fetch and add operations 
increased form around 2 MOps to nearly 8 MOps. This means by using device memory for the ring buffer metadata we can increase
the throughput of our reservation phase by nearly 300\%. We can also achieve a minor speed up for reading the head update after
reserving a buffer segment.


\subsection{Features}

The shared write protocol allows us to share a single ring buffer between multiple connections by using atomic operations.

This gives \emph{resource sharing} capabilities to a buffered read protocol, while keeping the features of 
\emph{variable message size}. This allows us to have a constant memory requirement for a lot of connections,
resulting in very effective use of buffer space.


\paragraph{} Our implementation also provides convenient \emph{interrupts}, while other approaches could have the possibility 
for a completely \emph{passive receiver} by using an offset approach, similarly to the write offset implementation in 
Section~\ref{sec:conn:buf_write}.

\paragraph{} The shared write protocol does still not provide \emph{true zero copy} capabilities, and the \emph{blocking}
behaviour is even more pronounced than for the unshared buffered write protocol. A single sender that reserves spaces, but does not
write to it can block all other connections. This means a single crashed sender can block the whole system. For a real world
application there needs to be some extension to achieve better resilience.

