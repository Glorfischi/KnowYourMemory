\section{Shared Write} \label{sec:conn:shared_write}
\subsection{Desgin}
In Section~\ref{sec:conn:buf_write} we presented multiple ways to build up a message passing protocol based on the 
RDMA write verb and a ring buffer. In this section we present a way to share a single ring buffer for multiple connections.


\paragraph{} We present a protocol with two phases

\begin{itemize}
  \item In the first phase, the sender reserves space in the buffer by modifying a remote counter using 
    the \emph{Fetch and Add} verb.
  \item In the second phase the sender makes sure that the reserved space is actually free and writes to it.
\end{itemize}

We only implement a single variant based on the \emph{Write with Immediate} verb and focus on a N to 1 connection pattern only.
We point out that there are multiple different approaches that use the same atomics based operation we present below. One
could use the same \emph{write offset} or \emph{write reverse} approach presented earlier and multiple receivers could
utilize the same ring buffer when using \emph{Write with Immediate}. A deep dive in all this variations however is out
of scope of this thesis.

\subsubsection{Reservation}
To allow for multiple sender to write to a shared ring buffer we need to coordinate them. This boils down to having to
assign unique destination addresses for each of sender. This translates to implementing a \emph{sequencer} that issues
increasing addresses based on the message size. This can either be implemented using atomic operations or by using RPCs
and handling the sequencing at the receives CPU. An RPC based approach is expected to provide us with higher 
throughput~\cite{anuj-guide}, but introduces more complexity and CPU overhead for the receiver. So we focus on an atomic based 
implementation.

\paragraph{} In our implementation we have 16 bytes of metadata at the receiver that is accessible to all sender. It consists
of a 64~bit tail and 64~bit tail. The tail is updated by the receiver and the head represents the end of the last reserved 
buffer segment of any sender. This does not mean that this message has been written. Both counters are monotonically increasing
and translate to the actual ring buffer offset by taking the remainder when dividing it by the buffer size.

Now when a sender wants to send a message of size $s$ it will issue a \emph{fetch and add} on the remote tail with the 
size $s$. It will receive its destination address and can be assured that the next $s$ bytes are reserved. It then needs
to check whether the buffer segment is actually free to write to by comparing the tail with the head. This might cause one 
or multiple reads to update the cached head value.

\paragraph{} We could also use \emph{compare and swap} for the reservation phase, but that would introduce another read and
we expect a lot of contention for high loads.

\subsubsection{Sending}
After the reservation phase the sender can write to it and needs to signal the incoming message to the receiver. We decided 
to do this using the \emph{Write with Immediate} verb. This allows us to notify the receiver with completion event. 

One thing we need to note is that this time the completion events do not have to arrive in the same oder as they appear in
the ring buffer. That means we will need to send the offset of the corresponding buffer segment as immediate data.


\subsubsection{Receiving}
We reuse the same single receiver approach presented in Section~\ref{sec:conn:send} with a single completion queue for multiple
QPs. We do not use a shared receive queue, as this will actually introduce a bottleneck as the 2~Mops are slower than the 
peak throughput we can achieve with our sequencer approach.

The out of order freeing of messages can then be solved in very similar way as presented in the last section.
\comment{Not quite. I actually used a little different approach to be able to support N*1:1 down the line. But this is not 
a bottleneck and is not really interesting}


\subsubsection{Device Memory}
Fairly recently RDMA enabled NIC provide on device memory that can be accessed using the Verbs API. This device memory can 
be accessed the same way as regular memory through verbs and is completely transparent for the remote system. It does however
reduce latency and does not have to make any PCI accesses. From this we especially expect atomic operations to be a lot faster
when using device memory.

\paragraph{} Quick micro benchmark reaffirm these expectations. In our case maximum throughput of fetch and add operations 
increased form around 2 MOps to nearly 8 MOps. This means by using device memory for the ring buffer metadata we can increase
the throughput of our reservation phase by nearly 300\%. We can also achieve a minor speed up for reading the head update after
reserving a buffer segment.

\subsection{Evaluation}

We again ran all our evaluations on two machines running CentOS 7 containing two Intel Xeon Gold 6152 and 384 GiB of
memory. The two nodes each contain a Mellanox ConnectX-5 (100Gbps) and are connected through a 100 Gbps switch. 
All measurements have been performed using RoCE.

\subsubsection{Latency}

\begin{figure}[]
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-atomic-lat-msgsize.png}
  \caption{Evaluation of the Shared Write latency between two nodes}
  \label{fig:plot-write-atomic-lat}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-atomic-bw-msgsize.png}
  \caption{Evaluation of the Shared Write bandwidth between two nodes}
  \label{fig:plot-write-atomic-bw}
\end{subfigure}
\end{figure}

Figure \ref{fig:plot-write-atomic-lat} shows the latency of our \emph{Shared Write} protocol between two nodes. 
We again show half the ping-pong round-trip time. We can clearly see that we have a large constant overhead, caused
by the reserving phase of over $2L$ independent of the message size.

We can reduce this constant overhead by about 0.25 $\mu$s by using device memory for the connections metadata. This is 
in line with what we expect given our micro benchmark, and is caused by the receiving NIC not having to access the 
receivers RAM.

\paragraph{} When we compare these results to our unshared buffered read connection, we can see that we are consistently
about 3.5 $\mu$s slower. Especially for small messages this is a very significant overhead and can more than double the 
total latency.


\subsubsection{Bandwidth}

In Figure \ref{fig:plot-write-atomic-bw} we can see the throughput of the \emph{Shared Write} protocol for a single 
connections, with varying message size.  We want to clearly point out the different scaling, the connection is 
significantly slower than the previously evaluated connections. The bandwidth seems to linearly grow with message
size, but stay very low below 20 Gbps event for a message size of 15 KB.

\paragraph{} This is caused by our two phase design. Because we did not want to switch to a more complicated interface 
then the \code{SendAsync()} and \code{Wait()} presented earlier, we are essentially limited to one message per phase.
This way we are unable to fill the pipeline of the device which causes this very low single threaded performance. With 
more engineering effort and a more complicated design this could be improved drastically, but as in this case we are 
a lot more interested in the multi threaded performance, which is not as strongly affected, we decided to simplify the
implementation.


\subsubsection{Resource Sharing}

\begin{figure}[h]
\includegraphics[width=1\textwidth]{write-atomic-bw-threads.png}
\caption{Shared Write bandwidth with a single receiver and varying number of sender}
\label{fig:plot-write-atomic-n1}
\end{figure}

The shared receive connection was first and foremost designed to run in a multithreaded setting. 
Figure~\ref{fig:plot-write-atomic-n1} shows the bandwidth with a single receiver and an increasing amount of senders. As we 
did for previous connections, we evaluate three different message sizes. We can clearly see that we are strongly limited by
our two phase approach. That means we see a linear increase in bandwidth with increasing number of senders as we are able to
issue more requests in parallel.

\paragraph{} For the large message size of 8 KB the total throughput increases linearly until we hit the line rate with 10 
active sender. The use of device memory allows us to saturate the link with only 9. This is not a large difference and we are
able to take full advantage of the bandwidth with or without the usage of device memory.

\paragraph{} Smaller messages give use more interesting results. For a message size of 512 bytes we first see a linear 
increase in bandwidth for both the version with and without usage of device memory. The version without device memory however
caps at around 7.5 Gbps, which is exactly what we predicted given our \emph{fetch and add} micro-benchmark that gave us a
peek throughput of 2 MOps. The version utilizing device memory for the metadata further achieves higher throughput with more
sender and we expect it to theoretically reach around 30 Gbps with enough concurrent sender.

\paragraph{} Although this graph does not clearly show it, we see very similar results for the 16 bytes messages. The version
performing fetch and add on RAM peaks at about 0.25 Gbps, while the version operating on device memory is able to achieve 
higher throughput.


\subsection{Conclusion}

We have shown that it is possible to use atomic operations to share a single ring buffer between multiple connections. This
way it keeps its feature of variable message size and allows us to have a constant memory requirement for a lot of connections,
resulting in very effective use of buffer space.

\paragraph{} With the use of on device memory we have also shown that we can drastically improve performance of RDMA atomic 
operations. Giving us a throughput that exceeds the one we could achieve using shared receive queues. But it 
also showed us how important it is to allows for enough concurrent requests to get the full throughput potential. 
With an event driven design approach we would be able to achieve up to four times better performance that the shared
send receive connection, by allowing us to easily issue more concurrent requests.

\paragraph{} We believe this kind of resource sharing has its practical use cases. We once again saw that sharing resources
will impact total performance, but the throughput we have observed should be enough for many real world systems. 
