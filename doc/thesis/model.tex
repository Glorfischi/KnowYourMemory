\section{Performance Model}\label{sec:perf-model} \label{sec:model}
In this section we introduce a general performance model for data exchange protocol that will allow us to estimate the 
performance of our protocol implementations. It gives us a better understanding of our evaluation results and helps us 
to locate bottlenecks.

\paragraph{}We took a fairly detailed look at the operations involved in transmitting a message using RDMA in Section~\ref{sec:rdma}.
Trying to closely model this however gives us a far too complex model, with a lot of parameters that are hard to assess,
especially if we start to look at more complex protocols. We introduce a more simplified model inspired by the  
\emph{LogGP model}~\cite{loggp}, which was developed  to model point to point communication with variable message sizes for 
traditional IP based networks.

\paragraph{} At its core the LogGP model uses a fixed CPU overhead $o$ per message, the communication latency $L$, and 
the bandwidth $G$. While this results in a decent model IP based system, it is unable to model the heavy offloading happening
in RDMA. We extended the LogGP model by splitting the offset $o$ into multiple offsets for each of the components. We also 
largely ignored the number of processes $P$ and use a slightly different definition for the gap $g$. This leaves 
us with the following parameters, which are illustrated in Figure~\ref{fig:model-base}.

\begin{figure}[!htp]
\begin{center}
\begin{tikzpicture}[node distance=1cm,auto,>=stealth']
  \node[] (p0) {P0};
  \node[right of=p0, node distance=10cm] (p0_g) {};
  \draw[dotted] (p0) -- (p0_g);

  \node[below of=p0, node distance=0.5cm] (p0_nic) {P0 NIC};
  \node[right of=p0_nic, node distance=10cm] (p0_nic_g) {};
  \draw[dotted] (p0_nic) -- (p0_nic_g);

  \node[below of=p0_nic, node distance=1.5cm] (p1_nic) {P1 NIC};
  \node[right of=p1_nic, node distance=10cm] (p1_nic_g) {};
  \draw[dotted] (p1_nic) -- (p1_nic_g);

  \node[below of=p1_nic, node distance=0.5cm] (p1) {P1};
  \node[right of=p1, node distance=10cm] (p1_g) {};
  \draw[dotted] (p1) -- (p1_g);
  

  %%%%

  \draw[very thick] (p0) --node[above,scale=0.75,midway]{$o_{snd}$} ($(p0)!0.15!(p0_g)$);
  \draw[very thick] ($(p0_nic)!0.15!(p0_nic_g)$) --node[above,scale=0.75,midway]{$o_{nsnd}$} ($(p0_nic)!0.225!(p0_nic_g)$);


  \path[] ($(p0_nic)!0.225!(p0_nic_g)$) --node[above,scale=0.75,midway]{$G$} ($(p0_nic)!0.26!(p0_nic_g)$);
  \draw[dotted, ->] ($(p0_nic)!0.225!(p0_nic_g)$) -- ($(p1_nic)!0.395!(p1_nic_g)$);

  \path[] ($(p0_nic)!0.26!(p0_nic_g)$) --node[above,scale=0.75,midway]{$G$} ($(p0_nic)!0.295!(p0_nic_g)$);
  \draw[dotted, ->] ($(p0_nic)!0.26!(p0_nic_g)$) -- ($(p1_nic)!0.43!(p1_nic_g)$);

  \path[] ($(p0_nic)!0.295!(p0_nic_g)$) --node[above,scale=0.75,midway]{$G$} ($(p0_nic)!0.33!(p0_nic_g)$);
  \draw[dotted, ->] ($(p0_nic)!0.295!(p0_nic_g)$) -- ($(p1_nic)!0.465!(p1_nic_g)$);

  \draw[dotted, ->] ($(p0_nic)!0.33!(p0_nic_g)$) -- ($(p1_nic)!0.5!(p1_nic_g)$);


  \draw[very thick] ($(p1_nic)!0.5!(p1_nic_g)$) --node[above,scale=0.75,midway]{$o_{nrcv}$} ($(p1_nic)!0.55!(p1_nic_g)$);
  \draw[very thick] ($(p1)!0.50!(p1_g)$) --node[above,scale=0.75,midway]{$o_{free}$} ($(p1)!0.545!(p1_g)$);
  \draw[very thick] ($(p1)!0.55!(p1_g)$) --node[above,scale=0.75,midway]{$o_{rcv}$} ($(p1)!0.6!(p1_g)$);
    
  %%%%

  \draw[dotted] ($(p0_nic)!0.225!(p0_nic_g)$) -- ($(p0_nic)!0.225!(p0_nic_g)+(0,-3)$);
  \draw[dotted] ($(p0_nic)!0.330!(p0_nic_g)$) -- ($(p0_nic)!0.330!(p0_nic_g)+(0,-3)$);
  \draw[<->] ($(p0_nic)!0.225!(p0_nic_g)+(0,-2.8)$) --node[above,scale=0.75,midway]{$(k-1)G$} ($(p0_nic)!0.330!(p0_nic_g)+(0,-2.8)$);

  \draw[dotted] ($(p1_nic)!0.5!(p1_nic_g)$) -- ($(p1_nic)!0.5!(p1_nic_g)+(0,-1.5)$);
  \draw[<->] ($(p0_nic)!0.330!(p0_nic_g)+(0,-2.8)$) --node[above,scale=0.75,midway]{$L$} ($(p1_nic)!0.5!(p1_nic_g)+(0,-1.3)$);


  \draw[dotted] ($(p0)!0.15!(p0_g)$) -- ($(p0)!0.15!(p0_g)+(0,.8)$);
  \draw[dotted] ($(p0)!0.65!(p0_g)$) -- ($(p0)!0.65!(p0_g)+(0,.8)$);
  \draw[<->] ($(p0)!0.15!(p0_g)+(0,.6)$) --node[above,scale=0.75,midway]{$g$} ($(p0)!0.65!(p0_g)+(0,.6)$);

  %%%%

  \draw[dotted, ->, lightgray] ($(p1_nic)!0.56!(p1_nic_g)$) -- ($(p0)!0.65!(p0_g)$);


\end{tikzpicture}
\end{center}
\caption{Sending an receiving messages under our model}
\label{fig:model-base}
\end{figure}




\begin{itemize}
  \item $L$: an upper bound on the Latency, incurred in sending a message from the senders NIC to the receivers NIC. This is
    the pure network latency.
  \item $G$: the Gap per byte for long messages. For our purposes this is the time of sending a single byte given the 
    maximum bandwidth of our link.
  \item $o_{snd}$: the \emph{send overhead}, defined as the length of time that a processor is engaged in sending each message.
    For some protocols this also includes any preparation an communication overhead necessary to send a message.
  \item $g$: the \emph{send gap}, defined as the minimum time interval until the sender can reuse the resources involved in 
    the transmission. (e.g. the send buffer) 
  \item $o_{nsnd}$: the \emph{send NIC overhead}, defined as the length of time that a NIC is engaged in sending each message.
  \item $o_{nrcv}$: the \emph{receive NIC overhead}, defined as the length of time that a NIC is engaged in receiving each message.
  \item $o_{rcv}$: the \emph{receive overhead}, defined as the length of time that a processor is engaged in receiving each message.
  \item $o_{free}$: the \emph{freeing overhead}, defined as the time that a processor is engaged in preparing a receive buffer
    to receive into (e.g. post receive)
\end{itemize}


\paragraph{} This still gives us a fairly complex model and evaluating each of these overheads is in practice very hard. 
Also this model is very much built around send or write based protocols and does not seem to be a very good representation 
for read based protocols. But as we will see it can still give us a good understanding of the observed performance, even if 
we cannot evaluate all parameters, and it still gives us a fairly good estimate for read based protocols, even if it was not
explicitly designed for them.

\paragraph{Latency Estimate}

Using this model we can estimate the latency $t$ of transferring a single message $m$ of size $k$ with:

$$
t \geq o_{snd} + o_{nsnd}  + (k-1)G + L + o_{nrcv} + o_{rcv}
$$


\paragraph{Throughput Estimate}

Message transfer is highly pipelined. So to estimate bandwidth $bw$ our model basically reduces to finding the bottleneck.

$$
bw \leq \max ( \frac{k}{o_{snd} + g_{snd}}, \frac{k}{o_{nsnd} + (k-1)G}, \frac{k}{o_{nrcv} + (k-1)G}, \frac{k}{o_{rcv} + g_{rcv}})
$$


\subsection{Bandwidth}

We established in Section~\ref{sec:model} that to get good performance we need to be able to queue enough transmission
to keep the NIC busy. The amount of such \emph{unacknowledged} outstanding messages drastically changes performance, 
but it is not a priory clear how many of these unacknowledged messages we need. 

\paragraph{} In Figure \ref{fig:plot-sndrcv-bw-unack} we show the bandwidth for varying number of unacknowledged
messages. We evaluate this for a message size of 16 bytes, so that we are definitely not limited by the device bandwidth.
As we can see the throughout increases linearly until we hit an other bottleneck for 32 unacknowledged messages. We can 
explain this behaviour with our model as it seems that the cost of posting a send $o_{snd}$ is about one 32th that of the 
delay $g_{snd}$ until the send is acknowledged.


\begin{figure}[]
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-unack.png}
  \caption{Bandwidth with message size of 16 bytes with varying number of unacknowledged messages}
  \label{fig:plot-sndrcv-bw-unack}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-batch.png}
  \caption{Bandwidth with message size of 16 bytes with varying batch size}
  \label{fig:plot-sndrcv-bw-batch}
\end{subfigure}
\begin{subfigure}[b]{1\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-msgsize.png}
  \caption{Evaluation of the Send Receive bandwidth between two nodes and our performance model}
  \label{fig:plot-sndrcv-bw}
\end{subfigure}
\end{figure}


\paragraph{} We established earlier we can reduce the overhead $o_{snd}$ per message by batching send requests using
doorbelling. Figure \ref{fig:plot-sndrcv-bw-batch} shows the throughout given a fixed message size of 16 bytes with 
varying batch sizes for multiple numbers of unacknowledged messages. We can see that that throughout increases more or
less linearly but we do not seem to gain any throughput with a batch size larger than 8. We also notice that we are
quickly bottleneck again by the amount of unacknowledged messages. When increasing both batch size and the number of
unacknowledged messages we seem to hit another bottleneck at a little over 1.7 Gbit/s or about 14 million requests 
per second.\comment{Check numbers. Also compare with perftest}. This now seems to be the sending device itself that 
is the bottleneck.



\paragraph{} This gives us three different configurations for the send connection. The \emph{sequential} configuration
which will only allow a single unacknowledged message, the \emph{unbatched} configuration, which allows for up to 64
unacknowledged messages, but does not batch send operations, and the \emph{batched} version which batches 8 message
in a single work request and allows for a total of 256 unacknowledged messages.

In Figure \ref{fig:plot-sndrcv-bw} we can that the bandwidth measurements for these three versions with varying message
sizes. We can clearly see the importance of having unacknowledged messages. The sequential version is nowhere near the
theoretical throughput of 100 Gbps. Our model actually predicts that the throughput scales sublinearly with message size,
which seems to fit our observations.

For the unbatched version we can clearly see that we scale linearly with the message size, as we are bottlenecked by 
the number of work request we can issue, until we seem to hit the devices network bandwidth. We are not quite able to
utilize the complete 100 Gbps, but this is to be expected and we see similar result from micorobenchmarking tools such
as perftest.

Finally the batched version seems to be limited by the device only. We see a quicker, more or less linear increase in 
bandwidth until we are limited again by the network speed.


\paragraph{} It is worth noting that we did not hit any receiver side bottlenecks in our single threaded benchmarks.
Even without receiver side batching, receiving and reposting is sufficiently fast when handling a single connection.
We can be sure of that, as we did not encounter any \emph{Reader-Not-Ready (RNR)} errors, which occur if an incoming
is unable to consume a prepared receive buffer.


