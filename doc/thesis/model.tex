\section{Communication Model and Design Space}\label{sec:model}
\todo{Give a clear definition of what type of connections we implement and give a reason why we chose to implement the ones we did}\\

The goal of this work is design, implement, and study a comprehensive list of RDMA based protocols for data exchange. We will 
first clearly define our communication model and give a definition of data exchange. Based on this model we will then build 
a design space of useful protocols. This way we show that our list of protocols is reasonably exhaustive.

\subsection{Communication Model}

Informally a protocol for data exchange is an algorithm for moving data residing in memory of one server, i.e. the sender, to 
a memory location at another server, i.e. the receiver.

\todo{I don't like this definition. It does not feel solid enough}
\begin{defn}
Let $A$ and $B$ be nodes connected by an RDMA enabled link. Let $buf_A$ be a memory region on host $A$ and let $buf_B$ 
be a memory region on host $B$. We define protocol for data exchange $P$ as an algorithm for copying the
data in $buf_A$ to $buf_B$, which is initiated by $A$. Both $A$ and $B$ need to be able to notice when the data transfer
is complete and when their respective buffer is usable again.
\end{defn}

\begin{rem}
  We assume that all memory regions are potentially registered to an RDMA device and can be accessed through RDMA verbs.
\end{rem}

\begin{rem}
  We assume that there is no out of band communication between the sender and receiver.
\end{rem}


\todo{example}

We are notably different to most other work that focuses on RPC \cite{herd, guidlines}, remote datastructure access 
\cite{pilaf, others}, or by replicating socket like interfaces \cite{SocksDirect}. We believe with this more general
communication model we can give engineers and researchers a better understanding of the building blocks that can be 
used to build more specific connection protocols, while not only micro-benchmarking RDMA verbs.


\todo{N:1,1:N,N:N}

\todo{N:1 - atomic write?}

\todo{1:N - atomic read?}

\pagebreak
\subsection{Features}

There is more to data exchange protocol than raw throughput or latency. Oftentimes it is more important for an application
that the connection is \emph{fair} or that the memory requirements do not grow too large, even if that protocol wastes
a few microseconds.

To be able to better compare our presented protocols we define a few traits that are often required by systems and will 
analyse which protocols provide these traits

\paragraph{True Zero-Copy} Whether the protocol can be employed to receive directly to the destination buffer. While RDMA 
claims to be zero-copy, most protocols are not truly zero-copy. Protocols using ringbuffers~\cite{} or mailboxes~\cite{} are
strictly speaking not zero copy, as the receiver will always have to copy the data from this buffer to its actual destination.
This is can be especially important when transferring a large amount of data that does not have to be further processed.

\paragraph{Variable Message Size} That the protocol allows us to send messages with different sizes, without using the 
complete buffer at the receiver. Ring buffers for example can be designed to only use the necessary space per message
while send receive will always use the complete receive buffer, no matter how small the message actually is.

\paragraph{Passive Sender/Receiver} Whether the sender or receiver is \emph{passive}. We define \emph{passivity} as not 
requiring any CPU involvement in the actual data transfer. That means a \emph{passive receiver} can receive messages by
simply reading from a memory location, without having to acknowledge anything or posting receive buffers. \emph{Passivity} 
reduces the necessary CPU usage to a minimum and can be useful for applications with uneven CPU requirements.
\draft{
\paragraph{Passive Notification} Whether there is some kind of notification system, that allows the receiver to be notified 
of an incoming message without having to constantly poll for it. While polling gives us better performance in almost all cases
constantly polling wastes a lot of CPU cycles when we do not receive a lot of messages.
}

\todo{This just randomly came to my mind. We don't actually us this. Should we keep this? Is this doable when using writes?}

\paragraph{Resource Sharing} If multiple connections can share resources, especially memory. We are using \emph{reliable connections}
so will inherently need to use one \emph{QP} per sender and receiver pair, but we can reduce memory usage by sharing receive 
buffers between multiple connections. Without any memory sharing utilization grows linearly with the number of QPs, which 
can quickly lead to unacceptable memory requirements, especially for $N$ to $N$ communication patterns.

\paragraph{Fairness} By fairness, we mean that a single QP cannot block other QPs. This becomes relevant, as soon as we start
sharing resources between QPs. It is oftentimes important that a single slow running task cannot completely block all 
connections.

\todo{
  Maybe we need something less strong? With this definition neither atomic write nor SRQ are fair. But with the atomic write
  a single unfreed message is able to block the complete ringbuffer, while a single not reposted receive buffer is not as
  tragic
}

\pagebreak
\subsection{Design Space}

The design space of all imaginable RDMA based data exchange protocols is huge and it is out of scope of this work to 
analyse all of them. 

However with some reasonable assumptions on which protocols are useful we can split the design space 
in a number of distinct groups and we will analyse one implementation for each of these groups.


\paragraph{} The first obvious way to split all possible exchange protocols is by the verbs they use to transfer the actual data. 
This would be:
\begin{itemize}
  \item Send
  \item Read
  \item Write
\end{itemize}
We ignore the atomic verbs as well as mixture of verbs. We did not see any advantages in splitting the actual data transfer 
into multiple verbs, as this only seems to add overhead and complexity. And while it would theoretically be possible to use
\emph{Compare and Swap} for generic data transfer, but the expected performance is far too low to get a viable protocol.


\subsubsection{Send}

\todo{This is straight forward}
\subsubsection{Write}
The \emph{Write} verb is a lot less restrictive. It allows the sender, with certain restriction, to write at an arbitrary 
location in the receivers memory, without any interaction of the receivers CPU. This also means we need to solve some of the 
problems which the \emph{Send} verb solved for us.

\begin{itemize}
  \item For the sender to be able to transfer data, it needs to know where to send it to and if the receiver is ready 
to receive the data. It needs what we will call the \emph{Target Information}.

  \item For the receiver to notice that it received data it needs some kind of way to notified. This is what we will
call \emph{Transfer Notification}
\end{itemize}

This two problems will always exist when using \emph{Write} as the data transfer verb. 

\paragraph{Buffered Write}
This is probably one of the most widely used data transfer protocol there is and usually the first protocol people think of
apart from send-receive. The idea is to have a buffer at the receiver, which is managed by the sender. In most cases this is 
a ring buffer. 

This reduces the \emph{Target Information} problem to noticing if there is space left in the buffer. This in turn does not 
have to be done for each data transfer individually, saving us a lot of communication overhead.

\todo{more info}
\begin{itemize}
  \item push
  \item pull
\end{itemize}

The problem of \emph{Transfer Notification} still exits. The receiver needs to notice that new data was written to the buffer.

\todo{more info}
\begin{itemize}
  \item Receive Queue (WriteImm)
  \item Metadata (WriteOff)
  \item Token (WriteRev)
\end{itemize}

While the \emph{Buffered Write} simplifies a lot of problems when using \emph{Write} as a data transfer verb, there are 
downsides to using a buffer to transfer data. Most importantly the buffer is usually not the place the data should finally 
end up in, which forces us to copy the received data again.

For a more detailed analysis of \emph{Buffered Write Protocols} and our implementation of one, see section \ref{sec:conn:buf_write}


\paragraph{Unbuffered Write}
The idea of an \emph{Unbuffered Write Protocol} is to avoid the additional copy which is usually necessary when using a
\emph{Bufferd Write Protocol}. 

This means to be able to transfer data we need to exchange the complete \emph{Target Information} for each message. The basic
wait of exchanging this information stays the same, and for most use cases this should be batchable.

\todo{more info}
\begin{itemize}
  \item push
  \item pull
\end{itemize}

The problem of \emph{Transfer Notifications} stays similar to the \emph{Buffer Write Protocol} with some subtle differences.

\todo{more info}
\begin{itemize}
  \item Receive Queue (WriteImm)
  \item Metadata (Some metadata region for posted buffer - could be merged when using pull target information - could be used to share posted buffers with comp and swap)
  \item Token (Check end of posted buffer)
\end{itemize}


For a more detailed analysis of \emph{Unbuffered Write Protocols} and our implementation of one, see section \ref{sec:conn:direct_write}


\subsubsection{Read}
The \emph{Read} verb is generally very similar to the \emph{Write} verbs. This time it allows the receiver, again with certain
restriction, to read from an arbitrary location in the senders memory, without any interaction of the senders CPU. Again for
any protocol using \emph{Read} as the data transfer verb we need to solve two problems.

\begin{itemize}
  \item For the receiver to be able to read the data, it needs to know that new data is ready and where to read it from.
    We will call that \emph{Read Request}

  \item As the sender is now not directly involved in the data transfer, we need to notify it when the data transfer is 
over. We will again call this  \emph{Transfer Notification}
\end{itemize}

\paragraph{Buffered Read}

\todo{This should be shareable using atomic verbs, but I'm not quite sure how..}

The buffered read works similarly to the \emph{Buffer Write}. This time the \emph{Sender} has a ring buffer where the 
\emph{Receiver} can read from. Similarly to the \emph{Buffer Write} this will reduce the communication overhead.

The problem of \emph{Read Requests} is reduced to a notification that new data is available, as the \emph{Receiver} already
knows where to read the next message from.

\todo{more info}
\begin{itemize}
  \item push (send message that new message is available)
  \item pull (write to meta data, receiver polls it with read - sharable?)
\end{itemize}


The problem of \emph{Transfer Notification} boils down to the receiver updating metadata at the sender.

\todo{more info}
\begin{itemize}
  \item push (Rcv writes to buffer of sender)
  \item pull (Sender pulls from metadata at sender)
\end{itemize}

For a more detailed analysis of \emph{Buffered Read Protocols} and our implementation of one, see section \ref{sec:conn:buf_read}

\paragraph{Unbuffered Read}
The idea of an \emph{Unbuffered Read Protocol} is again to avoid the additional copy which is usually necessary when using a
\emph{Bufferd Read Protocol}. 

This means to be able to transfer data we need to exchange the complete \emph{Read Request} for each message. 

\todo{more info}
\begin{itemize}
  \item push (send message that new message is available and where)
  \item pull (write to meta data, receiver polls it with read - sharable?)
\end{itemize}

For the \emph{Transfer Notification} we need to inform the sender that a specific buffer is free to be reused.

\todo{more info}
\begin{itemize}
  \item push (write token to read buffer)
  \item pull (write freed buffers to a list?)
\end{itemize}

For a more detailed analysis of \emph{Unbuffered Read Protocols} and our implementation of one, see section \ref{sec:conn:direct_read}

\pagebreak
\subsection{Model}
We looked at what is involved in an RDMA transmission in Figure \ref{fig:seq-sndrcv}.\todo{Figure in RDMA background}
Trying to closely model this however gives us a far too complex model, with a lot of parameters that are hard to assess,
especially if we start to look at more complex protocols. 

We simplify our model by using a modified \emph{LogGP model}~\cite{}. The LogGP model was designed to model point to point
communication with variable message sizes. \todo{Intro to LogGP}


The LogGP model was designed for classical IP based messaging and did not take into account any offloading to the NIC. We 
can however integrate the heavy offloading happening in RDMA by splitting the offset $o$ into multiple offsets for each
of the components. This leaves us with the following parameters.


\begin{itemize}
  \item $L$: an upper bound on the Latency, incurred in sending a message from the senders NIC to the receivers NIC
  \item $G$: the Gap per byte for long messages. For our purposes this is the time of sending a single byte given the 
    maximum bandwidth of our link.
  \item $g$: he gap, defined as the minimum time interval between consecutive message transmissions.
  \item $P$: the number of processes (or servers).
  \item $o_{snd}$: the \emph{send overhead}, defined as the length of time that a processor is engaged in sending each message.
  \item $o_{nsnd}$: the \emph{send NIC overhead}, defined as the length of time that a NIC is engaged in sending each message.
  \item $o_{rcv}$: the \emph{receive overhead}, defined as the length of time that a processor is engaged in receiving each message.
  \item $o_{nrcv}$: the \emph{receive NIC overhead}, defined as the length of time that a NIC is engaged in receiving each message.
  \item $g_{rcv}$: the \emph{send gap}, defined as the time interval between the receiving processor having received a message 
    and it being ready to process the next message. One example of this gap would be preparing a new receive buffer.
\end{itemize}

\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}[node distance=1cm,auto,>=stealth']
  \node[] (p0) {P0};
  \node[right of=p0, node distance=10cm] (p0_g) {};
  \draw[dotted] (p0) -- (p0_g);

  \node[below of=p0, node distance=0.5cm] (p0_nic) {P0 NIC};
  \node[right of=p0_nic, node distance=10cm] (p0_nic_g) {};
  \draw[dotted] (p0_nic) -- (p0_nic_g);

  \node[below of=p0_nic, node distance=1.5cm] (p1_nic) {P1 NIC};
  \node[right of=p1_nic, node distance=10cm] (p1_nic_g) {};
  \draw[dotted] (p1_nic) -- (p1_nic_g);

  \node[below of=p1_nic, node distance=0.5cm] (p1) {P1};
  \node[right of=p1, node distance=10cm] (p1_g) {};
  \draw[dotted] (p1) -- (p1_g);
  

  %%%%

  \draw[very thick] (p0) --node[above,scale=0.75,midway]{$o_{snd}$} ($(p0)!0.15!(p0_g)$);
  \draw[very thick] ($(p0_nic)!0.15!(p0_nic_g)$) --node[above,scale=0.75,midway]{$o_{nsnd}$} ($(p0_nic)!0.225!(p0_nic_g)$);


  \path[] ($(p0_nic)!0.225!(p0_nic_g)$) --node[above,scale=0.75,midway]{$G$} ($(p0_nic)!0.26!(p0_nic_g)$);
  \draw[dotted, ->] ($(p0_nic)!0.225!(p0_nic_g)$) -- ($(p1_nic)!0.395!(p1_nic_g)$);

  \path[] ($(p0_nic)!0.26!(p0_nic_g)$) --node[above,scale=0.75,midway]{$G$} ($(p0_nic)!0.295!(p0_nic_g)$);
  \draw[dotted, ->] ($(p0_nic)!0.26!(p0_nic_g)$) -- ($(p1_nic)!0.43!(p1_nic_g)$);

  \path[] ($(p0_nic)!0.295!(p0_nic_g)$) --node[above,scale=0.75,midway]{$G$} ($(p0_nic)!0.33!(p0_nic_g)$);
  \draw[dotted, ->] ($(p0_nic)!0.295!(p0_nic_g)$) -- ($(p1_nic)!0.465!(p1_nic_g)$);

  \draw[dotted, ->] ($(p0_nic)!0.33!(p0_nic_g)$) -- ($(p1_nic)!0.5!(p1_nic_g)$);


  \draw[very thick] ($(p1_nic)!0.5!(p1_nic_g)$) --node[above,scale=0.75,midway]{$o_{nrcv}$} ($(p1_nic)!0.55!(p1_nic_g)$);
  \draw[very thick] ($(p1)!0.55!(p1_g)$) --node[above,scale=0.75,midway]{$o_{rcv}$} ($(p1)!0.6!(p1_g)$);
    
  %%%%

  \draw[dotted] ($(p0_nic)!0.225!(p0_nic_g)$) -- ($(p0_nic)!0.225!(p0_nic_g)+(0,-3)$);
  \draw[dotted] ($(p0_nic)!0.330!(p0_nic_g)$) -- ($(p0_nic)!0.330!(p0_nic_g)+(0,-3)$);
  \draw[<->] ($(p0_nic)!0.225!(p0_nic_g)+(0,-2.8)$) --node[above,scale=0.75,midway]{$(k-1)G$} ($(p0_nic)!0.330!(p0_nic_g)+(0,-2.8)$);

  \draw[dotted] ($(p1_nic)!0.5!(p1_nic_g)$) -- ($(p1_nic)!0.5!(p1_nic_g)+(0,-1.5)$);
  \draw[<->] ($(p0_nic)!0.330!(p0_nic_g)+(0,-2.8)$) --node[above,scale=0.75,midway]{$L$} ($(p1_nic)!0.5!(p1_nic_g)+(0,-1.3)$);

  \draw[dotted] ($(p1)!0.6!(p1_g)$) -- ($(p1)!0.6!(p1_g)+(0,-1)$);
  \draw[dotted] ($(p1)!0.65!(p1_g)$) -- ($(p1)!0.65!(p1_g)+(0,-1)$);
  \draw[<->] ($(p1)!0.6!(p1_g)+(0,-.8)$) --node[above,scale=0.75,midway]{$g_{rcv}$} ($(p1)!0.65!(p1_g)+(0,-.8)$);

  \draw[dotted] ($(p0)!0.15!(p0_g)$) -- ($(p0)!0.15!(p0_g)+(0,.8)$);
  \draw[dotted] ($(p0)!0.35!(p0_g)$) -- ($(p0)!0.35!(p0_g)+(0,.8)$);
  \draw[<->] ($(p0)!0.15!(p0_g)+(0,.6)$) --node[above,scale=0.75,midway]{$g$} ($(p0)!0.35!(p0_g)+(0,.6)$);

  %%%%

  \draw[very thick, lightgray] ($(p0)!0.2!(p0_g)$) --node[above,scale=0.75,midway]{$o_{snd}$} ($(p0)!0.35!(p0_g)$);
  \draw[very thick, lightgray] ($(p0_nic)!0.35!(p0_nic_g)$) --node[above,scale=0.75,midway]{$o_{nsnd}$} ($(p0_nic)!0.425!(p0_nic_g)$);

  \path[lightgray] ($(p0_nic)!0.425!(p0_nic_g)$) --node[above,scale=0.75,midway]{$G$} ($(p0_nic)!0.45!(p0_nic_g)$);
  \draw[dotted, ->, lightgray] ($(p0_nic)!0.425!(p0_nic_g)$) -- ($(p1_nic)!0.625!(p1_nic_g)$);

  \path[lightgray] ($(p0_nic)!0.45!(p0_nic_g)$) --node[above,scale=0.75,midway]{$G$} ($(p0_nic)!0.475!(p0_nic_g)$);
  \draw[dotted, ->, lightgray] ($(p0_nic)!0.45!(p0_nic_g)$) -- ($(p1_nic)!0.65!(p1_nic_g)$);

  \path[lightgray] ($(p0_nic)!0.475!(p0_nic_g)$) --node[above,scale=0.75,midway]{$G$} ($(p0_nic)!0.5!(p0_nic_g)$);
  \draw[dotted, ->, lightgray] ($(p0_nic)!0.475!(p0_nic_g)$) -- ($(p1_nic)!0.675!(p1_nic_g)$);

  \draw[dotted, ->, lightgray] ($(p0_nic)!0.5!(p0_nic_g)$) -- ($(p1_nic)!0.7!(p1_nic_g)$);


  \draw[very thick, lightgray] ($(p1_nic)!0.7!(p1_nic_g)$) --node[above,scale=0.75,midway]{$o_{nrcv}$} ($(p1_nic)!0.75!(p1_nic_g)$);
  \draw[very thick, lightgray] ($(p1)!0.75!(p1_g)$) --node[above,scale=0.75,midway]{$o_{rcv}$} ($(p1)!0.8!(p1_g)$);



\end{tikzpicture}
\end{center}
\caption{Sending an receiving messages under our model}
\label{fig:model-base}
\end{figure}


\paragraph{Latency Estimate}

Using this model we can estimate the latency $t$ of transferring a single message $m$ of size $k$ with:

$$
t \geq o_{snd} + o_{nsnd}  + (k-1)G + L + o_{nrcv} + o_{rcv}
$$


\paragraph{Throughput Estimate}

Message transfer is highly pipelined. So to estimate bandwidth $bw$ our model basically reduces to finding the bottleneck.

$$
bw \leq \max (\frac{1}{g}, \frac{1}{o_{snd}}k, \frac{1}{o_{nsnd}}k + G, \frac{1}{o_{nrcv}}k + G, \frac{1}{o_{rcv} + g_{rcv}}k)
$$
