\section{Performance Model}\label{sec:perf-model} \label{sec:model}
In this section we introduce a general performance model for data exchange protocol that will allow us to estimate the 
performance of our protocol implementations. It gives us a better understanding of our evaluation results and helps us 
to locate bottlenecks.

\paragraph{}We took a fairly detailed look at the operations involved in transmitting a message using RDMA in Section~\ref{sec:rdma}.
Trying to closely model this however gives us a far too complex model, with a lot of parameters that are hard to assess,
especially if we start to look at more complex protocols. We introduce a more simplified model inspired by the  
\emph{LogGP model}~\cite{loggp}, which was developed  to model point to point communication with variable message sizes for 
traditional IP based networks.

\paragraph{} At its core the LogGP model uses a fixed CPU overhead $o$ per message, the communication latency $L$, and 
the bandwidth $G$. While this results in a decent model IP based system, it is unable to model the heavy NIC offloading 
happening in RDMA. We extended the LogGP model by splitting the offset $o$ into multiple offsets for each of the components.
We also largely ignored the number of processes $P$ and use a slightly different definition for the gap $g$. This leaves 
us with the following parameters, which are illustrated in Figure~\ref{fig:model-base}.

\begin{figure}[!htp]
\begin{center}
\begin{tikzpicture}[node distance=1cm,auto,>=stealth']
  \node[] (p0) {P0};
  \node[right of=p0, node distance=10cm] (p0_g) {};
  \draw[dotted] (p0) -- (p0_g);

  \node[below of=p0, node distance=0.5cm] (p0_nic) {P0 NIC};
  \node[right of=p0_nic, node distance=10cm] (p0_nic_g) {};
  \draw[dotted] (p0_nic) -- (p0_nic_g);

  \node[below of=p0_nic, node distance=1.5cm] (p1_nic) {P1 NIC};
  \node[right of=p1_nic, node distance=10cm] (p1_nic_g) {};
  \draw[dotted] (p1_nic) -- (p1_nic_g);

  \node[below of=p1_nic, node distance=0.5cm] (p1) {P1};
  \node[right of=p1, node distance=10cm] (p1_g) {};
  \draw[dotted] (p1) -- (p1_g);
  

  %%%%

  \draw[very thick] (p0) --node[above,scale=0.75,midway]{$o_{snd}$} ($(p0)!0.15!(p0_g)$);
  \draw[very thick] ($(p0_nic)!0.15!(p0_nic_g)$) --node[above,scale=0.75,midway]{$o_{nsnd}$} ($(p0_nic)!0.225!(p0_nic_g)$);


  \path[] ($(p0_nic)!0.225!(p0_nic_g)$) --node[above,scale=0.75,midway]{$G$} ($(p0_nic)!0.26!(p0_nic_g)$);
  \draw[dotted, ->] ($(p0_nic)!0.225!(p0_nic_g)$) -- ($(p1_nic)!0.395!(p1_nic_g)$);

  \path[] ($(p0_nic)!0.26!(p0_nic_g)$) --node[above,scale=0.75,midway]{$G$} ($(p0_nic)!0.295!(p0_nic_g)$);
  \draw[dotted, ->] ($(p0_nic)!0.26!(p0_nic_g)$) -- ($(p1_nic)!0.43!(p1_nic_g)$);

  \path[] ($(p0_nic)!0.295!(p0_nic_g)$) --node[above,scale=0.75,midway]{$G$} ($(p0_nic)!0.33!(p0_nic_g)$);
  \draw[dotted, ->] ($(p0_nic)!0.295!(p0_nic_g)$) -- ($(p1_nic)!0.465!(p1_nic_g)$);

  \draw[dotted, ->] ($(p0_nic)!0.33!(p0_nic_g)$) -- ($(p1_nic)!0.5!(p1_nic_g)$);


  \draw[very thick] ($(p1_nic)!0.5!(p1_nic_g)$) --node[above,scale=0.75,midway]{$o_{nrcv}$} ($(p1_nic)!0.55!(p1_nic_g)$);
  \draw[very thick] ($(p1)!0.50!(p1_g)$) --node[above,scale=0.75,midway]{$o_{free}$} ($(p1)!0.545!(p1_g)$);
  \draw[very thick] ($(p1)!0.55!(p1_g)$) --node[above,scale=0.75,midway]{$o_{rcv}$} ($(p1)!0.6!(p1_g)$);
    
  %%%%

  \draw[dotted] ($(p0_nic)!0.225!(p0_nic_g)$) -- ($(p0_nic)!0.225!(p0_nic_g)+(0,-3)$);
  \draw[dotted] ($(p0_nic)!0.330!(p0_nic_g)$) -- ($(p0_nic)!0.330!(p0_nic_g)+(0,-3)$);
  \draw[<->] ($(p0_nic)!0.225!(p0_nic_g)+(0,-2.8)$) --node[above,scale=0.75,midway]{$(k-1)G$} ($(p0_nic)!0.330!(p0_nic_g)+(0,-2.8)$);

  \draw[dotted] ($(p1_nic)!0.5!(p1_nic_g)$) -- ($(p1_nic)!0.5!(p1_nic_g)+(0,-1.5)$);
  \draw[<->] ($(p0_nic)!0.330!(p0_nic_g)+(0,-2.8)$) --node[above,scale=0.75,midway]{$L$} ($(p1_nic)!0.5!(p1_nic_g)+(0,-1.3)$);


  \draw[dotted] ($(p0)!0.15!(p0_g)$) -- ($(p0)!0.15!(p0_g)+(0,.8)$);
  \draw[dotted] ($(p0)!0.65!(p0_g)$) -- ($(p0)!0.65!(p0_g)+(0,.8)$);
  \draw[<->] ($(p0)!0.15!(p0_g)+(0,.6)$) --node[above,scale=0.75,midway]{$g$} ($(p0)!0.65!(p0_g)+(0,.6)$);

  %%%%

  \draw[dotted, ->, lightgray] ($(p1_nic)!0.56!(p1_nic_g)$) -- ($(p0)!0.65!(p0_g)$);


\end{tikzpicture}
\end{center}
\caption{Sending an receiving messages under our model}
\label{fig:model-base}
\end{figure}




\begin{itemize}
  \item $L$: the network Latency, incurred by sending a message from the senders NIC to the receivers NIC. In our model 
    this also includes the PCI latency.
  \item $G$: the gap per byte for long messages. For our purposes this is the time of sending a single byte given the 
    maximum bandwidth of our link.
  \item $o_{snd}$: the \emph{send overhead}, defined as the length of time that a processor is engaged in sending each message.
    For some protocols this also includes any preparation an communication overhead necessary to send a message.
  \item $g$: the \emph{send gap}, defined as the minimum time interval until the sender can reuse the resources involved in 
    the transmission. (e.g. the send buffer) 
  \item $o_{nsnd}$: the \emph{send NIC overhead}, defined as the length of time that a NIC is engaged in sending each message.
  \item $o_{nrcv}$: the \emph{receive NIC overhead}, defined as the length of time that a NIC is engaged in receiving each message.
  \item $o_{rcv}$: the \emph{receive overhead}, defined as the length of time that a processor is engaged in receiving each message.
  \item $o_{free}$: the \emph{freeing overhead}, defined as the time that a processor is engaged in preparing a receive buffer
    to receive into (e.g. post receive)
\end{itemize}

\paragraph{} This model completely ignores the PCI bandwidth and latency and essentially integrates it into the network latency
and bandwidth. It also does not take the MTU into account. While some prior work use a slightly different model when sending 
messages over the MTU, that span multiple transmission units~\cite{dare}, we found our model to work well enough to understand
most of our results.

\paragraph{} We built this model around send or write based protocols and it does not seem to be a very good representation 
for read based protocols. But as we will see it can still give us better insight in the observed performance, even if 
it was not explicitly designed for them.

\paragraph{} Even with these simplifications it still is a fairly complex model and measuring each of these overheads in 
practice is very hard. We do not try to quantitatively evaluate our model for all presented protocols, but rather use it 
get a better general understanding of the observed performance characteristics and to understand what is limiting throughput
or latency in which situations.

\subsection{Evaluating the Model}

\paragraph{} We evaluate the model in this section for the send-receive protocol only. Table~\ref{tab:model} shows our model
parameter estimations.

While we will see that our model is not quite able to accurately predict our bandwidth and latency measurements, it does 
give us a better understanding of our results and predicts the different bottlenecks we encounter in our evaluation.

\begin{table}[!ht]
\setlength\tabcolsep{1.5pt}
\centering
 \begin{tabular}{|x{2.15cm}|x{2cm}|x{2cm}|} % I specify the sizes of columns. x is for centering and p is  for left
 \hline
 Parameter    &  seq & batched \\
  \hline
  \hline
 $o_{snd}$    & $0.12 \mu s$ &  $0.016 \mu s$\\
  \hline
 $o_{rcv}$    &  & $0.007 \mu s$\\
  \hline
 $o_{free}$   &  & \\
  \hline
 $o_{nsnd}$   & $0.02 \mu s$ &  \cellcolor{black!40} \\
  \hline
 $o_{nrcv}$   & $0.02 \mu s$ &  \cellcolor{black!40} \\
  \hline
 $L$          & $1.9 \mu s$ &  \cellcolor{black!40}\\
  \hline
 $g$          & $\geq 3.4 \mu s$  & \cellcolor{black!40}\\
 \hline
 $G$          & $0.2\mu s / KB$  & $0.084\mu s / KB$\\

\hline
\end{tabular}
\caption{Model parameter for send-receive protocol}
\label{tab:model}
\end{table}

\subsubsection{Predicting Latency}

Predicting latency using our model essentially means evaluating the parameters in the critical path and adding them up. 
The predicted latency $t$ of transferring a single message of size $k$ is:

$$
t \geq o_{snd} + o_{nsnd}  + (k-1)G + L + o_{nrcv} + o_{rcv}
$$

Both $o_{snd}$ and $o_{rcv}$ can be directly measured. Evaluating the rest of the parameters however is a bit trickier. 
\comment{How did you evaluate network latency?} With the latency an bandwidth measurements we had and the assumption that
$o_{nsnd} = o_{nrcv}$ we come up with parameter estimations seen in Table~\ref{tab:model}. 
The assumption that $o_{nsnd}$ is the same as $o_{nrcv}$ is most likely not correct, but gives us a good enough estimation
for a 1:1 communication pattern.


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{send-lat-msgsize.png}
  \caption{Send-Receive latency and model prediction}
    \label{fig:model-lat}
\end{figure}

\paragraph{} Figure~\ref{fig:model-lat} shows our prediction and the actual latency measurements for the send-receive protocol.
We can see that we are not quite able to precisely match the actual data, most likely caused by our simplifications. It does
however confirm our prediction of a mostly linear increase in latency with increased message size.


\subsubsection{Predicting Bandwidth}
Predicting maximum bandwidth is not quite as simple as adding up all parameters. The model predicts three different kinds of
bottlenecks. We take a look at each of these bottlenecks and illustrate the send-receive protocol in Figure~\ref{fig:model-bw}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{send-bw-msgsize.png}
  \caption{Send-Receive bandwidth and model prediction}
    \label{fig:model-bw}
\end{figure}

\paragraph{Round-Trip bottleneck} One way to be bottlenecked is by simply not issuing enough concurrent send requests. In 
this case we are bottlenecked by the gap $g$, which usually contains a complete round-trip time and grows linearly with 
the size of the message. So when we issue only a single send at a time, for a message size of $k$ we predict to be limited by

$$
bw \leq \frac{k}{o_{snd} + g}\text{, where } g = g_{fix} + kg_{var} 
$$

For the send-receive protocol this is exactly what we see. SR-Seq shows the throughput with varying message size when only 
sending a single message at a time. 

\paragraph{} This means to get decent 1:1 performance it is vitally important to issue enough concurrent requests, or as we 
will call it from now on to allow for enough \emph{unacknowledged} messages. In a lot of related work this is not discussed
and either done without specifically mentioning it or sometimes the presented protocol only allows for a single unacknowledged
message, without bringing up the disadvantages of such an approach.


\paragraph{CPU bottleneck} Another bottleneck we encounter especially for small messages, is a CPU bottleneck, either at the 
sender or receiver.

A sending CPU bottleneck means we simply are not able to issue enough send requests and the RNIC is processing them faster
than we post them. This results in a predicted limit of

$$
bw \leq \frac{k}{o_{snd}}
$$



This happens surprisingly often for smaller messages. The send-receive protocol with a sufficient amount
of unacknowledged messages (SR) shows this performance characteristic in Figure~\ref{fig:model-bw}. We can see the linearly
increasing throughput we predict until we reach a message size of 2 KB where we start to be limited by the NIC.

\paragraph{} We can avoid this bottleneck by introducing batching. The verbs API allows us to post multiple work request at 
the same time. This \emph{Doorbell batching} reduces the number of generated MMIOs~\cite{anuj-guide} and reduces the CPU load.
By our measurements batching can reduce the sending CPU overhead $o_{snd}$ by up to a factor of 10. 

When introducing doorbell batching to the send-receive protocol we never seem to be bottlenecked by the sending CPU as 
can be seen for the batched measurement SR-Bat in Figure~\ref{fig:model-bw}.

Although this can drastically improve bandwidth, we will not evaluate other protocols with doorbell batching, as it is
not directly applicable to some of the protocols and for connected QPs application level batching, i.e. sending larger 
messages, is a better approach.

\paragraph{} A receiving CPU bottleneck means the receiver is unable to prepare enough receive buffers, staling the sender. 
This gives us a predicted limit of

$$
bw \leq \frac{k}{o_{rcv} + o_{free}}
$$

This results in a very similar bottleneck which is proportional to the message size. We do not encounter this bottleneck for
our 1:1 evaluation of the send-receive protocol, however when a single receiving thread handles multiple open connection 
in a N:1 communication pattern this quickly becomes the key bottleneck.

Similarly to the sending CPU bottleneck the impact can be reduced using batching.


\paragraph{Device Bottleneck} Finally if we are able to issue enough requests, and the receiver is not stalling the sender,
we are bottlenecked by either one of the involved RNICs. According to our model this results in one of these two limits

$$
bw \leq \frac{k}{o_{nsnd} + (k-1)G} \text{\quad or \quad} bw \leq \frac{k}{o_{nrcv} + (k-1)G}
$$

In both cases with increasing message size we first predict a linear increase in bandwidth which will eventually 
flatten out into the maximum possible goodput of the network link.

Looking at the batched throughput results SR-Bat in Figure~\ref{fig:model-bw}, this is qualitatively what we expect 
when being only limited by the RNIC. We are however not able to accurately predict the results for smaller messages 
using our current model. This is a limitation of our model, probably caused by ignoring the PCI interactions or possibly
by not modeled on NIC batching.
