
\section{Buffered Read}\label{sec:conn:buf_read}
\subsection{Design}

The idea of a buffered read protocol is to have a ring buffer at the sender from which the receiver fetches the messages using
RDMA reads. There are multiple different ways to implement such a protocol, with the main variations being in, how to notify
the receiver of new messages, where to transfer them to, and how to acknowledge to the sender that a message has been processed.

\paragraph{} We decided to focus on an implementation which gives us a \emph{Passive Sender} and allows for 
\emph{Variable Message Sizes}. We decided to stick with the basic interface defined in Section~\ref{sec:protocols}. This 
results in a system with two ring buffers, illustrated in Figure~\ref{fig:buf_read_struct}.

\begin{figure}[!ht]
\centering
\begin{subfigure}[b]{0.49\textwidth}
\begin{tikzpicture}[>=latex,font=\sffamily,semithick,scale=1]
    \draw[rounded corners] (-2.5, -2) rectangle (2.5, 2.5) {};
    \node[align=center] at (0, 2) {\large Sender};

    \fill [black!35] (0,0) -- (200:1) arc [end angle=-15, start angle=200, radius=1] -- cycle;

    \draw [thick] (0,0) circle (1cm);

    \draw[dashed] (90:1) -- (0:0);
    \draw (200:1) -- (0:0);
    \draw (-15:1) -- (0:0);
    \node [circle,thick,fill=white,draw=black,align=center,minimum size=1.5cm] at (0,0) {};


    \draw [<-,black] (200:1) -- (200:1.25) -- +(-.1,0)
      node [black,left,inner xsep=.1cm] (Head) {\small \code{head}};
    \draw [<-,black] (-15:1) -- (-15:1.25) -- +(.1,0)
      node [black,right,inner xsep=.1cm] (Tail) {\small \code{tail}};
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
\begin{tikzpicture}[>=latex,font=\sffamily,semithick,scale=1]
    \draw[rounded corners] (-3, -2) rectangle (3, 2.5) {};
    \node[align=center] at (0, 2) {\large Receiver};

    \fill [black!35] (0,0) -- (130:1) arc [end angle=55, start angle=130, radius=1] -- cycle;

    \fill [black!20] (0,0) -- (200:1) arc [end angle=160, start angle=200, radius=1] -- cycle;
    \draw [thick] (0,0) circle (1cm);

    \draw[dashed] (90:1) -- (0:0);
    \draw (200:1) -- (0:0);
    \draw (175:1) -- (0:0);
    \draw (160:1) -- (0:0);

    \draw (130:1) -- (0:0);
    \draw (75:1) -- (0:0);
    \draw (55:1) -- (0:0);
    \node [circle,thick,fill=white,draw=black,align=center,minimum size=1.5cm] at (0,0) {};


    \draw [<-,black] (130:1) -- (130:1.25) -- +(-.1,0)
      node [black,left,inner xsep=.1cm] (rptr) {\small \code{read\_pointer}};
    \draw [<-,black] (200:1) -- (200:1.25) -- +(-.1,0)
      node [black,left,inner xsep=.1cm] (Head) {\small \code{head}};
    \draw [<-,black] (55:1) -- (55:1.25) -- +(.1,0)
      node [black,right,inner xsep=.1cm] (Tail) {\small \code{remote\_tail}};
\end{tikzpicture}
\end{subfigure}
\caption{Structure of the buffered read connection}
\label{fig:buf_read_struct}
\end{figure}

\subsubsection{Sender}
As mentioned the sender of our buffered read connection is entirely passive, that means after the connection setup the sending
CPU does not issue any RDMA operations. The only thing it needs to do to send is to check its head if there is enough space
left to write its the message to its ring buffer and write to its tail if there is. It also prepends the message size when 
writing. The head is completely managed by the receiver and the tail pointer needs to be accessible to it.

\paragraph{} That means for the sender transmitting a message does only involve copying it to its local memory. This results in
a very low sending overhead.

\subsubsection{Receiver}
A completely passive sender means the receiver has to do the heavy lifting. The receiver manages three different pointers: 
the \code{read\_pointer} which marks the beginning of the next message to receive, the \code{remote\_tail}, which is a cache of
the senders tail and marks the end of the transmitted buffer section, and the \code{head} which marks the beginning of 
the earliest not freed message.

\paragraph{} Receiving a message now involves the following. We first check if there are already transmitted messages that 
we have not received yet. Meaning the \code{read\_pointer} is not equal to the \code{remote\_tail}. If so, we can simply 
return the next message with the length at the beginning of the buffer and update the \code{read\_pointer}.

If there are no transmitted messages to return, the receiver updates the \code{remote\_tail} with an RDMA read. If the remote
tail has not updated it retries until it has. With the updated tail the receiver issues a RDMA read for the whole buffer 
section between the read pointer and the updated tail. It then returns the next message in the newly transmitted section.

This gives us an interesting characteristic of most receives being very cheap, while some are very expensive as the need to 
perform a large read. We did not implement preemptive issuing of reads which could reduce this effect and improve performance
significantly.

\paragraph{} To keep track of the freed buffers at the receiver we use the same linked list structure we used in the buffered
write connection. This allows us to update the \code{head} accordingly when freeing messages out of order and freeing will
occasionally update the remote head using an RDMA write operation.

\paragraph{} For both ring buffer we use the same \emph{"Magic Buffer"} trick introduced in Section~\ref{sec:conn:buf_write} to 
allow us to wrap around the end of the buffer.

\subsection{Evaluation}

\subsubsection{Latency}

Figure~\ref{fig:plot-bufread-lat} shows the point to point latency of the buffered read protocol. As always this is half 
a pingpong latency. That means each read only transfers a single message and each transfer requires at least one additional 
RDMA read to to notice the new tail.

\paragraph{} The latency is very high even for very small messages. This is caused by the minimum of two reads per message and
the fact that RDMA reads have a fairly high and constant latency for messages of up 1024 byte length. This also gives us the
characteristic of a more or less constant latency up to a message size of 1024 bytes and the only slowly increasing latency
afterwards.


\begin{figure}[htp]
  \centering
\begin{subfigure}[b]{0.49\textwidth}
  \includegraphics[width=1\textwidth]{buf-read-lat-msgsize.png}
  \caption{Latency}
  \label{fig:plot-bufread-lat}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{buf-read-bw-msgsize.png}
  \caption{Bandwidth}
  \label{fig:plot-bufread-bw}
\end{subfigure}
\caption{Measurements between two nodes with varying message size}
\end{figure}

\subsubsection{Bandwidth}

Figure~\ref{fig:plot-bufread-bw} shows the point to point throughput of the Buffered Read connection, with varying message 
sizes. We also plot the \emph{mean transfer size}, so the data transfered by a single read.

\paragraph{} As expected with increasing message size the mean transfer size also increases linearly and with that the 
total bandwidth also increases until we hit a maximum at around 70 Gbit/s and a transfer size of about 100KB. After that 
the performance starts to degrade. \comment{Why? I can't explain it really. I'm having a hard time in general to explain 
this graph}

We can also see that we achieve significantly better performance for very small messages compared to the other protocols we
look at in this thesis. This is caused by automatic batching behaviour of this protocol.

\paragraph{} Performance of our protocol in a single connection situation could be drastically improved by limiting the 
maximum transfer size and preemptively issuing reads to fill up the pipeline and not having to explicitly wait for each
read to complete.


\subsubsection{Multithreading}

Figure~\ref{fig:plot-bufread-bw-threads} shows the protocols performance with multiple open connections, each with a dedicated
thread to receive messages. The throughput is again evaluated for three different message 
sizes: 16 bytes, 512 bytes, and 8192 bytes. 

\paragraph{} We see drastically improved performance compared to the single connection evaluation. This is mainly caused by
the fact that we now have multiple concurrent active read operations. Through the inbuilt message batching we achieve 
significantly higher throughput for small messages compared to the other connection types we evaluated.

\paragraph{} Throughput for all message sizes grow linearly with increasing number of connections and are only limited by the
 line rate or eventually by the individual copying of buffers at the sender and its function call overhead.

\begin{figure}[ht]
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{buf-read-bw-threads.png}
  \caption{Using $N$ receivers}
  \label{fig:plot-bufread-bw-threads}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{buf-read-bw-n1.png}
  \caption{Using a single receiver}
  \label{fig:plot-bufread-bw-n1}
  \end{subfigure}
  \caption{Bandwidth with varying number of connections $N$}
  \comment{We need to recheck the 512 data point. This one looks like a measurement error} 
\end{figure}

\subsubsection{Single Receiver}

Figure~\ref{fig:plot-bufread-bw-n1} shows the protocols throughput with a single receiver and varying number of senders, 
again for three different message sizes. The single receiver simply round robins over the open connections.

\paragraph{} We can see that in this case we are again very much limited by the receiver and achieve basically the same
performance as when using a single connection. This is exactly what we expect as receiving from multiple connections 
behaves the same way as receiving from a single connection.

\paragraph{} There seems to be a performance degradation when increasing the number of senders. This can for one be explained
by increased cache misses and more importantly for large messages the same performance drop we see in the single connection 
benchmark when further increasing the transfer size.

\subsection{Conclusion}
The Buffered Read connection is probably the most different protocol compared to the other connections we evaluate in this
thesis. Our design ended up very similar to a \emph{Flow Structure}~\cite{sharma2020design}.

The protocol gives us a completely \emph{Passive Sender} and through the inbuilt batching achieves the best throughput for 
very small messages, but this also results in very high latency.

\paragraph{} With more optimization work and by relaxing some of the interface requirements, we could potentially solve 
some of the performance limitations we encountered and we think this could result in near line rate throughput for nearly 
all message sizes.

For future work it would also be interesting to where multiple threads can write to and receive from a single connection 
by atomically updating head or tail pointers. There could then be a dedicated thread at the receiver to handle the network
transfer.

\paragraph{} In the end the Buffered Read protocol has the potential to achieve very hight throughput by sacrificing low 
latency. Such a protocol could be interesting for high volume systems that do not require extremely low latency, like some
stream processing systems.
