\section{Data Exchange Protocols}\label{sec:protocols}
The goal of this work is design, implement, and study a comprehensive list of RDMA based protocols for data exchange. 
However without a clear definition of \emph{data exchange} the number of possible protocols is virtually endless.

In this section we limit our definition of \emph{data exchange protocol} to a more clearly defined message passing
protocol, which reduces the size of our design space and allows us to compare protocols. We then introduce
features outside of raw performance that have been relevant for real-world applications. Finally we introduce six 
protocols that all use different RDMA features and design approaches to implement message passing protocols with different 
features and performance guarantees.

\subsection{Definition}\label{sec:proto-def}

Most research focuses on RPCs \cite{anuj-guide, fasst, herd}, remote datastructure access \cite{pilaf, farm}, or replicate 
socket like interfaces \cite{socksdirect}. In this thesis we look at message passing protocols with fairly few guarantees.
We believe with this more general communication model we can give engineers and researchers a better understanding of the 
building blocks that can be  used to build more specific connection protocols, like RPCs, while not only 
micro-benchmarking RDMA verbs.


\begin{defn}
We define a message passing protocol $P$ as an algorithm for moving a message $m$ that resides in the memory of
node $N_a$ to another node $N_b$. The transfer must be initiated by $N_a$. After the transfer, $N_b$ must know that the 
message was fully received and $N_a$ must know that it can reuse $m$.
\end{defn}

We explicitly have no synchronisation requirements, so the sender does not need to know whether the receiver has actually 
received the message.



\pagebreak

\subsection{Features}

There is more to data exchange protocols than raw throughput or latency. Oftentimes it is more important for an application
that the connection is \emph{non-blocking} or that the memory requirements do not grow too large, even if that protocol wastes
a few microseconds.

To be able to better compare our presented protocols we define a few traits that are often required by systems and
analyse which protocols provide these traits

\paragraph{True Zero-Copy} Whether the protocol can be employed to receive directly to the destination buffer. While RDMA 
claims to be zero-copy, most protocols are not truly zero-copy. Protocols using ringbuffers or mailboxes are
strictly speaking not zero-copy, as the receiver will always have to copy the data from this buffer to its actual destination.
This is can be especially important when transferring a large amount of data that does not have to be further processed.

\paragraph{Variable Message Size} That the protocol allows us to send messages with different sizes, without using the 
complete buffer at the receiver. Ring buffers for example can be designed to only use the necessary space per message
while send receive will always use the complete receive buffer, no matter how small the message actually is. Variable
message sizes allow us to avoid memory fragmentation and reduce total memory usage in general.

\paragraph{Passive Sender/Receiver} Whether the sender or receiver is \emph{passive}. We define \emph{passivity} as not 
requiring any CPU involvement in the actual data transfer. That means a \emph{passive receiver} can receive messages by
simply reading from a memory location, without having to acknowledge anything or posting receive buffers. \emph{Passivity} 
reduces the necessary CPU usage to a minimum and can be useful for applications with heterogenous CPU requirements.

\paragraph{Interrupts} Whether there is some kind of notification system, that allows the receiver to be notified 
of an incoming message without having to constantly poll for it. While polling gives us better performance in almost all cases
constantly polling wastes a lot of CPU cycles when we do not receive a lot of messages.

\paragraph{Resource Sharing} If multiple connections can share resources, especially memory. We are using \emph{reliable connections}
so we inherently need to use one \emph{QP} per sender and receiver pair, but we can reduce memory usage by sharing receive 
buffers between multiple connections. Without any memory sharing, utilization grows linearly with the number of QPs, which 
can quickly lead to unacceptable memory requirements, especially for $N$ to $N$ communication patterns.

\paragraph{Non-Blocking} By non-blocking we mean that a single not freed buffer cannot block a complete connection. In systems
is it very common to distribute incoming messages messages to different threads. It is important that a single slow running 
task cannot completely block the system of making progress. The way we use it a ring buffer is a good example of a blocking
behaviour. If a single buffer segment is not marked as available to be reused it may block the whole buffer. A send receive 
based connection however is non-blocking. As long as there is at least one posted receive buffer, the connection is able to
make progress.




\subsection{Design Space} \label{sec:proto-ds}


Even with the more limited definition of \emph{data exchange} in Section~\ref{sec:proto-def} there are countless numbers
of non trivial ways to implement such a protocol using RDMA features. In this section we take a look at the design 
space of data exchange protocols, we show different approaches and point out specific protocols that we implemented
as an inspiration for other developers to design their own protocols.


\paragraph{} One way to split all possible exchange protocols is by the verbs they use to transfer the actual 
data, and we look at each of these verbs separately, as these all work in different ways and have different requirements

We did not see any advantages in splitting the actual data transfer into multiple verbs, as this only seems to add overhead 
and complexity. And while it would theoretically be possible to use \emph{Compare and Swap} for generic data transfer,
the expected performance is far too low to get a viable protocol. So we do not look into these options but focus on the 
following verbs.

\paragraph{} We want to point out that this is in no way an exhausting list of protocols, but should rather show that the 
design space is very large and that there are a lot of interesting protocols that can still be explored.

\subsubsection{Send-based Protocols}
The \emph{Send} verb basically fulfills our requirements for a \emph{data exchange protocol} out of the box. It is sender
initiated, the sender can reuse the buffer after the send is complete, and the receiver is notified of a new message by
a completion event.

There are however still multiple ways to tweak and tune a send-receive-based protocol by batching, inline sending, or 
sharing receive buffers between multiple connections using a \emph{Shared Receive Queue}. We explore these implementation 
details and their performance characteristics in Section \ref{sec:conn:send}.

\subsubsection{Write-based Protocols}
The \emph{Write} verb is a lot less restrictive. It allows the sender, with certain restrictions, to write at an arbitrary 
location in the receivers memory, without any interaction of the receivers CPU. This also means we need to solve some of the 
problems which the \emph{Send} verb solved for us.

\begin{itemize}
  \item The sender needs to know where to send it to and if the receiver is ready to receive the data, to safely transfer data.

  \item The receiver needs to notice that it received data.
\end{itemize}

We look at and implement two general approaches for write-based protocols, a \emph{buffered} and a \emph{unbuffered} approach.

\paragraph{} The idea of a buffered read connection is to have a dedicated, structured buffer at the receiver. In most cases 
this is a ring buffer. This allows the sender to always know where to write to without any communication overhead. There is
however there are downsides to using a buffer to transfer data. Most importantly the buffer is usually not the place the 
data should finally end up in, which forces us to copy the received data again.

There is a more detailed analysis of \emph{Buffered Write Protocols} with our implementation of one using a ring buffer in 
Section \ref{sec:conn:buf_write}. Further in Section \ref{sec:conn:shared_write} we look at a way to share this ring
buffer between multiple connections using \emph{fetch and add}.


\paragraph{} The  \emph{Unbuffered Write Protocol} avoids the additional copy which is usually necessary when 
using a \emph{Buffered Write Protocol}. The receiver should be able to decide the target location for each message. 
That means there is a communication overhead for each message, as the sender either need to query the receiver where to 
write to or the receiver needs to preemptively send locations to write to.

For a more detailed analysis of \emph{Unbuffered Write Protocols} and our implementation of one, 
see Section \ref{sec:conn:direct_write}


\paragraph{}Each of these two write-based protocol approaches can again be implemented in a lot different ways which can
in turn have drastically different performance characteristics. 


\subsubsection{Read-based Protocols}
The \emph{Read} verb is generally very similar to the \emph{Write} verbs. This time it allows the receiver, again with certain
restriction, to read from an arbitrary location in the senders memory, without any interaction of the senders CPU. Any protocol
using \emph{Read} as the data transfer verb we need to solve two problems.

\begin{itemize}
  \item The receiver needs to know that new data is ready and where to read it from.
  \item The sender needs to be notified when the data transfer is completed.
\end{itemize}

We again looked at and implemented a buffered as well as an unbuffered approach to a read-based protocol. 


The buffered read works similarly to the buffer write approach. This time the \emph{sender} has a structured buffer where the 
\emph{Receiver} can read from. This reduces the communication overhead as the receiver always knows where to read the next 
message from. Section \ref{sec:conn:buf_read} contains a more detailed look at \emph{Buffered Read Protocols} and our 
implementation of one.

A \emph{Unbuffered Read Protocol} avoids the additional copy which is usually necessary when using a buffer. This means the 
sender needs to notify the receiver when a new message arrives and where to read it from.  This introduces a large
communication overhead but by being truly zero-copy this approach can be efficient, especially for large messages.

We present our implementation of an \emph{Unbuffered Read Protocol} in Section~\ref{sec:conn:direct_read}


\pagebreak
\subsection{Interface}

One of our goals is to be able to compare the different protocols we evaluate, we introduce a programming 
interface that is implemented by nearly all of the six presented connection types. In this section we introduce this interface
and describe a few implementation details that are shared by almost all of our connections.

\subsubsection{Sender}

Our protocol definition requires that the sender initiates the transfer. To achieve optimal performance it is
necessary to avoid unnecessary copies and it is very important to not block on a single transfer, but to give a RNIC multiple
requests to work on and fill up its processing pipeline. 

This results roughly in the interface shown in Listing~\ref{list:sender}. Where we send a message by providing a reference
to a previously allocated memory region, which asynchronously starts the message transfer. This asynchronous approach allows
us to start multiple concurrent transmissions. Throughout this thesis we will talk about \emph{unacknowledged messages}, which
are messages that have been sent, but the transfer has not yet been completed. These unacknowledged messages are very 
important to achieve the best possible performance.

\begin{figure}[htp]
\begin{lstlisting} [caption={Sender Interface},captionpos=b, label={list:sender}] 
class Sender {
    // Takes a registered region containing the message
    // and initiates the transfer. Returns an ID 
    // associated with this request
    uint64_t SendAsync(SendRegion reg);

    // Blocks until the request with the given ID is
    // completed
    void Wait(uint64_t id);
}
\end{lstlisting}
\end{figure}

\paragraph{} To allow us to \code{Wait} for a specific work request, most of our implementation use the same process. 
Initiating a send request returns a monotonically increasing IDs. This ID is also used as a \code{wr\_id} for the issued RDMA
operation. We store the last ID with a matching completion event. If we try to wait on a lower ID we know we already received 
a matching completion event prior, because of the in order guarantees of InfiniBand. If the provided ID is higher then the
last seen ID, we poll the completion queue until we receive one with a matching \code{wr\_id}.

This approach will eventually lead to an overflow, but given our observed  message rates we do not expect to see this 
happen in the next 10 thousand years. 

\subsubsection{Receiver}

\paragraph{} Almost all of our implementations of a receiver have fixed regions of memory to receive messages into. 
For an application receiving means that the protocol returns a reference one of, or a part of, this memory regions. 

As soon as the application  has processed this message it needs to the protocol that it can reuse the corresponding buffer.
We call this \emph{freeing} the receive buffer. This approach avoids unnecessary copying from the protocols receive buffer 
to an application buffer if this is not necessary.

\begin{figure}[htp]
\begin{lstlisting} [caption={Receiver Interface},captionpos=b, label={list:receiver}] 
class Receiver {
    // Waits for an incoming message and returns
    // the buffer containing it
    ReceiveRegion Receive();

    // Returns the previously received region 
    // to be reused by the protocol
    void Free(ReceiveRegion reg);
}
\end{lstlisting}
\end{figure}






