\section{Data Exchange Protocols}\label{sec:protocols}
The goal of this work is design, implement, and study a comprehensive list of RDMA based protocols for data exchange. 
However without a clear definition of \emph{data exchange} the number of possible protocols is virtually endless.

In this section we will limit our definition of \emph{data exchange protocol} to a more clearly defined message passing
protocol, which reduces the size of our design space an allows us to more reasonably compare protocols. We will then introduce
features outside of raw performance that have been relevant for real-world applications. Finally we introduce six 
protocols that all use different RDMA features and design approaches to implement message passing protocols with different 
features and performance guarantees.

\comment{need better title}
\subsection{Definition}\label{sec:proto-def}

Most research focuses on RPCs \cite{anuj-guide, fasst, herd}, remote datastructure access \cite{pilaf, farm}, or replicate 
socket like interfaces \cite{socksdirect}. In this thesis we will look at message passing protocols with fairly few guarantees.
We believe with this more general communication model we can give engineers and researchers a better understanding of the 
building blocks that can be  used to build more specific connection protocols, like RPCs, while not only 
micro-benchmarking RDMA verbs.



\begin{defn}
We define a message passing protocol $P$ as an algorithm for moving a message $m$ that resides in the memory of
node $N_a$ to another node $N_b$. The transfer must be initiated by $N_a$. After the transfer, $N_b$ must know that the 
message was fully received and $N_a$ must know that it can reuse $m$.
\end{defn}

We explicitly have no synchronisation requirements, so the sender does not need to know whether the receiver has actually 
received the message.

\paragraph{}\todo{Add more details. Maybe show interface?}

\pagebreak

\subsection{Features}

There is more to data exchange protocols than raw throughput or latency. Oftentimes it is more important for an application
that the connection is \emph{fair} or that the memory requirements do not grow too large, even if that protocol wastes
a few microseconds.

To be able to better compare our presented protocols we define a few traits that are often required by systems and will 
analyse which protocols provide these traits

\paragraph{True Zero-Copy} Whether the protocol can be employed to receive directly to the destination buffer. While RDMA 
claims to be zero-copy, most protocols are not truly zero-copy. Protocols using ringbuffers or mailboxes are
strictly speaking not zero copy, as the receiver will always have to copy the data from this buffer to its actual destination.
This is can be especially important when transferring a large amount of data that does not have to be further processed.

\paragraph{Variable Message Size} That the protocol allows us to send messages with different sizes, without using the 
complete buffer at the receiver. Ring buffers for example can be designed to only use the necessary space per message
while send receive will always use the complete receive buffer, no matter how small the message actually is.

\paragraph{Passive Sender/Receiver} Whether the sender or receiver is \emph{passive}. We define \emph{passivity} as not 
requiring any CPU involvement in the actual data transfer. That means a \emph{passive receiver} can receive messages by
simply reading from a memory location, without having to acknowledge anything or posting receive buffers. \emph{Passivity} 
reduces the necessary CPU usage to a minimum and can be useful for applications with uneven CPU requirements.

\paragraph{Interrupts} Whether there is some kind of notification system, that allows the receiver to be notified 
of an incoming message without having to constantly poll for it. While polling gives us better performance in almost all cases
constantly polling wastes a lot of CPU cycles when we do not receive a lot of messages.

\paragraph{Resource Sharing} If multiple connections can share resources, especially memory. We are using \emph{reliable connections}
so we will inherently need to use one \emph{QP} per sender and receiver pair, but we can reduce memory usage by sharing receive 
buffers between multiple connections. Without any memory sharing, utilization grows linearly with the number of QPs, which 
can quickly lead to unacceptable memory requirements, especially for $N$ to $N$ communication patterns.

\paragraph{Fairness} By fairness, we mean that a single QP cannot block other QPs. This becomes relevant, as soon as we start
sharing resources between QPs. It is oftentimes important that a single slow running task cannot completely block all 
connections.\comment{We need something else.}




\subsection{Design Space}

Even with the more limited definition of \emph{data exchange} in Section~\ref{sec:proto-def} there are countless numbers
of non trivial ways to implement such a protocol using RDMA features. In this section we will take a look at the design 
space of data exchange protocols, we will show different approaches and point out sepecific protocols that we implemented
as a datapoint for other developers to design their own protocols.



\paragraph{} The first obvious way to split all possible exchange protocols is by the verbs they use to transfer the actual 
data, and we will look at each of these verbs separately, as these all work in different ways and have different requirements

We did not see any advantages in splitting the actual data transfer into multiple verbs, as this only seems to add overhead 
and complexity. And while it would theoretically be possible to use \emph{Compare and Swap} for generic data transfer,
the expected performance is far too low to get a viable protocol. So we will not look into these options but focus on the 
following verbs.

\paragraph{} We want to point out that this is in no way an exhausting list of protocols, but should rather show that the 
design space is very large and that there are a lot of interesting protocols that can still be explored.

\subsubsection{Send}
The \emph{Send} verb basically fulfills our requirements for a \emph{data exchange protocol} out of the box. It is sender
initiated, the sender can reuse the buffer after the send is complete, and the receiver is notified of a new message by
a completion event.

There are however sill multiple ways to tweak and tune a send receive based protocol by batching, inline sending, or 
sharing receive buffers between multiple connections using a \emph{Shared Receive Queue}. We explore these implementation detail and their performance
characteristics in Section \ref{sec:conn:send}.

\subsubsection{Write}
The \emph{Write} verb is a lot less restrictive. It allows the sender, with certain restrictions, to write at an arbitrary 
location in the receivers memory, without any interaction of the receivers CPU. This also means we need to solve some of the 
problems which the \emph{Send} verb solved for us.

\begin{itemize}
  \item For the sender to be able to transfer data, it needs to know where to send it to and if the receiver is ready 
to receive the data.

  \item For the receiver to notice that it received data it needs some kind of way to notified.
\end{itemize}

This two problems will always exist when using \emph{Write} as the data transfer verb and there are many ways to to solve
these problems. We looked at and implemented two general approaches, each of them can again be implemented in subtly 
different ways which can in turn have drastically different performance characteristics.

\paragraph{Buffered Write}
This is probably one of the most widely used data transfer protocol there is and usually the first protocol people think of
apart from send-receive. The idea is to have a buffer at the receiver, which is managed by the sender. In most cases this is 
a ring buffer. 

This reduces the problem of where to write to, to noticing if there is space left in the buffer. This in turn does not 
have to be done for each data transfer individually, saving us a lot of communication overhead. But this can be done in 
may different ways. The problem of noticing incoming messages also still exits. The receiver needs to notice that new data was 
written to the buffer and how much was written to the buffer. 

So while the \emph{Buffered Write} simplifies a lot of problems when using \emph{Write} as a data transfer verb, there are 
downsides to using a buffer to transfer data. Most importantly the buffer is usually not the place the data should finally 
end up in, which forces us to copy the received data again.

For a more detailed analysis of \emph{Buffered Write Protocols} and our implementation of one, 
see Section \ref{sec:conn:buf_write}. Further in Section \ref{sec:conn:shared_write} we look at a way to share this ring
buffer between multiple connections using \emph{fetch and add}.


\paragraph{Unbuffered Write}
The idea of an \emph{Unbuffered Write Protocol} is to avoid the additional copy which is usually necessary when using a
\emph{Buffered Write Protocol}. The receiver should be able to decide the target location for each message. That means there
is a communication overhead for each message, as the sender either need to query the receiver where to write to or the receiver
needs to preemptively send locations to write to. The problem of notifying the receiver of new messages stays very much the same

For a more detailed analysis of \emph{Unbuffered Write Protocols} and our implementation of one, 
see Section \ref{sec:conn:direct_write}


\subsubsection{Read}
The \emph{Read} verb is generally very similar to the \emph{Write} verbs. This time it allows the receiver, again with certain
restriction, to read from an arbitrary location in the senders memory, without any interaction of the senders CPU. Again for
any protocol using \emph{Read} as the data transfer verb we need to solve two problems.

\begin{itemize}
  \item For the receiver to be able to read the data, it needs to know that new data is ready and where to read it from.
  \item As the sender is now not directly involved in the data transfer, we need to notify it when the data transfer is over.
\end{itemize}

We again looked at and implemented two general approaches.


\paragraph{Buffered Read}
The buffered read works similarly to the \emph{Buffer Write}. This time the \emph{Sender} has a ring buffer where the 
\emph{Receiver} can read from. Similarly to the \emph{Buffer Write} this will reduce the communication overhead. 

This similarly reduces the problem of where to read from, to noticing if there is a new message in the buffer and we can tell 
the sender which messages we received by updating an offset.

For a more detailed analysis of \emph{Buffered Read Protocols} and our implementation of one, see 
Section \ref{sec:conn:buf_read}. Sharing the ring buffer between multiple connections using atomics should again be possible,
but was not explored in this thesis.

\paragraph{Unbuffered Read}
The idea of an \emph{Unbuffered Read Protocol} is again to avoid the additional copy which is usually necessary when using a
\emph{Buffered Read Protocol}. This means the sender will need to notify the receiver when a new message arrives and where to
read it from. After the read is complete the receiver will then need to notify the sender which specific buffer is ready to 
be reused. This introduces a large communication overhead but by being truly zero-copy this approach can be efficient, 
especially for large messages.

We present our implementation of an \emph{Unbuffered Read Protocols} in Section \ref{sec:conn:direct_read}



