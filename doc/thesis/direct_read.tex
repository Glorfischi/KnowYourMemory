
\section{Direct Read} \label{sec:conn:direct_read}
\subsection{Design}

In Section~\ref{sec:conn:direct_write} we discussed how we can possibly avoid an additional copy at the receiver by giving 
the sender information which allows him to potentially write the data to the correct final memory location. The next logical
step is to let the receiver decide for each message where to write it to. We can achieve this by our implementation of a
\emph{Direct Read Connection}.

\paragraph{} The core idea of a direct read protocol is that instead of directly sending a message through a send or write 
verb, the sender simply notifies the receiver of a new message and where it is located, we will call this a \emph{Read Request}.
The receiver then issues an RDMA read operation to directly read this message to the correct location.

By allowing to sender to attach additional domain specific information to this read request, the receiver can directly 
move this data to the correct space in a data structure, or potentially even directly NVMe storage for certain applications.
\comment{I remember correctly this is possible right? I had a hard time finding a relevant paper}


\subsubsection{Sender}
The sender interface is again largely unchanged and consists of a \code{SendAsync()} method which takes a buffer containing the
message and a \code{Wait()} method that waits until the transfer is completed.

\paragraph{} Instead of sending the complete buffer, the sender only sends a small \emph{Read Request} using an inline send,
containing the location of the buffer. It is then the job of the receiver to access this buffer.

\paragraph{} To wait for the transfer to be completed, and for the buffer to be able to be reused, we can not simply wait 
for the completion event of the send, like we do for the send or write based connections. We need to wait for the receiver 
to explicitly signal that the buffer was transfered. We append a signaling byte at the end of the sending buffer. 
When sending this byte will be set to 0 and we can wait for the transport to be completed by polling this byte until the 
receiver will update it.

This push based implementation introduces little additional complexity, but there are many different ways to implement such 
signaling. The signaling bit forces us to use a specific memory arrangement, which could prevent us to send data directly 
from certain data structures. In such cases a pull based approach or an implementation using send might be a better approach
and depending on the implementation could actually result in better performance.

\subsubsection{Receiver}
The receiver polls the receive queue and receive the \emph{Read Request}. It then instantly reposts the associated 
receive buffer. With this information it then issues a read request for the message. As for this connection the receiver is
doing the heavy lifting, it is crucial that we do not block until the read is completed to get reasonable performance. 

\paragraph{} This means the receiver has a slightly different interface that the previously presented connections. 
We split the receive call into a \code{RequestAsync} and a \code{Wait} method. The \code{RequestAsync} takes a receive buffer
to read into. It will wait for an incoming read request and issue the corresponding read. It uses the same increasing 
\code{wr\_id} approach we use for sending with which the \code{Wait} method can wait for the read to complete. This approach
allows us to pipeline receives the same way we pipeline sends.

\paragraph{} To update the signal byte to notify the that the transfer is complete we implement to different approaches.

Our first approach is to issue the write with the update at the same time as we issue the read. This however introduces a 
problem. While RDMA guarantees that two consecutive will happen in the issued order, it will not guarantee us that reads issued
before writes will be completed before we can observe the write~\cite{}. \comment{I think we need some kind of references for
that} To still be able to issue these two operations at the same time, we need to use a \code{IBV\_SEND\_FENCE}. When we add 
fence indicator to a write request its processing will not begin until all previous read and atomic operations on the same QP 
have completed. This allows us to issue both the read as well as the acknowledging write at the same time.

The other approach is simply to wait for the read to be completed before issuing the acknowledging write. In other words the
write will be posted as soon as \code{Wait} returns. That way we can avoid the fence which can be quite expensive.

\subsection{Conclusion}

The Direct Read connection is very different approach to a data transfer protocol with the goal of achieving 
\emph{True Zero Copy} capabilities as the receive itself can read the message to its final place, preventing any additional 
copying. The protocol also provides us with good \emph{Fairness} guarantees and efficient memory usage though 
\emph{Variable Message Sizes} and not having any fixed buffers. With that \emph{Resource Sharing} follows as multiple
connections can reuse the same receive and send buffers. It also allows us to use \emph{Interrupts} form completion events
giving us low CPU utilization in low load situations as no component needs to constantly poll.

\paragraph{} All these features make this protocol viable, especially when handling large messages. However the very high
base latency and the impact of the communication overhead on bandwidth makes this not a good fit for transferring small 
messages. Further as the bulk of the work has to be done by the receiver this protocol is not ideal for N:1 communication
either.

\paragraph{} We saw that while \emph{fences} can be very powerful to ensure correctness, the can introduce severe performance
penalties and oftentimes simply waiting for the first operation to complete yields better throughput. So fences need to be
applied with care.

\paragraph{} The Direct Read protocol is not a good fit for all situations but can be a very good choice when transferring 
large amounts of data for for 1:N communication, for example when replicating data to other nodes. 
\comment{Actually saw a paper on distributed FS that uses this. Should we add such references to this section or simply leave
it in related work?}

