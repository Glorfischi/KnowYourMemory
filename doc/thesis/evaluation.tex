\section{Evaluation}

We ran all our evaluations on two machines running CentOS 7 containing two Intel Xeon Gold 6152 and 384 GiB of memory.
The two nodes each contain a Mellanox ConnectX-5 (100Gbps) and are connected through a 100 Gbps switch. All measurements
have been performed using RoCE.

\subsection{Latency}

 In our latency benchmark a single client and server perform a \emph{ping-pong}. With that 
we mean that the client initiates the communication and measures the RTT. The server mirrors all received packages. 
We then take half of this RTT as our measurement of latency.

\begin{figure}[h]
\includegraphics[width=1\textwidth]{lat-msgsize.png}
\caption{Latency evaluation}
\label{fig:plot-lat}
\end{figure}


Figure~\ref{fig:plot-lat} shows the median latency of nearly all of our protocol implementations. For all but one of these 
connections we see more or less what we have predicted given our model. They all have different base latency overheads
and then the latency grows linearly with increasing message sizes. The one connection that is standing out
and not quite adheres to our model is the buffered read connection (BR).

\paragraph{} The send-receive connection (SR) as well as the direct-read connection (DR) achieve very similar low latency,
with a base overhead of just over 2 $\mu s$. This reinforces the similarities between these two protocols and that the 
direct-write connection is essentially a reimagining of the send-receive protocol using only RDMA writes.



\paragraph{} All buffered-write protocols (BR) show a slightly increased latency compared to the send-receive protocol.
Both the write reverse (BR-Rev) as well as the not pictured write immediate implementations have a base latency overhead 
of about 2.25 $\mu s$, which is still very much comparable to the send-receive protocol. The write offset 
(BR-Off) implementation shows a higher constant overhead of about 3 $\mu s$. This is expected as this approach needs to 
issue two writes for a single message.


\paragraph{} The two read based protocols have significantly higher latency for our ping-pong evaluation. This vast
difference in latency 
is however not only caused by the read verb. For this experiment both of our read protocols have a significant 
communication overhead. Coupled with the fact that smaller reads do have a higher latency this results in the very high
base overheads we see.

The direct read protocol (DR) first sends a read request from the sender to the receive using a send operation. Only then the
receive can issue the RDMA read operation that transfers the message. So the observed base latency overhead of 
nearly 6~$\mu s$
does not only contain the read operation, but also a complete send latency for a small message.

For the buffered read protocol~(BR) the receiver needs to read from the senders memory to notice new messages. 
This means for our
latency experiment the buffered read protocol needs to issue at least two read operations per message. This results in the
very high base overhead of over 7 $\mu s$. We see another jump in latency for messages over 1 KB. This is likely cause by the
sender not having written the complete message to the buffer when the receiver issues its first polling read operation. 
This results in up to three read operations per message for large messages.

\paragraph{} We can summarize this subsection with: Simplicity is key. The most important thing to achieve low 
latency is to avoid unnecessary round-trip times and minimize the number of RDMA operations. We achieved the best
performance  using send-receive or single writes. We could however not reproduce the common assumption that RDMA writes 
are faster than send-receive.






\pagebreak

\subsection{Bandwidth}

We evaluate the single connection bandwidth of our protocol implementations for varying message sizes. Where applicable all 
protocols allow for sufficient unacknowledged messages to not run into a round-trip bottleneck. If not 
stated otherwise we report the median throughput of sending a million messages from a single sender to a single receiver
on two separate nodes.

\begin{figure}[htp]
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-direct-bw-msgsize.png}
  \caption{Send-Receive / Direct-Write}
  \label{fig:plot-sr-dw-bw}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-bw-msgsize.png}
  \caption{Buffer-Write}
  \label{fig:plot-bw-bw}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{dir-read-bw-msgsize.png}
  \caption{Direct-Read}
  \label{fig:plot-dr-bw}
\end{subfigure}
\begin{subfigure}[b]{0.57\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{buf-read-bw-msgsize.png}
  \caption{Buffered-Read}
  \label{fig:plot-br-bw}
\end{subfigure}
  \caption{1:1 Bandwidth}
  \label{fig:plot-bw}
\end{figure}

Figure~\ref{fig:plot-sr-dw-bw} shows the point to point throughput for different message sizes for both the send-receive (SR)
protocol as well as the direct-write (DW) protocol. 

For the send-receive connection we see a linear increase in bandwidth with increasing message size until we hit the
maximum goodput of our network link. This indicates that we are client side CPU bound. This is confirmed by the fact that we can
improve performance for smaller messages using sender side batching which we have shown in Section~\ref{sec:model}.

The direct-write protocol however seems to be limited by the sending NIC instead or possibly by the PCIe bus, as the CPU needs
to wait for the operation to be completed and we are not limited by our ability to issue work requests like we are for the 
send-receive connection. This results in up to 30\% lower throughput for medium sized messages. We suspect that the large 
amount of returning writes interferes with outgoing writes and increases the per message overhead for the NIC. 


\paragraph{} Figure~\ref{fig:plot-bw-bw} shows the 1:1 bandwidth for all buffered write variants. We only show the measurements
using the \emph{send} acknowledger as, with one exception we will explain below, there are no significant differences
in performance between these two implementations.

Both the write immediate (BW-Imm) as well
as the write reverse (BW-Rev) implementations achieve very similar performance. They achieve slightly lower throughout
than the send-receive protocol, but show the same linear increase in performance indicating a sender CPU bottleneck. The
slightly lower throughout can be explained by the small CPU overhead of managing the ring-buffer and more importantly  
polling for tail updates.

The write offset (BW-Off) implementation shows consistently lower throughput until it also achieves
link speed for message sizes of 16 KB. This is what we would expect as this implementation needs to issue 
twice as many operations. It seems that we are bottleneck by the sending NIC overhead when the other
implementations already achieve line rate.

\paragraph{} Figure~\ref{fig:plot-dr-bw} show the bandwidth for the direct-read protocol, with using a memory fence
to issue the read and send operation at the same time, and without this fence.  The main thing we notice is that the 
fenced version achieves drastically lower throughput. The fence essentially serializes the reads and prevents the NIC 
to effectively pipeline operations. This gives us the low and linearly increasing bandwidth we observe.

While the direct read protocol actually works very differently from what we represent in our model the performance 
characteristics of the unfenced direct read connection is very similar to what we observed for previous connections. The
throughput increases linearly for small messages as we are limited by the number of request we are able to post. For larger
messages we are limited by the receiving NIC giving us this sub-linear curve until we reach the maximum goodput of our link.

For all further measurement we only focus on the unfenced implementation as the fenced version is generally unable
to achieve comparable throughput.


\paragraph{} Figure~\ref{fig:plot-br-bw} shows the 1:1 bandwidth for the buffered-read protocol. We also plot the 
\emph{mean transfer size}, so the amount of data transfered by a single read. 
This can vary greatly for this protocol as it periodically
fetches the complete sending ring-buffer. For this reason we also plot the mean bandwidth instead of the median bandwidth 
as the data is not normally distributed.

As expected with increasing message size the mean transfer size also increases linearly. The total bandwidth 
also increases quickly with approaches a maximum of a little over 80 Gbit/s. This is what we expect given our current 
implementation. We only issue a new read as soon as the previous read was completed. This results in the sub-linear
curve we can see. The observed performance could be drastically improved by preemptively issuing reads to fill up
the pipeline.

We can also see that we achieve significantly better performance for small messages compared to the other protocols we
look at in this thesis. This is a result of the automatic batching behaviour of this protocol. 


\begin{figure}[h]
\includegraphics[width=1\textwidth]{write-bw-rev-anom.png}
\caption{Buffered write reverse bandwidth for message sizes around 4 KB with read acknowledgements}
\label{fig:plot-write-rev-anom}
\end{figure}

\paragraph{Write Reverse Anomaly} During our evaluation we encountered a anomaly we cannot fully explain at
this point. As seen in Figure~\ref{fig:plot-write-rev-anom}, when using a \emph{write reverse} sender and a 
\emph{read} acknowledger we see a significant drop in performance when sending messages that are slightly larger
than 4090 bytes. It is worth noting that when adding the 6 byte overhead from the protocol explained in 
Section~\ref{sec:conn:write:sender} this happens to be exactly when the write is larger than 4 KB, 
which is both the pagesize and MTU. Bandwidth then seems to linearly increase and will drop again very similarly 
for all multiples of 4 KB.

Interestingly this cannot be observed with other sender implementations and the effect is greatly
reduce when using send acknowledgements instead of read based acknowledgements.

We suspect this anomaly is caused by our unusual write direction which causes suboptimal NIC cache usage and 
results in many TLB cache misses while writing. But this is just one possible explanation and further research is 
necessary to fully explain these results.

For all other plots we purposely avoid to exactly hit this window for the \emph{write reverse}
sender but will send slightly smaller messages.


\paragraph{} To maximize 1:1 bandwidth, reducing the number of necessary RDMA operations seems to be important.
The best performing protocols are the send-receive 
protocol as well as the buffer-write implementations that only issue a single write. Compared to our latency evaluation
however the difference are not as severe. 
Protocols like direct-read or direct-write, which can achieve true zero-copy capabilities, can still achieve decent 
throughout and if we are handling messages larger than 16 KB almost all approaches can achieve line rate.








\pagebreak
\subsection{N:N}


We can see that when we have single point to point connection, we are almost always limited by the sender and the 
amount of requests we are able to issue. In practice however we usually do not have a such a simple setup, but we
need to send to and receive from multiple different nodes. This means each node needs to handle multiple open
connections. 

To evaluate the performance of our protocols with multiple open connections we again only use two nodes, however  
on each node we run $T$ threads. Each thread $t_k$ opens a connection with the corresponding 
thread on the other node, giving us a total of $T$ connection sharing the same NIC. We evaluate the throughput for 
three different message sizes which had different performance characteristics in our single threaded evaluation: 16 bytes, 
which was usually heavily bound by how fast we could post send requests, 512 bytes, which was also normally limited by the 
sender but less extremely, and 8192 bytes, which is limited by the actual device bandwidth for almost all protocols. 

We do not perform any sender side batching but allow for sufficient unacknowledged messages to keeping the pipeline full.
In our plots we report the sum of all connection throughputs.

\begin{figure}[ht]
  \centering
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-threads.png}
  \caption{Send-Receive}
  \label{fig:plot-sndrcv-bw-thread-nosrq}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-srq-threads.png}
  \caption{Send-Receive with SRQ}
  \label{fig:plot-sndrcv-bw-thread-srq}
\end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-direct-bw-threads.png}
  \caption{Direct-Write}
  \label{fig:plot-wdir-bw-threads}
  \end{subfigure}
\caption{N:N Send-Receive / Direct-Write Bandwidth}
  \label{fig:plot-sndrcv-bw-thread}
\end{figure}


\paragraph{} Figure \ref{fig:plot-sndrcv-bw-thread-nosrq} shows the N:N throughput for the send-receive protocol.
We can see that for large messages we keep being bottlenecked by the network bandwidth of 100 Gbit/s. For smaller
messages the throughout first increases linearly until we hit a bottleneck. Interestingly for a message size 
of 16 bytes we are able to send over twice the amount of messages per second compared to a message size of 512 bytes.
We suspect this to be caused by NIC level optimizations for small messages such as inline receiving~\cite{anuj-guide} which 
is supported by Mellanox NICs up to 64 bytes. This bottleneck we encounter seems to be the total throughput of the 
receiving NIC. This is significantly higher than our maximum throughput seen for a single connection, this can be 
attributed to the usage of multiple processing units~\cite{anuj-guide}


Figure \ref{fig:plot-sndrcv-bw-thread-srq} shows the same data when using a shared receive queue (SRQ) to share memory
between the connections. In this case we seem to be limited by the rate at which the receiver is able to
repost receive buffers. We
had to limit all senders to about 1.8 MOp/s to not run into RNR erros.

We later show that when using SRQs we are limited a maximum of 2 MOp/s. So even with more optimized receive buffer management
this is drastically lower than without any resource sharing and heavily limits the performance for small messages. This seems
to be a limit of the receivers NIC.

\paragraph{} Figure~\ref{fig:plot-wdir-bw-threads} shows the N:N bandwidth for the direct-write protocol. When using 8192 
byte messages, we are again limited by the maximum link speed. When using 512 byte messages we seem to bottleneck at around 
70 Gbit/s, and for small messages we are capped at around 6 Gbit/s, which is very close to the performance of the send-receive
protocol.

However the 70 Gbit/s limit for 512 byte messages is significantly less than the 80 Gbit/s maximum we saw for the
send-receive protocol. We also see a slightly lower goodput for 8192 byte messages. We assume this is caused by the 
large amount of returning write, limiting the NICs maximum throughout. We can see very similar performance for the
write offset implementation in Figure~\ref{fig:plot-write-bw-thread-512}. This reinforces our assumption that the returning write
for each message is impacting our throughput, as the write offset protocol also needs to issue two writes per message.




\begin{figure}[ht]
  \centering
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-bw-threads-16.png}
  \caption{Message size 16 bytes}
  \label{fig:plot-write-bw-thread-16}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-bw-threads-512.png}
  \caption{Message size 512 bytes}
  \label{fig:plot-write-bw-thread-512}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-bw-threads-8192.png}
  \caption{Message size 8192 bytes}
  \label{fig:plot-write-bw-thread-8192}
\end{subfigure}
  \caption{N:N Buffered-Write Bandwidth}
  \label{fig:plot-write-bw-thread}
\end{figure}


\paragraph{} Figure \ref{fig:plot-write-bw-thread} shows the total throughput of all buffered write implementations
with varying number of connections and the three different message sizes.

We can again see that for large messages we keep being bottlenecked by the network bandwidth of 100 Gbit/s, regardless of 
the sender, however the write offset (BW-Off) implementation shows a small overhead and does not quite achieve the same 
throughput as the other two implementation. 

For smaller messages we see a linear increase in throughput with increasing number of connections until we hit a bottleneck. 
There does seem to be a fair amount of noise,
but for both message sizes the write immediate (BW-Imm) and write reverse (BW-Rev) implementation achieve similar performance,
while the write offset implementation is significantly slower. This can again be explained by the additionally issued write, 
which in this case  gives us a more pronounced overhead as we are bottlenecked by the number of operations the NIC can process.


Compared to the previously evaluated send-receive protocol, both the write immediate as well as write reverse implementation
achieve very 
similar bandwidth, while the results of the write offset implementation are very comparable to the direct-write protocol.


\begin{figure}[ht]
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{dir-read-bw-threads.png}
  \caption{Direct-Read}
  \label{fig:plot-dirread-bw-threads}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{buf-read-bw-threads.png}
  \caption{Buffered-Read}
  \label{fig:plot-bufread-bw-threads}
  \end{subfigure}
  \caption{N:N Bandwidth Read-based Protocols}
\end{figure}

\paragraph{}Figure~\ref{fig:plot-dirread-bw-threads} shows the direct-read protocols N:N performance. The results look very similar to 
other connections. There is a linear increase in performance, both by being able to post more work request for smaller 
messages and being able to utilize more NIC processing units.~\cite{anuj-guide}

When using 8192 byte messages, we are limited by the maximum link speed. When using 512 byte messages we hit a bottleneck at
around 70 Gbit/s, which is in line with what we observe with other protocols that need to issue two operations per message, 
like the direct-write or write-offset connection.



\paragraph{} Figure~\ref{fig:plot-bufread-bw-threads} shows the buffered-read protocols N:N performance. We see drastically 
improved performance compared to the single connection evaluation. This is mainly caused by the fact that we now have multiple
concurrent active read operations. Through the inbuilt message batching we achieve  significantly higher throughput for small
messages compared to the other connection types we evaluated. Throughput for all message sizes grow linearly with increasing 
number of connections and are only limited by the line rate or eventually by the individual copying of buffers at the sender 
and its function call overhead.

This is a drastically different performance profile compared to our other protocols. 
The aggressive sender side batching allows
for very high bandwidth, but at the cost of increased latency.


\paragraph{} The main takeaway when designing protocols for N:N communication patterns is to reduce the number of operations
per message. We saw very similar throughput for all connections with a single operation per message and two operations per
message respectively. Interestingly it does not seem to matter in which direction these operations are performed. The 
direct-write protocol with its returning writes performed very similarly to the write-offset or direct-read protocol.

One thing the buffered read connection has shown us is that to achieve optimal bandwidth for small messages sending side and
whenever possible application level batching is necessary. And we also saw that SRQs, while saving memory usage, have 
significant performance limitations.



\pagebreak
\subsection{N:1}
One of the most prevalent communication pattern is the N:1 configuration, where a single server handles the messages
of multiple clients. 

For our evaluation we only use two nodes. On the sending node we have $N$ threads that send messages to a
single receiving thread on the other node. If not stated otherwise this receiving thread will simply 
round robin over the $N$ open connections. 

As we did for the N:N experiments, we evaluate the throughput for three different message sizes: 16 bytes, 
512 bytes, and 8192 bytes. We do not perform any sender side batching but allow for sufficient unacknowledged messages 
and in our plots we report the sum of all connection throughputs.


\begin{figure}[ht]
  \centering
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-n1.png}
  \caption{Send-Receive}
  \label{fig:plot-sndrcv-bw-n1-nosrq}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-srq-n1.png}
  \caption{Send-Receive with SRQ}
  \label{fig:plot-sndrcv-bw-n1-srq}
\end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-direct-bw-n1.png}
  \caption{Direct-Write}
  \label{fig:plot-wdir-bw-n1}
  \end{subfigure}
\caption{N:1 Send-Receive / Direct-Write Bandwidth}
  \label{fig:plot-sndrcv-bw-n1}
\end{figure}



\paragraph{} Figure~\ref{fig:plot-sndrcv-bw-n1-nosrq} shows the throughput for the send-receive protocol and a single receiver. We
use the single receive approach we described in Section~\ref{sec:conn:send}, which allows us to route all completion events for 
multiple QPs to a single receiving thread. To prevent Reader-Not-Ready (RNR) errors, which happen when the receiving CPU is 
unable to repost receive buffers quickly enough, we limit the sender to a stable sending rate.

For 16 byte messages we seem to be limited at around 11 MOp/s, while for 512 bytes we limited at around 16 MOp/s. Both bottlenecks
are caused by the receiving CPU. We expect the difference in sustainable message rates to be the result of inline receives for 
16 byte messages, which actually has detrimental effects on throughput, as this causes additional overhead for the CPU which 
is already the bottleneck in this situation.

However for both smaller message sizes we see a drop in performance when further increasing the number of sender. We explain 
this by increased cache misses, caused by having to access more QPs and the linearly growing number of receive buffers.

\paragraph{} Figure~\ref{fig:plot-sndrcv-bw-n1-srq} shows the same plot while using a shared receive queue for all QPs. For
large messages we are again only limited by the link speed.

For small messages of 32 bytes or lower we see similar throughout as without using a shared receive queue. We are still
limited by the receiving CPU. We do however not see any performance drops when using an increasing amount of senders. This
would fit our explanation of cache misses for receive buffers, as when we are using a SRQ the number of receive buffer stays
constant.

More interesting are message sizes above 32 bytes that are not limited by the link speed. Without the receive optimizations 
for very small messages we seem to be limited at 2 MOp/s
when using an SRQ. Interestingly this effect does not show up when only
one QP is using the SRQ. We expect this to be some kind of locking or atomic operation of the NIC itself. This results in a
major bottleneck for any implementation using shared receive queues.

\paragraph{} Figure~\ref{fig:plot-wdir-bw-n1} shows the direct-write protocols throughput. Similarly to the 
send-receive protocol we are very much limited by the receiving CPU.

Very large messages achieve the maximum goodput,  while smaller messages seem to be limited by the reposting speed of the 
receiver at around 4 MOp/s, which is already a bottleneck with two open connections.

This bottleneck gives us performance caps for 512 byte messages at around 15 Gbit/s and at 0.5 Gbit/s for 16 byte messages.
There seems to be a very slight reduction in throughput when further increasing the number of open connections, which we 
attribute to increased cache misses.

The maximum throughput could be be improved by batching returning writes, but as it stands the direct-write connections
achieves significantly worse performance than the send-receive protocol.



\begin{figure}[ht]
  \centering
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-bw-n1-16.png}
  \caption{Message size 16 bytes}
  \label{fig:plot-write-bw-n1-16}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-bw-n1-512.png}
  \caption{Message size 512 bytes}
  \label{fig:plot-write-bw-n1-512}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-bw-n1-8192.png}
  \caption{Message size 8192 bytes}
  \label{fig:plot-write-bw-n1-8192}
\end{subfigure}
  \caption{Bandwidth with varying number of threads and a single receiver}
  \label{fig:plot-write-bw-n1}
\end{figure}

\paragraph{} Figure~\ref{fig:plot-write-bw-n1} shows the N:1 throughput for all buffered-write protocols as 
well as the two shared-write implementations.

For large messages of size 8196 throughput is again limited by the link speed for all buffered-write implementations, 
with the same slight overhead for the write offset (BW-Off) implementation we have seen in all previous bandwidth plots.
For smaller messages the buffered-write protocols are limited by the receivers CPU. We can avoid any 
RNR errors for the write immediate sender by limiting the ring-buffer size. This way we do not have to artificially limit 
the sender as we did for the send-receive protocol.

For both 16 byte as well as 512 byte messages, the write reverse \mbox{(BW-Rev)} implementation achieves up to 20\% higher throughout
compared to the write immediate (BW-Imm) protocol. This can be explained by the additional receive buffer reposting the 
receiving CPU needs to perform when using write with immediate. 

The write offset (BW-Off) implementation achieves similar performance to BW-Imm. Interestingly BW-Off seems to reach
slightly higher performance when opening more than 6 concurrent connections with both 512 byte as well as 16 byte
sized messages. As of
writing this, we do not have a clear explanation for it. Our current best guess is, that with more than 6 connections the 
metadata is spread over more than one cache line. This can result in less cache invalidation and in turn in lower receiver 
CPU usage and better overall performance. Further research into this is necessary. 


\paragraph{} Figure~\ref{fig:plot-write-bw-n1} also evaluates the shared write protocol (SW). We can see that we are strongly limited
by our two phase approach for all message sizes. That means we see a linear increase in bandwidth with increasing number of 
senders as we are able to issue more requests in parallel.

For the large message size of 8 KB the total throughput increases linearly until we hit the line rate with 10 
active senders. The use of device memory \mbox{(SW-DM)} allows us to saturate the link with only 9 connections.
This is not a 
large difference and we are able to take full advantage of the bandwidth with or without the usage of device memory.

Smaller messages give use more interesting results. For a message size of 512 bytes we first see a linear 
increase in bandwidth for both versions with and without usage of device memory. The version without device memory however
caps at around 7.5 Gbps, which is exactly what we predicted given our \emph{fetch and add} micro-benchmark that gave us a
peek throughput of 2 MOp/s. The version utilizing device memory for the metadata achieves higher throughput with more
senders and we expect it to theoretically reach a maximum throughput of around 30 Gbps with enough concurrent senders.

We see very similar results for the 16 bytes messages. The version performing fetch and add on RAM peaks at about 0.25 Gbps, 
while the version operating on device memory is able to achieve higher throughput.



\begin{figure}[ht]
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{dir-read-bw-n1.png}
  \caption{Direct-Read}
  \label{fig:plot-dirread-bw-n1}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{buf-read-bw-n1.png}
  \caption{Buffered-Read}
  \label{fig:plot-bufread-bw-n1}
  \end{subfigure}
  \caption{N:1 Bandwidth Read-based Protocols}
\end{figure}

Figure~\ref{fig:plot-dirread-bw-n1} shows the direct-read protocols throughput with a single receiver and varying number of senders.
We are again very much limited by the receiving CPU in this case, as the receiver already needs to do the heavy lifting for 
this protocol. 

For smaller messages there is no throughput improvements at all with increasing number of connections as we already have 
been limited by the ability of the receiving CPU to post work requests. For very large messages we see small improvements
in throughput. This is caused by the ability to use multiple NIC processing units.

This communication pattern does not seem to be good fit for this protocol as it is always limited at around 
2.6 MOp/s by the receiving CPU speed. This gives us the 10 Gbit/s and 0.3 Gbit/s bottlenecks for the 512 byte and
16 byte size messages respectively.

\paragraph{} Figure~\ref{fig:plot-bufread-bw-n1} shows the buffered-read protocol's N:1 throughput. It 
achieves basically the 
same performance as when using a single connection. This is exactly what we expect as receiving from multiple connections
behaves the same way as receiving from a single connection.

There seems to be performance degradation when increasing the number of senders. This can  be explained
by increased cache misses, caused by the growing total buffer space and the usage of multiple QPs.


\paragraph{} Unsurprisingly when developing a protocol for a N:1 communication it is vitally important to reduce the 
involvement of the receiving CPU in the transmission. This makes read based protocols unsuitable. Interestingly in our
evaluation the send-receive protocol outperformed both the direct as well as buffered-read protocols. But we need to 
keep in mind that we should be able to drastically improve the direct-write protocol's performance by reposting
buffers in batches or by redesigning this reposting entirely to reduce the number of returning writes.

Also for the send-receive protocol to be stable enough for production use we would need to add some kind of acknowledging to
avoid RNR errors, which should further level the playing field and result in similar performance for send and write based 
protocols.

Finally we again saw that resource sharing does not come for free. Both the shared write as well as shared receive queue based
protocols are significantly slower than their unshared counterparts. 








