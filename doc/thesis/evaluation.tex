\section{Evaluation}

We ran all our evaluations on two machines running CentOS 7 containing two Intel Xeon Gold 6152 and 384 GiB of memory.
The two nodes each contain a Mellanox ConnectX-5 (100Gbps) and are connected through a 100 Gbps switch. All measurements
have been performed using RoCE.

\subsection{Latency}

 In our benchmark a single client and server perform a \emph{ping-pong}. With that 
we mean that the client initiates the communication and measures the RTT and the server mirrors all received packages. 
We then take half of this RTT as our measurement of latency.

\begin{figure}[h]
\includegraphics[width=1\textwidth]{lat-msgsize.png}
\caption{Evaluation of the Send Receive latency between two nodes with varying message sizes}
\label{fig:plot-lat}
\end{figure}


Figure \ref{fig:plot-lat} shows the latency of nearly all of our protocol implementations. For all but one of these 
connections we see more or less what we have predicted given our model. They all have different base latency overheads
and then the latency grows more or less linearly with increasing message sizes. The one connection that is standing out
and not quite adheres to our model is the buffered read connection (BR).

\paragraph{} The send-receive connection (SR) as well as the direct-read connection (DR) achieve very similar low latency,
with a base overhead of just over 2 $\mu s$. This reinforces the similarities between these two protocols and that the 
direct-write connection is essentially a reimagining of the send-receive protocol using only RDMA writes.



\paragraph{} All buffered-write protocols (BR) show a slight increase in latency. Both the write reverse (BR-Rev) as well 
as the not pictured write immediate implementations have a base latency overhead of about 2.25 $\mu s$, which is which is
still very much comparable to the send-receive protocol. The write offset (BR-Off) implementation shows a constant overhead 
of about 3 $\mu s$. This was expected as this approach needs to issue two writes for a single message.


\paragraph{} The two read based protocols have significantly higher latency for our ping-pong evaluation. This stark difference
is however not only caused by the read verb. For this experiment both of our read protocols have a significant 
communication overhead. Coupled with the fact that smaller reads do have a higher latency than comparable writes~\cite{}
this results in the very high base overheads we see.

The direct read protocol (DR) first sends a read request from the sender to the receive using a send operation. Only then it the
receive can issue the RDMA read operation that transfers the message. So the observed base latency overhead of nearly 6~$\mu s$
does not only contain the read operation, but also a complete send latency for a small message.

For buffered read protocol~(BR) the receiver needs to read from the senders memory to notice new messages. This means for our
latency experiment the buffered read protocol needs to issue at least two read operations per message. This results in the
very high base overhead of over 7 $\mu s$. We see another jump in latency for messages over 1 KB. This is likely cause by the
sender not having complete message to the buffer when the receiver issues its first polling read operation. This result in up 
to three read operations per message for large messages.

\paragraph{} We can summarize this subsection with simplicity is key. The most important thing to achieve low 
latency is to avoid unnecessary round-trip times and minimize the number of RDMA operations. We achieved the best performance 
using send-receive or single writes, and we could not reproduce the wisdom of RDMA writes being faster than send-receive.






\pagebreak

\subsection{Bandwidth}

We evaluate the single connection bandwidth our protocol implementations for varying message sizes. Where applicable all 
protocols allow for sufficient unacknowledged messages to not run into a round-trip bottleneck.\comment{Naming!} If not 
stated otherwise we report the median throughput over sending a million messages from a single sender to a single receiver
on two separate nodes.

\begin{figure}[htp]
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-direct-bw-msgsize.png}
  \caption{Send-Receive / Direct-Write}
  \label{fig:plot-sr-dw-bw}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-bw-msgsize.png}
  \caption{Buffer-Write}
  \label{fig:plot-bw-bw}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{dir-read-bw-msgsize.png}
  \caption{Direct-Read}
  \label{fig:plot-dr-bw}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{buf-read-bw-msgsize.png}
  \caption{Buffered-Read}
  \label{fig:plot-br-bw}
\end{subfigure}
  \caption{1:1 Bandwidth}
  \label{fig:plot-br-bw}
\end{figure}

Figure~\ref{fig:plot-sr-dw-bw} shows the point to point throughput for different message sizes for both the send-receive (SR)
protocol as well as the direct-write (DW) protocol. 

For the send-receive connection we see a linearly increase in bandwidth with increasing message size until we hit the
maximum goodput of our network link. This indicates that we are client side CPU. This is confirmed by the fact that we can
improve performance for smaller messages using sender side batching which we have shown in Section~\ref{sec:model}.

The direct-write protocol however seems to be limited by the sending NIC instead or possibly by the PCIe bus, as the CPU needs
to wait for the operation to be completed and we are not limited by our ability to issue work requests like we are for the 
send-receive connection. This results in up to 30\% lower throughput for medium sized messages. We suspect that the large 
amount of returning writes interferes with outgoing writes and increases the per message overhead for the NIC. 
\comment{I'm in no way sure if this is correct. I made some quick hacks to check that we actually routinely need to wait 
  multiple polls to receive a completion event, so I believe that this is a NIC bottleneck, but I don't know whether
this explanation is reasonable}
 

\paragraph{} Figure~\ref{fig:plot-bw-bw} shows the 1:1 bandwidth for all buffered write variants. We only show the measurements
using the \emph{send} acknowledger as, with one exception we will explain below, there are not significant differences
in performance between these two implementations.

Both the write immediate (BW-Imm) as well
as the write reverse (BW-Rev) implementations achieve very similar performance. They achieve very slightly lower throughout
than the send-receive protocol, but show the very same linear increase in performance indicating a sender CPU bottleneck. The
slightly lower throughout can be explained by the small CPU overhead of managing the ring buffer and more importantly polling 
polling for tail updates.

The write offset (BW-Off) implementation shows consistently lower throughput until it also achieves
link speed for message sizes of 16 KB. This is what we would expect as this implementation needs to issue 
twice as many operations. It seems that we are bottleneck by the sending NIC overhead when the other
implementations already achieve line rate.

\paragraph{} Figure~\ref{fig:plot-dr-bw} show the bandwidth for the direct-read protocol, with with using a memory fence
to issue the read and send operation at the same time, and without this fence.  The main thing we notice is that the 
fenced version achieves drastically lower throughput. The fence essentially serializes the reads and prevents the NIC 
to effectively pipeline operations. This gives us the on a low level linearly increasing bandwidth we observe.

While the direct read actually works very differently from what we represent in our model the performance 
characteristics of the unfenced direct read connection are very similar to what we observed for previous connections. The
throughput increases linearly for small messages as we are limited by the number of request we are able to post. For larger
messages we are limited by the NIC giving us this sub-linear curve until we reach the maximum goodput of our link.

For all further measurement we only focus on the unfenced implementation as the fenced version is generally unable
to achieve comparable throughput.


\paragraph{} Figure~\ref{fig:plot-br-bw} shows the 1:1 bandwidth for the buffered-read protocol. We also plot the 
\emph{mean transfer size}, so the data transfered by a single read. This can vary greatly for this protocol as it periodically
fetches the complete sending ring buffer. For this reason we also plot the mean bandwidth instead of the median bandwidth 
as the data is not normally distributed.

As expected with increasing message size the mean transfer size also increases linearly and with that the 
total bandwidth also increases until we hit a maximum at around 70 Gbit/s and a transfer size of about 100KB. After that 
the performance starts to degrade. \comment{Why? I can't explain it really. I'm having a hard time in general to explain 
this graph}

We can also see that we achieve significantly better performance for very small messages compared to the other protocols we
look at in this thesis. This is caused by automatic batching behaviour of this protocol. Performance of the buffered-read
protocol in a single connection situation could be drastically improved by limiting the maximum transfer size and 
preemptively issuing reads to fill up the pipeline and not having to explicitly wait for each
read to complete.


\begin{figure}[h]
\includegraphics[width=1\textwidth]{write-bw-rev-anom.png}
\caption{Write Reverse bandwidth for message sizes around 4 KB with read acknowledgements}
\label{fig:plot-write-rev-anom}
\end{figure}

\paragraph{Write Reverse Anomaly} During our evaluation we encountered a anomaly we cannot fully explain at
this point. A seen in Figure  When using a \emph{write reverse} sender and a \emph{read} acknowledger we see 
significant drop in performance when sending messages that are slightly larger than 4090 bytes. It is worth 
noting that when adding the 6 byte overhead from the protocol explained in Section~\ref{sec:conn:write:sender}
this happen exactly when we the final write is larger than 4 KB, which is both the pagesize and MTU. Bandwidth
then seems to linearly increase and will drop again very similarly for multiples of 4 KB.

Interestingly this cannot be observed with other sender implementation and the effect is greatly
reduce when using the send acknowledgements. We suspect this could be caused by our unusual write direction 
which causes suboptimal NIC cache usage and results in many TLB cache misses while writing. But this is just one
possible explanation and further research would be necessary to fully explain these results.

For all other plots we purposely avoid to exactly hit this window for the \emph{write reverse}
sender but will send slightly smaller messages.


\paragraph{}More is less seems to also hold for maximizing 1:1 bandwidth. The best performing protocols are the send-receive 
protocol as well as the buffer-write implementations that only issue a single write. Compared to our latency evaluation
however the difference are not as stark. 
Protocols like direct-read or direct-write, which can achieve true zero-copy capabilities, can still achieve decent 
throughout and if we are handling messages larger than 16 KB almost all approaches can achieve line rate.








\pagebreak
\subsection{N:N}


We can see that when we have single point to point connection, we are almost always limited by the sender and the 
amount of requests we are able to issue. In practice however we usually do not have a such a simple setup, but we
need to send and receive from and to multiple different nodes. This means each nodes needs to handle multiple open
connections. 

To evaluate the performance of our protocols with multiple open connections. We again only use two nodes, however  
on each node we run $T$ threads, each thread $t_k$ opens a connection with the corresponding 
thread on the other node, giving us a total of $T$ connection sharing the same NIC. We evaluate the throughput for 
three different message sizes which had different characteristics in our single threaded evaluation: 16 bytes, 
which was usually heavily bound by how fast we could post send requests, 512 bytes, which was also normally limited by the 
sender but less extreme, and 8192 bytes, which is limited by the actual device bandwidth for almost all protocols. 

We do not perform any sender side batching but allow for sufficient unacknowledged messages to keeping the pipeline full.
In our plots we report the sum of all connections throughput.

\begin{figure}[ht]
  \centering
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-threads.png}
  \caption{Send-Receive}
  \label{fig:plot-sndrcv-bw-thread-nosrq}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-srq-threads.png}
  \caption{Send-Receive with SRQ}
  \label{fig:plot-sndrcv-bw-thread-srq}
\end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-direct-bw-threads.png}
  \caption{Direct-Write}
  \label{fig:plot-wdir-bw-threads}
  \end{subfigure}
\caption{N:N Send-Receive / Direct-Write Bandwidth}
  \label{fig:plot-sndrcv-bw-thread}
\end{figure}


\paragraph{} Figure \ref{fig:plot-sndrcv-bw-thread-nosrq} shows the N:N throughput for the send-receive protocol.
We can see that for large messages we keep being bottlenecked by the device bandwidth of 100 Gbit/s. For smaller
messages first increases linearly until we hit a bottleneck. Interestingly for a message size of 16 bytes we are able to 
send over twice the amount of messages per second compared to a message size of 512 bytes. We suspect this to be caused by 
NIC level optimizations for small messages such as inline receiving~\cite{anuj-guide} which is supported by Mellanox NICs up 
to about 64 bytes. This bottleneck we encounter seems to be the total throughput of the receiving NIC. This is significantly 
higher than our maximum throughput seen for a single connection, this can be attributed to the usage of multiple processing 
units~\cite{anuj-guide}


Figure \ref{fig:plot-sndrcv-bw-thread-srq} shows the same data when using a shared receive queue (SRQ) to share memory
between the connections. In this case we seem to be limited by the speed the receiver can repost receive buffers. We
had to limit all senders to about 1.8 Mop/s to not into RNR erros.

We later show that when using SRQs we are limited a maximum of 2 MOp/s. So even with more optimized receive buffer management
this is drastically lower than without any resource  sharing and heavily limits the performance for small messages. This seems
to be a limit of the receivers NIC.

\paragraph{} Figure~\ref{fig:plot-wdir-bw-threads} shows the N:N bandwidth for the direct-write protocol. When using 8192 
byte messages, we are again limited by the maximum link speed. When using 512 byte messages we seem to bottleneck at around 
70 Gbit/s, and for small messages we are capped at around 6 Gbit/s, just like the send-receive protocol.

However the 70 Gbit/s limit for 512 byte messages is significantly less than the 80 Gbit/s maximum we saw for the
send-receive protocol. We also see a very slightly lower goodput for 8192 byte messages. We assume this is caused by the 
large amount of returning write, limiting the NICs maximum throughout. We can see very similar performance for the
write offset connections in Figure~\ref{fig:plot-write-bw-thread-512}. This reinforces our assumption that the returning write
for each message is impacting our throughput, as the write offset implementation also needs to issue two writes per message.




\begin{figure}[ht]
  \centering
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-bw-threads-16.png}
  \caption{Message size 16 bytes}
  \label{fig:plot-write-bw-thread-16}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-bw-threads-512.png}
  \caption{Message size 512 bytes}
  \label{fig:plot-write-bw-thread-512}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-bw-threads-8192.png}
  \caption{Message size 8192 bytes}
  \label{fig:plot-write-bw-thread-8192}
\end{subfigure}
  \caption{N:N Buffered-Write Bandwidth}
  \label{fig:plot-write-bw-thread}
\end{figure}


\paragraph{} Figure \ref{fig:plot-write-bw-thread} shows the total throughput of all buffered write implementations
with varying number of connections and the three different message sizes.

We can again see that for large messages we keep being bottlenecked by the device bandwidth of 100 Gbit/s, regardless of 
the sender, however the write offset (BW-Off) implementation shows a small overhead and does not quite achieve the same 
throughput as the other two implementation. 

For smaller messages we  first increases linearly until we hit a bottleneck. There does seem to be a fair amount of noise,
but for both message sizes the write immediate (BW-Imm) and write reverse (BW-Rev) implementation achieve similar performance,
while the write offset implementation is significantly slower. This can again be explained by the additionally issued write, 
which in this case  gives us a more pronounced overhead as we are bottlenecked by the number of operations the NIC can process.


Compared to the previously evaluated send-receive protocol, both the write immediate as well as write reverse achieve very 
similar bandwidth, while the results of the write offset implementation are very comparable to the direct-write protocol.


\begin{figure}[ht]
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{dir-read-bw-threads.png}
  \caption{Direct-Read}
  \label{fig:plot-dirread-bw-threads}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{buf-read-bw-threads.png}
  \caption{Buffered-Read}
  \label{fig:plot-bufread-bw-threads}
  \end{subfigure}
  \caption{N:N Bandwidth Read-based Protocols}
\end{figure}

\paragraph{}Figure~\ref{fig:plot-dirread-bw-threads} shows the direct-read protocols N:N performance. The results look very similar to 
other connections. There is a linear increase in performance, both by being able to post more work request for smaller 
messages and being able to utilize more NIC processing units.~\cite{anuj-guide}

When using 8192 byte messages, we are limited by the maximum link speed. When using 512 byte messages we hit a bottleneck at
around 70 Gbit/s, which is in line with what we observe with other protocols that need to issue two operations per message, 
like the direct-write or write-offset connection.



\paragraph{} Figure~\ref{fig:plot-bufread-bw-threads} shows the buffered-read protocols N:N performance. We see drastically 
improved performance compared to the single connection evaluation. This is mainly caused by the fact that we now have multiple
concurrent active read operations. Through the inbuilt message batching we achieve  significantly higher throughput for small
messages compared to the other connection types we evaluated. Throughput for all message sizes grow linearly with increasing 
number of connections and are only limited by the line rate or eventually by the individual copying of buffers at the sender 
and its function call overhead.

This is a drastically different performance profile than for our previous protocols. The aggressive sender side batching allows
for very high bandwidth, but at the cost of increased latency.


\paragraph{} The main takeaway when designing protocols for N:N communication patterns is to reduce the number of operations
per message. We saw very similar throughput for all connections with a single operation per message and two operations per
message respectively. Interestingly it does not seem to matter in which direction these operations are performed. The 
direct-write protocol with its returning writes performed very similarly to the write-offset or direct-read protocol.

One thing the buffered read connection has shown us is that to achieve optimal bandwidth for small messages sending side and
whenever possible application level batching is necessary. And we also saw that SRQs, while saving memory usage, have 
significant performance limitations.



\pagebreak
\subsection{N:1}
One of the most prevalent communication pattern is the N:1 configuration, where a single server handles the messages
of multiple clients. We again only use the nodes. On the sending node we have $N$ open threads send to a single receiving
thread on the other node, which will round robin over the $N$ open connections. 

Like for the N:N experiments, we evaluate the throughput for three different message sizes: 16 bytes, 
512 bytes, and 8192 bytes. We do again not perform any sender side batching but allow for sufficient unacknowledged messages 
and in our plots we report the sum of all connections throughput.


\begin{figure}[ht]
  \centering
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-n1.png}
  \caption{Send-Receive}
  \label{fig:plot-sndrcv-bw-n1-nosrq}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{send-bw-srq-n1.png}
  \caption{Send-Receive with SRQ}
  \label{fig:plot-sndrcv-bw-n1-srq}
\end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-direct-bw-n1.png}
  \caption{Direct-Write}
  \label{fig:plot-wdir-bw-n1}
  \end{subfigure}
\caption{N:1 Send-Receive / Direct-Write Bandwidth}
  \label{fig:plot-sndrcv-bw-n1}
\end{figure}



\paragraph{} Figure~\ref{fig:plot-sndrcv-bw-n1} shows the throughput for the send-receive protocol and a single receiver. We
use the single receive approach we described in Section~\ref{sec:send}, which allows us to route all completion events for 
multiple QPs to a single receiving thread. To prevent Reader-Not-Ready (RNR) errors, which happen when the receiving CPU is 
unable to repost receive buffers quick enough, we limit the sender to a stable sending rate.

For 16 byte messages we seem to be limited at around 11 Mops, while for 512 bytes we limited at around 16 Mops. Both bottlenecks
are caused by the receiving CPU. We expect the difference in sustainable message rates to be cause by the inline receives for 
16 byte messages, which actually has detrimental effects on throughput, as this causes additional overhead for the CPU which 
is already the bottleneck in this situation.

However for both smaller message sizes we see a drop in performance when further increasing the number of sender. We explain 
this by increased cache misses, caused by having to access more QPs and the linearly growing number of receive buffers.

\paragraph{} Figure~\ref{fig:plot-sndrcv-bw-n1-srq} shows the same plot while using a shared receive queue for all QPs. For
large messages we are again only limited by the link speed.

For small messages of 32 bytes or lower we see similar throughout as without using a shared receive queue. We are still
limited by the receiving CPU. We do however not see any performance drops when using an increasing amount of sender. This
would fit our explanation of cache misses for receive buffers, as when we are using a SRQ the number of receive buffer stays
constant.

More interesting are message sizes above 32 bytes that are not limited by the link speed. Without the receive optimizations 
for very small messages we seem to be limited at 2 Mops when using SRQs. Interestingly this effect does not show up when only
one QP is using the SRQ. We expect this to be some kind of locking or atomic operation of the NIC itself. This results in a
major bottleneck for any implementation using SRQs.

\paragraph{} Figure~\ref{fig:plot-wdir-bw-n1} shows the direct-write protocols throughput for N:1. For our evaluation the 
single receive simply round-robins over all open connections. Similarly to the send-receive protocol we are very much limited
by the receiving CPU.

Very large messages achieve the maximum goodput,  while smaller messages seem to be limited by the reposting speed of the 
receiver at around 4 MOp/s, which is already a bottleneck with two open connections.

This bottleneck gives us a performance caps for 512 byte messages at around 15 Gbit/s and at 0.5 Gbit/s for 16 byte messages.
There seems to be a very slight reduction in throughput when further increasing the number of open connections, which we 
attribute to increased cache misses.

The maximum throughput could be be improved by batching returning writes, but as it stands the direct-write connections
achieves significantly worse performance than the send-receive protocol.



\begin{figure}[ht]
  \centering
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-bw-n1-16.png}
  \caption{Message size 16 bytes}
  \label{fig:plot-write-bw-n1-16}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-bw-n1-512.png}
  \caption{Message size 512 bytes}
  \label{fig:plot-write-bw-n1-512}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{write-bw-n1-8192.png}
  \caption{Message size 8192 bytes}
  \label{fig:plot-write-bw-n1-8192}
\end{subfigure}
  \caption{Bandwidth with varying number of threads and a single receiver}
  \label{fig:plot-write-bw-n1}
\end{figure}

\paragraph{} Figure~\ref{fig:plot-write-bw-n1} shows the N:1 throughput for all buffered-write protocols as 
well as the two shared-write implementation.

For large messages of size 8196 are again limited by the link speed for all buffered-read implementations, 
with the same slight overhead for the write-offset (BW-Off) implementation we have seen in all previous bandwidth plots.
For smaller messages the buffered-write protocols are limited by the receivers CPU. For this connections we can avoid any 
RNR errors for the Write Immediate sender by limiting the ring buffer size. This way we did not have to artificially limit 
the sender as we did for the send-receive protocol.

For both 16 byte as well as 512 byte messages, the write-reverse (BW-Rev) implementation achieves up to 20\% higher throughout
compared to the write-immediate (BW-Imm) protocol. This can be explained by the additional receive buffer reposting the 
receiving CPU needs to perform when using write with immediate. 

The write-offset (BW-Off) implementation achieves similar performance to BW-Imm. Interestingly BW-Off seems to achieve slightly 
higher performance when opening more than 7 concurrent connections with both 512 byte as well as 16 byte sized messages. We do 
not have a reasonable explanation for this as of writing this and further research into this is necessary.


Figure~\ref{fig:plot-write-bw-n1} also evaluates the shared receive protocol. We can clearly see that we are strongly limited
by our two phase approach for all message sizes. That means we see a linear increase in bandwidth with increasing number of 
senders as we are able to issue more requests in parallel.

For the large message size of 8 KB the total throughput increases linearly until we hit the line rate with 10 
active sender. The use of device memory allows us to saturate the link with only 9. This is not a large difference and we are
able to take full advantage of the bandwidth with or without the usage of device memory.

Smaller messages give use more interesting results. For a message size of 512 bytes we first see a linear 
increase in bandwidth for both the version with and without usage of device memory. The version without device memory however
caps at around 7.5 Gbps, which is exactly what we predicted given our \emph{fetch and add} micro-benchmark that gave us a
peek throughput of 2 MOps. The version utilizing device memory for the metadata further achieves higher throughput with more
sender and we expect it to theoretically reach around 30 Gbps with enough concurrent sender.

We see very similar results for the 16 bytes messages. The version performing fetch and add on RAM peaks at about 0.25 Gbps, 
while the version operating on device memory is able to achieve higher throughput.



\begin{figure}[ht]
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{dir-read-bw-n1.png}
  \caption{Direct-Read}
  \label{fig:plot-dirread-bw-n1}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{buf-read-bw-n1.png}
  \caption{Buffered-Read}
  \label{fig:plot-bufread-bw-n1}
  \end{subfigure}
  \caption{N:1 Bandwidth Read-based Protocols}
\end{figure}

Figure~\ref{fig:plot-dirread-bw-n1} shows the direct-read protocols throughput with a single receiver and varying number of senders.
We are again very much limited by the receiving CPU in this case, as the receiver already needs to do the heavy lifting for 
this protocol. 

For smaller messages there is no throughput improvements at all with increasing number of connections as we already have 
been limited by the ability of the receiving CPU to post work requests. For very large messages there is a small improvement 
by the ability to use multiple NIC processing units, but these are then quickly limited by the maximum link speed.

This communication pattern does not seem to be good fit for this protocol as it is always limited at around 
2.6 MOp/s by the receiving CPU speed. This gives us the 10 Gbit/s and 0.3 Gbit/s bottleneck for the 512 byte and 16 byte size
messages respectively.

Figure~\ref{fig:plot-bufread-bw-n1} shows the buffered-read protocols N:1 throughput.  We can see that in this case we are 
again very much limited by the receiver and achieve basically the same performance as when using a single connection. This is 
exactly what we expect as receiving from multiple connections behaves the same way as receiving from a single connection.

There seems to be a performance degradation when increasing the number of senders. This can for one be explained
by increased cache misses and more importantly for large messages the same performance drop we see in the single connection 
benchmark when further increasing the transfer size.


\paragraph{} Unsurprisingly when developing a protocol for a N:1 communication it is vitally important to reduce the 
involvement of the receiving CPU in the transmission. This makes read based protocols unsuitable. Interestingly in our
evaluation the send-receive protocol outperformed both the direct as well as buffered-read protocols. But we need to 
keep in mind that we should be able to drastically improve the direct-writes performance by reposting buffers in batches or 
by redesigning this reposting entirely to reduce the number of returning writes.

Also for the send-receive protocol to be stable enough for production use we would need to add some kind of acknowledging to
avoid RNR errors, which should further level the playing field and result in similar performance for send and write based 
protocols.

Finally we again saw that resource sharing does not come for free. Both the shared write as well as shared receive queue based
protocols are significantly slower than their unshared counterpart. 








